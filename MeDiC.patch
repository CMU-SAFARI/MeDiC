diff -Naur gpgpu-sim-baseline/addrdec.cc gpgpu-sim/addrdec.cc
--- gpgpu-sim-baseline/addrdec.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/addrdec.cc	2015-10-13 06:38:50.566481857 -0400
@@ -67,14 +67,17 @@
 new_addr_type linear_to_raw_address_translation::partition_address( new_addr_type addr ) const 
 { 
    if (!gap) {
-      return addrdec_packbits( ~(addrdec_mask[CHIP] | sub_partition_id_mask), addr, 64, 0 ); 
+      // Saugata: modifying bit pack function based on GPGPU-Sim v3.2.2
+      // return addrdec_packbits( ~addrdec_mask[CHIP], addr, 64, 0 ); 
+      return addrdec_packbits( ~(addrdec_mask[CHIP] | L2_bank_id_mask), addr, 64, 0 ); 
    } else {
       // see addrdec_tlx for explanation 
       unsigned long long int partition_addr; 
-      partition_addr = ( (addr>>ADDR_CHIP_S) / m_n_channel) << ADDR_CHIP_S; 
+      partition_addr = ( (addr>>ADDR_CHIP_S) / Nchips) << ADDR_CHIP_S; 
       partition_addr |= addr & ((1 << ADDR_CHIP_S) - 1); 
-      // remove the part of address that constributes to the sub partition ID
-      partition_addr = addrdec_packbits( ~sub_partition_id_mask, partition_addr, 64, 0); 
+      // Saugata: add L2 bank ID to partitioning logic
+      // (from GPGPU-Sim v.3.2.2) remove the part of address that constributes to the sub partition ID
+      partition_addr = addrdec_packbits( ~L2_bank_id_mask, partition_addr, 64, 0); 
       return partition_addr; 
    }
 }
@@ -92,8 +95,8 @@
       // Split the given address at ADDR_CHIP_S into (MSBs,LSBs)
       // - extract chip address using modulus of MSBs
       // - recreate the rest of the address by stitching the quotient of MSBs and the LSBs 
-      addr_for_chip = (addr>>ADDR_CHIP_S) % m_n_channel; 
-      rest_of_addr = ( (addr>>ADDR_CHIP_S) / m_n_channel) << ADDR_CHIP_S; 
+      addr_for_chip = (addr>>ADDR_CHIP_S) % Nchips; 
+      rest_of_addr = ( (addr>>ADDR_CHIP_S) / Nchips) << ADDR_CHIP_S; 
       rest_of_addr |= addr & ((1 << ADDR_CHIP_S) - 1); 
 
       tlx->chip = addr_for_chip; 
@@ -103,10 +106,9 @@
       tlx->burst= addrdec_packbits(addrdec_mask[BURST], rest_of_addr, addrdec_mkhigh[BURST], addrdec_mklow[BURST]);
    }
 
-   // combine the chip address and the lower bits of DRAM bank address to form the subpartition ID
-   unsigned sub_partition_addr_mask = m_n_sub_partition_in_channel - 1; 
-   tlx->sub_partition = tlx->chip * m_n_sub_partition_in_channel
-                        + (tlx->bk & sub_partition_addr_mask); 
+   // Saugata: translating L2 cache bank ID
+   unsigned L2_bank_id_addr_mask = NL2_banks_per_mpu - 1;
+   tlx->L2_bank_id = tlx->bk & L2_bank_id_addr_mask;
 }
 
 void linear_to_raw_address_translation::addrdec_parseoption(const char *option)
@@ -159,15 +161,16 @@
    }
 }
 
-void linear_to_raw_address_translation::init(unsigned int n_channel, unsigned int n_sub_partition_in_channel) 
+void linear_to_raw_address_translation::init(unsigned int nchips, unsigned int n_L2_banks_per_mpu) 
 {
    unsigned i;
    unsigned long long int mask;
-   unsigned int nchipbits = ::LOGB2_32(n_channel);
-   m_n_channel = n_channel;
-   m_n_sub_partition_in_channel = n_sub_partition_in_channel; 
+   unsigned int nchipbits = ::LOGB2_32(nchips);
+   Nchips = nchips;
+   // Saugata: added number of L2 banks per MPU to address translation
+   NL2_banks_per_mpu = n_L2_banks_per_mpu;
 
-   gap = (n_channel - ::powli(2,nchipbits));
+   gap = (nchips - ::powli(2,nchipbits));
    if (gap) {
       nchipbits++;
    }
@@ -288,11 +291,12 @@
          }
       } // otherwise, no need to change the masks
    } else {
-      // make sure n_channel is power of two when explicit dram id mask is used
-      assert((n_channel & (n_channel - 1)) == 0); 
+      // make sure nchips is power of two when explicit dram id mask is used
+      assert((nchips & (nchips - 1)) == 0); 
    }
-   // make sure m_n_sub_partition_in_channel is power of two 
-   assert((m_n_sub_partition_in_channel & (m_n_sub_partition_in_channel - 1)) == 0); 
+   
+   // Saugata: make sure MPU bank ID is power of two
+   assert((NL2_banks_per_mpu & (NL2_banks_per_mpu - 1)) == 0);
 
    addrdec_getmasklimit(addrdec_mask[CHIP],  &addrdec_mkhigh[CHIP],  &addrdec_mklow[CHIP] );
    addrdec_getmasklimit(addrdec_mask[BK],    &addrdec_mkhigh[BK],    &addrdec_mklow[BK]   );
@@ -306,21 +310,22 @@
    printf("addr_dec_mask[COL]   = %016llx \thigh:%d low:%d\n", addrdec_mask[COL],   addrdec_mkhigh[COL],   addrdec_mklow[COL]  );
    printf("addr_dec_mask[BURST] = %016llx \thigh:%d low:%d\n", addrdec_mask[BURST], addrdec_mkhigh[BURST], addrdec_mklow[BURST]);
 
-   // create the sub partition ID mask (for removing the sub partition ID from the partition address)
-   sub_partition_id_mask = 0; 
-   if (m_n_sub_partition_in_channel > 1) {
-      unsigned n_sub_partition_log2 = LOGB2_32(m_n_sub_partition_in_channel); 
+   // Saugata: calculating the MPU L2 bank ID mask
+   // (from GPGPU-Sim v.3.2.2) create the sub partition ID mask (for removing the sub partition ID from the partition address)
+   L2_bank_id_mask = 0; 
+   if (NL2_banks_per_mpu > 1) {
+      unsigned n_L2_bank_log2 = LOGB2_32(NL2_banks_per_mpu); 
       unsigned pos=0;
       for (unsigned i=addrdec_mklow[BK];i<addrdec_mkhigh[BK];i++) {
          if ((addrdec_mask[BK] & ((unsigned long long int)1<<i)) != 0) {
-            sub_partition_id_mask |= ((unsigned long long int)1<<i);
+            L2_bank_id_mask |= ((unsigned long long int)1<<i);
             pos++;
-            if (pos >= n_sub_partition_log2) 
+            if (pos >= n_L2_bank_log2) 
                break; 
          }
       }
    }
-   printf("sub_partition_id_mask = %016llx\n", sub_partition_id_mask);
+   printf("L2_bank_id_mask = %016llx\n", L2_bank_id_mask);
 
    if (run_test) {
       sweep_test(); 
@@ -374,7 +379,7 @@
          printf("[AddrDec] ** Error: address decoding mapping aliases two addresses to same partition with same intra-partition address: %llx %llx\n", h->second, raw_addr); 
          abort(); 
       } else {
-         assert((int)tlx.chip < m_n_channel); 
+         assert((int)tlx.chip < Nchips); 
          // ensure that partition_address() returns the concatenated address 
          if ((ADDR_CHIP_S != -1 and raw_addr >= (1ULL << ADDR_CHIP_S)) or 
              (ADDR_CHIP_S == -1 and raw_addr >= (1ULL << addrdec_mklow[CHIP]))) {
@@ -389,12 +394,12 @@
 
 void addrdec_t::print( FILE *fp ) const
 {
-   fprintf(fp,"\tchip:%x ", chip);
-   fprintf(fp,"\trow:%x ", row);
-   fprintf(fp,"\tcol:%x ", col);
-   fprintf(fp,"\tbk:%x ", bk);
-   fprintf(fp,"\tburst:%x ", burst);
-   fprintf(fp,"\tsub_partition:%x ", sub_partition);
+   if (chip) fprintf(fp,"\tchip:%x ", chip);
+   if (row) fprintf(fp,"\trow:%x ", row);
+   if (col) fprintf(fp,"\tcol:%x ", col);
+   if (bk) fprintf(fp,"\tbk:%x ", bk);
+   if (burst) fprintf(fp,"\tburst:%x ", burst);
+   if (L2_bank_id) fprintf(fp,"  mpu_bank_id:%x ", L2_bank_id);
 } 
 
 
diff -Naur gpgpu-sim-baseline/addrdec.h gpgpu-sim/addrdec.h
--- gpgpu-sim-baseline/addrdec.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/addrdec.h	2015-10-13 06:38:50.534481857 -0400
@@ -43,15 +43,17 @@
    unsigned row;
    unsigned col;
    unsigned burst;
-
-   unsigned sub_partition; 
+   // Saugata: added L2 bank ID to address decoder
+   unsigned L2_bank_id;
 };
 
 class linear_to_raw_address_translation {
 public:
    linear_to_raw_address_translation();
    void addrdec_setoption(option_parser_t opp);
-   void init(unsigned int n_channel, unsigned int n_sub_partition_in_channel); 
+   // Saugata: need to initialize with the number of L2 banks per memory channel
+   // void init(unsigned int nchips); 
+   void init(unsigned int nchips, unsigned int n_L2_banks_per_mpu); 
 
    // accessors
    void addrdec_tlx(new_addr_type addr, addrdec_t *tlx) const; 
@@ -78,11 +80,13 @@
    unsigned char addrdec_mklow[N_ADDRDEC];
    unsigned char addrdec_mkhigh[N_ADDRDEC];
    new_addr_type addrdec_mask[N_ADDRDEC];
-   new_addr_type sub_partition_id_mask; 
-
+   // Saugata: address mask for L2 bank ID
+   new_addr_type L2_bank_id_mask;
+   
    unsigned int gap;
-   int m_n_channel;
-   int m_n_sub_partition_in_channel; 
+   int Nchips;
+   // Saugata: added number of L2 banks per memory channel
+   int NL2_banks_per_mpu;
 };
 
 #endif
diff -Naur gpgpu-sim-baseline/contention.h gpgpu-sim/contention.h
--- gpgpu-sim-baseline/contention.h	1969-12-31 19:00:00.000000000 -0500
+++ gpgpu-sim/contention.h	2015-10-13 06:38:50.558481857 -0400
@@ -0,0 +1,152 @@
+#ifndef __CONTENTION_H__
+#define __CONTENTION_H__
+
+#include <assert.h>
+#include <stdio.h>
+
+class queue_contention_model {
+    protected:
+        unsigned int num_ports;
+        unsigned int serial_queue_latency;
+        unsigned long long current_cycle;
+        unsigned long long * port_busy_until;
+
+        unsigned long long port_free_at(unsigned int portID) {
+            assert(portID < num_ports);
+            return port_busy_until[portID];
+        }
+
+        void add_job(unsigned int portID) {
+            assert(portID < num_ports);
+
+            if(port_busy_until[portID] < current_cycle) {
+                port_busy_until[portID] = current_cycle;
+            }
+
+            port_busy_until[portID] += serial_queue_latency;
+        }
+
+        unsigned int find_earliest_port(void) {
+            unsigned int earliest_port = 0;
+            unsigned long long earliest_port_time = port_free_at(earliest_port);
+
+            for(unsigned int i = 1; i < num_ports; ++i) {
+                if(earliest_port_time <= current_cycle) {
+                    break;
+                }
+
+                if(port_free_at(i) < earliest_port_time) {
+                    earliest_port_time = port_free_at(i);
+                    earliest_port = i;
+                }
+            }
+
+            return earliest_port;
+        }
+
+    public:
+        queue_contention_model(unsigned int n_ports, unsigned int latency)
+            : num_ports(n_ports)
+            , serial_queue_latency(latency)
+            , current_cycle(0)
+        {
+            assert(num_ports > 0);
+
+            // latency is not checked as greater than 0
+            // -> set to 0 if you want no contention to be modeled
+            // -> non-zero values simulate actual contention
+
+            port_busy_until = new unsigned long long[num_ports];
+
+            for(unsigned int i = 0; i < num_ports; ++i) {
+                port_busy_until[i] = 0;
+            }
+        }
+
+        ~queue_contention_model() {
+            delete [] port_busy_until;
+            port_busy_until = NULL;
+        }
+
+        void cycle(unsigned long long curr_cycle) { current_cycle = curr_cycle; } 
+
+        unsigned long long schedule() { // returns number of cycles *from now* that the command finishes
+            unsigned int portID = find_earliest_port();
+            add_job(portID);
+            return (port_free_at(portID) - current_cycle);
+        }
+
+        unsigned long long lookup_next_availability(void) { return port_free_at(find_earliest_port()); }
+
+        bool available(void) { return (port_free_at(find_earliest_port()) <= current_cycle); }
+};
+
+
+// Saugata: added queuing histogram
+template <class T>
+class contention_histogram {
+    public:
+        contention_histogram(unsigned int n_buckets, T gran)
+            : num_buckets(n_buckets)
+            , granularity(gran)
+            , sum_of_readings(0)
+        {
+            assert(num_buckets > 0);
+
+            num_readings = new unsigned int[num_buckets + 1];
+
+            for(unsigned int i = 0; i <= num_buckets; ++i) {
+                num_readings[i] = 0;
+            }
+        }
+        ~contention_histogram() {
+            delete[] num_readings;
+            num_readings = NULL;
+        }
+
+        void sample(T data) {
+            unsigned int bucket = (unsigned int)(data / granularity);
+
+            if(bucket >= num_buckets) {
+                bucket = num_buckets - 1;
+            }
+
+            ++num_readings[bucket];
+            ++num_readings[num_buckets];
+
+            sum_of_readings += data;
+        }
+
+        void print(FILE * fp, char * name = "") const {
+            unsigned long long total_readings = 0;
+
+            fprintf(fp, "-- %s Distribution\n", name);
+            for(unsigned int i = 0; i < num_buckets; ++i) {
+                if(i == (num_buckets - 1)) {
+                    fprintf(fp, "---- %s %d +: %u\n", name, i * granularity, num_readings[i]);
+                }
+                else {
+                    fprintf(fp, "---- %s %d - %d: %u\n", name, i * granularity, ((i + 1) * granularity) - 1, num_readings[i]);
+                }
+                total_readings += num_readings[i];
+            }
+
+            fprintf(fp, "-- %s Total Samples: %llu\n", name, total_readings);
+            fprintf(fp, "-- %s Average Value: ", name);
+            if(total_readings) {
+                fprintf(fp, "%f\n\n", sum_of_readings / (double)(total_readings));
+            }
+            else {
+                fprintf(fp, "0\n\n");
+            }
+        }
+
+    protected:
+        unsigned int num_buckets;
+        T granularity;
+        T sum_of_readings;
+        unsigned int * num_readings;
+};
+
+
+#endif // __CONTENTION_H__
diff -Naur gpgpu-sim-baseline/delayqueue.h gpgpu-sim/delayqueue.h
--- gpgpu-sim-baseline/delayqueue.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/delayqueue.h	2015-10-13 06:38:50.546481857 -0400
@@ -32,7 +32,7 @@
 #ifndef DELAYQUEUE_H
 #define DELAYQUEUE_H
 
-#include "../statwrapper.h"
+#include "../intersim/statwraper.h"
 #include "gpu-misc.h"
 
 template <class T>
diff -Naur gpgpu-sim-baseline/dram.cc gpgpu-sim/dram.cc
--- gpgpu-sim-baseline/dram.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/dram.cc	2015-10-13 06:38:50.558481857 -0400
@@ -163,6 +163,7 @@
    addr = mf->get_addr();
    insertion_time = (unsigned) gpu_sim_cycle;
    rw = data->get_is_write()?WRITE:READ;
+   priority = mf->m_priority;
 }
 
 void dram_t::push( class mem_fetch *data ) 
@@ -379,7 +380,7 @@
       } else {
          if (!CCDc && !RRDc && !RTWc && !WTRc && !bk[j]->RCDc && !bk[j]->RASc
              && !bk[j]->RCc && !bk[j]->RPc  && !bk[j]->RCDWRc) k--;
-         bk[j]->n_idle++;
+         bk[i]->n_idle++;
       }
    }
    if (!issued) {
@@ -421,16 +422,11 @@
 }
 
 //if mrq is being serviced by dram, gets popped after CL latency fulfilled
-class mem_fetch* dram_t::return_queue_pop() 
+class mem_fetch* dram_t::pop() 
 {
     return returnq->pop();
 }
 
-class mem_fetch* dram_t::return_queue_top() 
-{
-    return returnq->top();
-}
-
 void dram_t::print( FILE* simFile) const
 {
    unsigned i;
diff -Naur gpgpu-sim-baseline/dram.h gpgpu-sim/dram.h
--- gpgpu-sim-baseline/dram.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/dram.h	2015-10-13 06:38:50.558481857 -0400
@@ -55,6 +55,7 @@
    unsigned char rw;    //is the request a read or a write?
    unsigned long long int addr;
    unsigned int insertion_time;
+   unsigned priority;
    class mem_fetch * data;
 };
 
@@ -104,9 +105,10 @@
    unsigned int queue_limit() const;
    void visualizer_print( gzFile visualizer_file );
 
-   class mem_fetch* return_queue_pop();
-   class mem_fetch* return_queue_top();
+   class mem_fetch* pop();
    void push( class mem_fetch *data );
+   // Saugata: added function to peek at top of return queue
+   class mem_fetch * top(void) { return returnq->top(); }
    void cycle();
    void dram_log (int task);
 
diff -Naur gpgpu-sim-baseline/dram_sched.cc gpgpu-sim/dram_sched.cc
--- gpgpu-sim-baseline/dram_sched.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/dram_sched.cc	2015-10-13 06:38:50.602481858 -0400
@@ -37,8 +37,11 @@
    m_stats = stats;
    m_num_pending = 0;
    m_dram = dm;
+   chain_counter = 0;
    m_queue = new std::list<dram_req_t*>[m_config->nbk];
+   m_queue_high = new std::list<dram_req_t*>[m_config->nbk];
    m_bins = new std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >[ m_config->nbk ];
+//   m_bins_high = new std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >[ m_config->nbk ];
    m_last_row = new std::list<std::list<dram_req_t*>::iterator>*[ m_config->nbk ];
    curr_row_service_time = new unsigned[m_config->nbk];
    row_service_timestamp = new unsigned[m_config->nbk];
@@ -52,14 +55,52 @@
 
 }
 
+// Old version
+// void frfcfs_scheduler::add_req( dram_req_t *req )
+// {
+//    m_num_pending++;
+//    m_queue[req->bk].push_front(req);
+//    std::list<dram_req_t*>::iterator ptr = m_queue[req->bk].begin();
+//    m_bins[req->bk][req->row].push_front( ptr ); //newest reqs to the front
+// }
+
+//New Version
 void frfcfs_scheduler::add_req( dram_req_t *req )
 {
    m_num_pending++;
-   m_queue[req->bk].push_front(req);
-   std::list<dram_req_t*>::iterator ptr = m_queue[req->bk].begin();
-   m_bins[req->bk][req->row].push_front( ptr ); //newest reqs to the front
+   if(req->priority == 1)
+   {
+      m_stats->DRAM_high_prio++;
+      m_queue_high[req->bk].push_front(req);
+      std::list<dram_req_t*>::iterator ptr = m_queue_high[req->bk].begin();
+      m_bins[req->bk][req->row].push_front( ptr ); //newest reqs to the front
+   }
+   else
+   {
+      m_queue[req->bk].push_front(req);
+      std::list<dram_req_t*>::iterator ptr = m_queue[req->bk].begin();
+      m_bins[req->bk][req->row].push_front( ptr ); //newest reqs to the front
+   }
 }
 
+//void frfcfs_scheduler::add_req( dram_req_t *req )
+//{
+//   m_num_pending++;
+//   if(req->priority == 1)
+//   {
+//      m_stats->DRAM_high_prio++;
+//      m_queue_high[req->bk].push_front(req);
+//      std::list<dram_req_t*>::iterator ptr = m_queue_high[req->bk].begin();
+//      m_bins_high[req->bk][req->row].push_front( ptr ); //newest reqs to the front
+//   }
+//   else
+//   {
+//      m_queue[req->bk].push_front(req);
+//      std::list<dram_req_t*>::iterator ptr = m_queue[req->bk].begin();
+//      m_bins[req->bk][req->row].push_front( ptr ); //newest reqs to the front
+//   }
+//}
+
 void frfcfs_scheduler::data_collection(unsigned int bank)
 {
    if (gpu_sim_cycle > row_service_timestamp[bank]) {
@@ -76,36 +117,104 @@
    m_stats->num_activates[m_dram->id][bank]++;
 }
 
+// Old version
+// dram_req_t *frfcfs_scheduler::schedule( unsigned bank, unsigned curr_row )
+// {
+//    if ( m_last_row[bank] == NULL ) {
+//       if ( m_queue[bank].empty() )
+//          return NULL;
+// 
+//       std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >::iterator bin_ptr = m_bins[bank].find( curr_row );
+//       if ( bin_ptr == m_bins[bank].end()) {
+//          dram_req_t *req = m_queue[bank].back();
+//          bin_ptr = m_bins[bank].find( req->row );
+//          assert( bin_ptr != m_bins[bank].end() ); // where did the request go???
+//          m_last_row[bank] = &(bin_ptr->second);
+//          data_collection(bank);
+//       } else {
+//          m_last_row[bank] = &(bin_ptr->second);
+// 
+//       }
+//    }
+//    std::list<dram_req_t*>::iterator next = m_last_row[bank]->back();
+//    dram_req_t *req = (*next);
+// 
+//    m_stats->concurrent_row_access[m_dram->id][bank]++;
+//    m_stats->row_access[m_dram->id][bank]++;
+//    m_last_row[bank]->pop_back();
+// 
+//    m_queue[bank].erase(next);
+//    if ( m_last_row[bank]->empty() ) {
+//       m_bins[bank].erase( req->row );
+//       m_last_row[bank] = NULL;
+//    }
+// #ifdef DEBUG_FAST_IDEAL_SCHED
+//    if ( req )
+//       printf("%08u : DRAM(%u) scheduling memory request to bank=%u, row=%u\n", 
+//              (unsigned)gpu_sim_cycle, m_dram->id, req->bk, req->row );
+// #endif
+//    assert( req != NULL && m_num_pending != 0 ); 
+//    m_num_pending--;
+// 
+//    return req;
+// }
+
+
+//New Version
 dram_req_t *frfcfs_scheduler::schedule( unsigned bank, unsigned curr_row )
 {
-   if ( m_last_row[bank] == NULL ) {
-      if ( m_queue[bank].empty() )
-         return NULL;
-
-      std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >::iterator bin_ptr = m_bins[bank].find( curr_row );
-      if ( bin_ptr == m_bins[bank].end()) {
-         dram_req_t *req = m_queue[bank].back();
-         bin_ptr = m_bins[bank].find( req->row );
-         assert( bin_ptr != m_bins[bank].end() ); // where did the request go???
-         m_last_row[bank] = &(bin_ptr->second);
-         data_collection(bank);
-      } else {
-         m_last_row[bank] = &(bin_ptr->second);
 
-      }
-   }
-   std::list<dram_req_t*>::iterator next = m_last_row[bank]->back();
-   dram_req_t *req = (*next);
+// Rachata --> being cache aware --> Search for the high_priority queue first
+// TODO --> batch requests
 
-   m_stats->concurrent_row_access[m_dram->id][bank]++;
-   m_stats->row_access[m_dram->id][bank]++;
-   m_last_row[bank]->pop_back();
-
-   m_queue[bank].erase(next);
-   if ( m_last_row[bank]->empty() ) {
-      m_bins[bank].erase( req->row );
-      m_last_row[bank] = NULL;
-   }
+   dram_req_t *req;
+   bool fromHighPool = false;
+
+      if ( m_last_row[bank] == NULL ) {
+         // If there are anything in the high prio queue
+         if ( !m_queue_high[bank].empty() ){
+            std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >::iterator bin_ptr = m_bins[bank].find( curr_row );
+            if ( bin_ptr == m_bins[bank].end()) {
+               dram_req_t *tmp = m_queue_high[bank].back();
+               bin_ptr = m_bins[bank].find( tmp->row );
+               assert( bin_ptr != m_bins[bank].end() ); // where did the request go???
+               m_last_row[bank] = &(bin_ptr->second);
+               data_collection(bank);
+            } else {
+               m_last_row[bank] = &(bin_ptr->second);
+            }
+            fromHighPool = true;
+            m_stats->sched_from_high_prio++;
+         }
+         else{
+            if ( m_queue[bank].empty() )
+               return NULL;
+            std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >::iterator bin_ptr = m_bins[bank].find( curr_row );
+            if ( bin_ptr == m_bins[bank].end()) {
+               dram_req_t *tmp = m_queue[bank].back();
+               bin_ptr = m_bins[bank].find( tmp->row );
+               assert( bin_ptr != m_bins[bank].end() ); // where did the request go???
+               m_last_row[bank] = &(bin_ptr->second);
+               data_collection(bank);
+            } else {
+               m_last_row[bank] = &(bin_ptr->second);
+            }
+         }
+      }
+      std::list<dram_req_t*>::iterator next = m_last_row[bank]->back();
+      req = (*next);
+   
+      m_stats->concurrent_row_access[m_dram->id][bank]++;
+      m_stats->row_access[m_dram->id][bank]++;
+      m_last_row[bank]->pop_back();
+      if(fromHighPool)
+         m_queue_high[bank].erase(next);
+      else 
+         m_queue[bank].erase(next);
+      if ( m_last_row[bank]->empty() ) {
+         m_bins[bank].erase( req->row );
+         m_last_row[bank] = NULL;
+      }
 #ifdef DEBUG_FAST_IDEAL_SCHED
    if ( req )
       printf("%08u : DRAM(%u) scheduling memory request to bank=%u, row=%u\n", 
@@ -118,6 +227,73 @@
 }
 
 
+
+//dram_req_t *frfcfs_scheduler::schedule( unsigned bank, unsigned curr_row )
+//{
+//
+//// TODO: Rachata --> being cache aware --> Search for the high_priority queue first
+//
+//   dram_req_t *req;
+//   bool fromHighPool = false;
+//
+//      if ( m_last_row[bank] == NULL ) {
+//         // If there are anything in the high prio queue
+//         if ( !m_queue_high[bank].empty() ){
+//            std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >::iterator bin_ptr = m_bins_high[bank].find( curr_row );
+//            if ( bin_ptr == m_bins_high[bank].end()) {
+//               dram_req_t *tmp = m_queue_high[bank].back();
+//               bin_ptr = m_bins_high[bank].find( tmp->row );
+//               assert( bin_ptr != m_bins_high[bank].end() ); // where did the request go???
+//               m_last_row[bank] = &(bin_ptr->second);
+//               data_collection(bank);
+//            } else {
+//               m_last_row[bank] = &(bin_ptr->second);
+//            }
+//            fromHighPool = true;
+//         }
+//         else{
+//            if ( m_queue[bank].empty() )
+//               return NULL;
+//            std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >::iterator bin_ptr = m_bins[bank].find( curr_row );
+//            if ( bin_ptr == m_bins[bank].end()) {
+//               dram_req_t *tmp = m_queue[bank].back();
+//               bin_ptr = m_bins[bank].find( tmp->row );
+//               assert( bin_ptr != m_bins[bank].end() ); // where did the request go???
+//               m_last_row[bank] = &(bin_ptr->second);
+//               data_collection(bank);
+//            } else {
+//               m_last_row[bank] = &(bin_ptr->second);
+//            }
+//         }
+//      }
+//      std::list<dram_req_t*>::iterator next = m_last_row[bank]->back();
+//      req = (*next);
+//   
+//      m_stats->concurrent_row_access[m_dram->id][bank]++;
+//      m_stats->row_access[m_dram->id][bank]++;
+//      m_last_row[bank]->pop_back();
+//      if(fromHighPool)
+//         m_queue_high[bank].erase(next);
+//      else 
+//         m_queue[bank].erase(next);
+//      if ( m_last_row[bank]->empty() ) {
+//         if(fromHighPool)
+//            m_bins_high[bank].erase( req->row );
+//         else
+//            m_bins[bank].erase( req->row );
+//         m_last_row[bank] = NULL;
+//      }
+//#ifdef DEBUG_FAST_IDEAL_SCHED
+//   if ( req )
+//      printf("%08u : DRAM(%u) scheduling memory request to bank=%u, row=%u\n", 
+//             (unsigned)gpu_sim_cycle, m_dram->id, req->bk, req->row );
+//#endif
+//   assert( req != NULL && m_num_pending != 0 ); 
+//   m_num_pending--;
+//
+//   return req;
+//}
+
 void frfcfs_scheduler::print( FILE *fp )
 {
    for ( unsigned b=0; b < m_config->nbk; b++ ) {
diff -Naur gpgpu-sim-baseline/dram_sched.h gpgpu-sim/dram_sched.h
--- gpgpu-sim-baseline/dram_sched.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/dram_sched.h	2015-10-13 06:38:50.558481857 -0400
@@ -43,13 +43,16 @@
    dram_req_t *schedule( unsigned bank, unsigned curr_row );
    void print( FILE *fp );
    unsigned num_pending() const { return m_num_pending;}
+   unsigned chain_counter;
 
 private:
    const memory_config *m_config;
    dram_t *m_dram;
    unsigned m_num_pending;
    std::list<dram_req_t*>                                    *m_queue;
+   std::list<dram_req_t*>                                    *m_queue_high;
    std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >    *m_bins;
+//   std::map<unsigned,std::list<std::list<dram_req_t*>::iterator> >    *m_bins_high;
    std::list<std::list<dram_req_t*>::iterator>                 **m_last_row;
    unsigned *curr_row_service_time; //one set of variables for each bank.
    unsigned *row_service_timestamp; //tracks when scheduler began servicing current row
diff -Naur gpgpu-sim-baseline/gpu-cache.cc gpgpu-sim/gpu-cache.cc
--- gpgpu-sim-baseline/gpu-cache.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/gpu-cache.cc	2015-10-13 06:38:50.602481858 -0400
@@ -29,76 +29,17 @@
 #include "stat-tool.h"
 #include <assert.h>
 
-#define MAX_DEFAULT_CACHE_SIZE_MULTIBLIER 4
-// used to allocate memory that is large enough to adapt the changes in cache size across kernels
 
-const char * cache_request_status_str(enum cache_request_status status) 
-{
-   static const char * static_cache_request_status_str[] = {
-      "HIT",
-      "HIT_RESERVED",
-      "MISS",
-      "RESERVATION_FAIL"
-   }; 
-
-   assert(sizeof(static_cache_request_status_str) / sizeof(const char*) == NUM_CACHE_REQUEST_STATUS); 
-   assert(status < NUM_CACHE_REQUEST_STATUS); 
-
-   return static_cache_request_status_str[status]; 
-}
-
-unsigned l1d_cache_config::set_index(new_addr_type addr) const{
-    unsigned set_index = m_nset; // Default to linear set index function
-    unsigned lower_xor = 0;
-    unsigned upper_xor = 0;
-
-    switch(m_set_index_function){
-    case FERMI_HASH_SET_FUNCTION:
-        /*
-        * Set Indexing function from "A Detailed GPU Cache Model Based on Reuse Distance Theory"
-        * Cedric Nugteren et al.
-        * ISCA 2014
-        */
-        if(m_nset == 32 || m_nset == 64){
-            // Lower xor value is bits 7-11
-            lower_xor = (addr >> m_line_sz_log2) & 0x1F;
-
-            // Upper xor value is bits 13, 14, 15, 17, and 19
-            upper_xor  = (addr & 0xE000)  >> 13; // Bits 13, 14, 15
-            upper_xor |= (addr & 0x20000) >> 14; // Bit 17
-            upper_xor |= (addr & 0x80000) >> 15; // Bit 19
-
-            set_index = (lower_xor ^ upper_xor);
-
-            // 48KB cache prepends the set_index with bit 12
-            if(m_nset == 64)
-                set_index |= (addr & 0x1000) >> 7;
-
-        }else{ /* Else incorrect number of sets for the hashing function */
-            assert("\nGPGPU-Sim cache configuration error: The number of sets should be "
-                    "32 or 64 for the hashing set index function.\n" && 0);
-        }
-        break;
-
-    case CUSTOM_SET_FUNCTION:
-        /* No custom set function implemented */
-        break;
-
-    case LINEAR_SET_FUNCTION:
-        set_index = (addr >> m_line_sz_log2) & (m_nset-1);
-        break;
-    }
-
-    // Linear function selected or custom set index function not implemented
-    assert((set_index < m_nset) && "\nError: Set index out of bounds. This is caused by "
-            "an incorrect or unimplemented custom set index function.\n");
-
-    return set_index;
-}
-
-void l2_cache_config::init(linear_to_raw_address_translation *address_mapping){
-	cache_config::init(m_config_string,FuncCachePreferNone);
+// Saugata: added in number of L2 cache banks
+// void l2_cache_config::init(linear_to_raw_address_translation *address_mapping){
+void l2_cache_config::init(linear_to_raw_address_translation *address_mapping, unsigned int n_banks) {
+	cache_config::init();
 	m_address_mapping = address_mapping;
+
+    // Saugata: divide ways based on number of banks
+    assert(m_assoc >= n_banks);
+    assert(!(m_assoc & (m_assoc - 1)));
+    m_assoc /= n_banks;
 }
 
 unsigned l2_cache_config::set_index(new_addr_type addr) const{
@@ -116,7 +57,7 @@
     delete[] m_lines;
 }
 
-tag_array::tag_array( cache_config &config,
+tag_array::tag_array( const cache_config &config,
                       int core_id,
                       int type_id,
                       cache_block_t* new_lines)
@@ -126,18 +67,13 @@
     init( core_id, type_id );
 }
 
-void tag_array::update_cache_parameters(cache_config &config)
-{
-	m_config=config;
-}
-
-tag_array::tag_array( cache_config &config,
+tag_array::tag_array( const cache_config &config,
                       int core_id,
                       int type_id )
     : m_config( config )
 {
     //assert( m_config.m_write_policy == READ_ONLY ); Old assert
-    m_lines = new cache_block_t[MAX_DEFAULT_CACHE_SIZE_MULTIBLIER*config.get_num_lines()];
+    m_lines = new cache_block_t[ config.get_num_lines()];
     init( core_id, type_id );
 }
 
@@ -146,7 +82,6 @@
     m_access = 0;
     m_miss = 0;
     m_pending_hit = 0;
-    m_res_fail = 0;
     // initialize snapshot counters for visualizer
     m_prev_snapshot_access = 0;
     m_prev_snapshot_miss = 0;
@@ -155,6 +90,60 @@
     m_type_id = type_id;
 }
 
+//Check for the cache_hit stat
+enum cache_request_status tag_array::shadow_probe( new_addr_type addr ) const {
+    //assert( m_config.m_write_policy == READ_ONLY );
+    unsigned set_index = m_config.set_index(addr);
+    new_addr_type tag = m_config.tag(addr);
+
+    unsigned invalid_line = (unsigned)-1;
+    unsigned valid_line = (unsigned)-1;
+    unsigned valid_timestamp = (unsigned)-1;
+
+    bool all_reserved = true;
+
+    // check for hit or pending hit
+    for (unsigned way=0; way<m_config.m_assoc; way++) {
+        unsigned index = set_index*m_config.m_assoc+way;
+        cache_block_t *line = &m_lines[index];
+        if (line->m_tag == tag) {
+            if ( line->m_status == RESERVED ) {
+                return HIT_RESERVED;
+            } else if ( line->m_status == VALID ) {
+                return HIT;
+            } else if ( line->m_status == MODIFIED ) {
+                return HIT;
+            } else {
+                assert( line->m_status == INVALID );
+            }
+        }
+        if (line->m_status != RESERVED) {
+            all_reserved = false;
+            if (line->m_status == INVALID) {
+                invalid_line = index;
+            } else {
+                // valid line : keep track of most appropriate replacement candidate
+                if ( m_config.m_replacement_policy == LRU ) {
+                    if ( line->m_last_access_time < valid_timestamp ) {
+                        valid_timestamp = line->m_last_access_time;
+                        valid_line = index;
+                    }
+                } else if ( m_config.m_replacement_policy == FIFO ) {
+                    if ( line->m_alloc_time < valid_timestamp ) {
+                        valid_timestamp = line->m_alloc_time;
+                        valid_line = index;
+                    }
+                }
+            }
+        }
+    }
+    if ( all_reserved ) {
+        assert( m_config.m_alloc_policy == ON_MISS ); 
+        return RESERVATION_FAIL; // miss and not enough space in cache to allocate on miss
+    }
+    return MISS;
+}
+
 enum cache_request_status tag_array::probe( new_addr_type addr, unsigned &idx ) const {
     //assert( m_config.m_write_policy == READ_ONLY );
     unsigned set_index = m_config.set_index(addr);
@@ -209,6 +198,7 @@
         return RESERVATION_FAIL; // miss and not enough space in cache to allocate on miss
     }
 
+    // TODO: Modify the replacement policy
     if ( invalid_line != (unsigned)-1 ) {
         idx = invalid_line;
     } else if ( valid_line != (unsigned)-1) {
@@ -250,13 +240,9 @@
         }
         break;
     case RESERVATION_FAIL:
-        m_res_fail++;
+        m_miss++;
         shader_cache_access_log(m_core_id, m_type_id, 1); // log cache misses
         break;
-    default:
-        fprintf( stderr, "tag_array::access - Error: Unknown"
-            "cache_request_status %d\n", status );
-        abort();
     }
     return status;
 }
@@ -267,6 +253,7 @@
     unsigned idx;
     enum cache_request_status status = probe(addr,idx);
     assert(status==MISS); // MSHR should have prevented redundant memory request
+    //TODO: Put in MRU if low util --> This "time" should take care of the l2util from an earlier fn call
     m_lines[idx].allocate( m_config.tag(addr), m_config.block_addr(addr), time );
     m_lines[idx].fill(time);
 }
@@ -312,12 +299,10 @@
     total_access+=m_access;
 }
 
-void tag_array::get_stats(unsigned &total_access, unsigned &total_misses, unsigned &total_hit_res, unsigned &total_res_fail) const{
-    // Update statistics from the tag array
-    total_access    = m_access;
-    total_misses    = m_miss;
-    total_hit_res   = m_pending_hit;
-    total_res_fail  = m_res_fail;
+void tag_array::get_stats(unsigned &total_access, unsigned &total_misses) const{
+	// Get the access and miss counts from the tag array
+	total_misses = m_miss;
+	total_access = m_access;
 }
 
 
@@ -330,15 +315,6 @@
     return false;
 }
 
-bool was_writeback_sent( const std::list<cache_event> &events )
-{
-    for( std::list<cache_event>::const_iterator e=events.begin(); e!=events.end(); e++ ) {
-        if( *e == WRITE_BACK_REQUEST_SENT ) 
-            return true;
-    }
-    return false;
-}
-
 bool was_read_sent( const std::list<cache_event> &events )
 {
     for( std::list<cache_event>::const_iterator e=events.begin(); e!=events.end(); e++ ) {
@@ -415,306 +391,50 @@
     }
 }
 /***************************************************************** Caches *****************************************************************/
-cache_stats::cache_stats(){
-    m_stats.resize(NUM_MEM_ACCESS_TYPE);
-    for(unsigned i=0; i<NUM_MEM_ACCESS_TYPE; ++i){
-        m_stats[i].resize(NUM_CACHE_REQUEST_STATUS, 0);
-    }
-    m_cache_port_available_cycles = 0; 
-    m_cache_data_port_busy_cycles = 0; 
-    m_cache_fill_port_busy_cycles = 0; 
-}
-
-void cache_stats::clear(){
-    ///
-    /// Zero out all current cache statistics
-    ///
-    for(unsigned i=0; i<NUM_MEM_ACCESS_TYPE; ++i){
-        std::fill(m_stats[i].begin(), m_stats[i].end(), 0);
-    }
-    m_cache_port_available_cycles = 0; 
-    m_cache_data_port_busy_cycles = 0; 
-    m_cache_fill_port_busy_cycles = 0; 
-}
-
-void cache_stats::inc_stats(int access_type, int access_outcome){
-    ///
-    /// Increment the stat corresponding to (access_type, access_outcome) by 1.
-    ///
-    if(!check_valid(access_type, access_outcome))
-        assert(0 && "Unknown cache access type or access outcome");
-
-    m_stats[access_type][access_outcome]++;
-}
-
-enum cache_request_status cache_stats::select_stats_status(enum cache_request_status probe, enum cache_request_status access) const {
-	///
-	/// This function selects how the cache access outcome should be counted. HIT_RESERVED is considered as a MISS
-	/// in the cores, however, it should be counted as a HIT_RESERVED in the caches.
-	///
-	if(probe == HIT_RESERVED && access != RESERVATION_FAIL)
-		return probe;
-	else
-		return access;
-}
-
-unsigned &cache_stats::operator()(int access_type, int access_outcome){
-    ///
-    /// Simple method to read/modify the stat corresponding to (access_type, access_outcome)
-    /// Used overloaded () to avoid the need for separate read/write member functions
-    ///
-    if(!check_valid(access_type, access_outcome))
-        assert(0 && "Unknown cache access type or access outcome");
-
-    return m_stats[access_type][access_outcome];
-}
-
-unsigned cache_stats::operator()(int access_type, int access_outcome) const{
-    ///
-    /// Const accessor into m_stats.
-    ///
-    if(!check_valid(access_type, access_outcome))
-        assert(0 && "Unknown cache access type or access outcome");
-
-    return m_stats[access_type][access_outcome];
-}
-
-cache_stats cache_stats::operator+(const cache_stats &cs){
-    ///
-    /// Overloaded + operator to allow for simple stat accumulation
-    ///
-    cache_stats ret;
-    for(unsigned type=0; type<NUM_MEM_ACCESS_TYPE; ++type){
-        for(unsigned status=0; status<NUM_CACHE_REQUEST_STATUS; ++status){
-            ret(type, status) = m_stats[type][status] + cs(type, status);
-        }
-    }
-    ret.m_cache_port_available_cycles = m_cache_port_available_cycles + cs.m_cache_port_available_cycles; 
-    ret.m_cache_data_port_busy_cycles = m_cache_data_port_busy_cycles + cs.m_cache_data_port_busy_cycles; 
-    ret.m_cache_fill_port_busy_cycles = m_cache_fill_port_busy_cycles + cs.m_cache_fill_port_busy_cycles; 
-    return ret;
-}
-
-cache_stats &cache_stats::operator+=(const cache_stats &cs){
-    ///
-    /// Overloaded += operator to allow for simple stat accumulation
-    ///
-    for(unsigned type=0; type<NUM_MEM_ACCESS_TYPE; ++type){
-        for(unsigned status=0; status<NUM_CACHE_REQUEST_STATUS; ++status){
-            m_stats[type][status] += cs(type, status);
-        }
-    }
-    m_cache_port_available_cycles += cs.m_cache_port_available_cycles; 
-    m_cache_data_port_busy_cycles += cs.m_cache_data_port_busy_cycles; 
-    m_cache_fill_port_busy_cycles += cs.m_cache_fill_port_busy_cycles; 
-    return *this;
-}
-
-void cache_stats::print_stats(FILE *fout, const char *cache_name) const{
-    ///
-    /// Print out each non-zero cache statistic for every memory access type and status
-    /// "cache_name" defaults to "Cache_stats" when no argument is provided, otherwise
-    /// the provided name is used.
-    /// The printed format is "<cache_name>[<request_type>][<request_status>] = <stat_value>"
-    ///
-    std::string m_cache_name = cache_name;
-    for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
-        for (unsigned status = 0; status < NUM_CACHE_REQUEST_STATUS; ++status) {
-            if(m_stats[type][status] > 0){
-                fprintf(fout, "\t%s[%s][%s] = %u\n",
-                    m_cache_name.c_str(),
-                    mem_access_type_str((enum mem_access_type)type),
-                    cache_request_status_str((enum cache_request_status)status),
-                    m_stats[type][status]);
-            }
-        }
-    }
-}
-
-void cache_sub_stats::print_port_stats(FILE *fout, const char *cache_name) const
-{
-    float data_port_util = 0.0f; 
-    if (port_available_cycles > 0) {
-        data_port_util = (float) data_port_busy_cycles / port_available_cycles; 
-    }
-    fprintf(fout, "%s_data_port_util = %.3f\n", cache_name, data_port_util); 
-    float fill_port_util = 0.0f; 
-    if (port_available_cycles > 0) {
-        fill_port_util = (float) fill_port_busy_cycles / port_available_cycles; 
-    }
-    fprintf(fout, "%s_fill_port_util = %.3f\n", cache_name, fill_port_util); 
-}
-
-unsigned cache_stats::get_stats(enum mem_access_type *access_type, unsigned num_access_type, enum cache_request_status *access_status, unsigned num_access_status) const{
-    ///
-    /// Returns a sum of the stats corresponding to each "access_type" and "access_status" pair.
-    /// "access_type" is an array of "num_access_type" mem_access_types.
-    /// "access_status" is an array of "num_access_status" cache_request_statuses.
-    ///
-    unsigned total=0;
-    for(unsigned type =0; type < num_access_type; ++type){
-        for(unsigned status=0; status < num_access_status; ++status){
-            if(!check_valid((int)access_type[type], (int)access_status[status]))
-                assert(0 && "Unknown cache access type or access outcome");
-            total += m_stats[access_type[type]][access_status[status]];
-        }
-    }
-    return total;
-}
-void cache_stats::get_sub_stats(struct cache_sub_stats &css) const{
-    ///
-    /// Overwrites "css" with the appropriate statistics from this cache.
-    ///
-    struct cache_sub_stats t_css;
-    t_css.clear();
-
-    for (unsigned type = 0; type < NUM_MEM_ACCESS_TYPE; ++type) {
-        for (unsigned status = 0; status < NUM_CACHE_REQUEST_STATUS; ++status) {
-            if(status == HIT || status == MISS || status == HIT_RESERVED)
-                t_css.accesses += m_stats[type][status];
-
-            if(status == MISS)
-                t_css.misses += m_stats[type][status];
-
-            if(status == HIT_RESERVED)
-                t_css.pending_hits += m_stats[type][status];
-
-            if(status == RESERVATION_FAIL)
-                t_css.res_fails += m_stats[type][status];
-        }
-    }
-
-    t_css.port_available_cycles = m_cache_port_available_cycles; 
-    t_css.data_port_busy_cycles = m_cache_data_port_busy_cycles; 
-    t_css.fill_port_busy_cycles = m_cache_fill_port_busy_cycles; 
-
-    css = t_css;
-}
-
-bool cache_stats::check_valid(int type, int status) const{
-    ///
-    /// Verify a valid access_type/access_status
-    ///
-    if((type >= 0) && (type < NUM_MEM_ACCESS_TYPE) && (status >= 0) && (status < NUM_CACHE_REQUEST_STATUS))
-        return true;
-    else
-        return false;
-}
-
-void cache_stats::sample_cache_port_utility(bool data_port_busy, bool fill_port_busy) 
-{
-    m_cache_port_available_cycles += 1; 
-    if (data_port_busy) {
-        m_cache_data_port_busy_cycles += 1; 
-    } 
-    if (fill_port_busy) {
-        m_cache_fill_port_busy_cycles += 1; 
-    } 
-}
-
-baseline_cache::bandwidth_management::bandwidth_management(cache_config &config) 
-: m_config(config)
-{
-    m_data_port_occupied_cycles = 0; 
-    m_fill_port_occupied_cycles = 0; 
-}
-
-/// use the data port based on the outcome and events generated by the mem_fetch request 
-void baseline_cache::bandwidth_management::use_data_port(mem_fetch *mf, enum cache_request_status outcome, const std::list<cache_event> &events)
-{
-    unsigned data_size = mf->get_data_size(); 
-    unsigned port_width = m_config.m_data_port_width; 
-    switch (outcome) {
-    case HIT: {
-        unsigned data_cycles = data_size / port_width + ((data_size % port_width > 0)? 1 : 0); 
-        m_data_port_occupied_cycles += data_cycles; 
-        } break; 
-    case HIT_RESERVED: 
-    case MISS: {
-        // the data array is accessed to read out the entire line for write-back 
-        if (was_writeback_sent(events)) {
-            unsigned data_cycles = m_config.m_line_sz / port_width; 
-            m_data_port_occupied_cycles += data_cycles; 
-        }
-        } break; 
-    case RESERVATION_FAIL: 
-        // Does not consume any port bandwidth 
-        break; 
-    default: 
-        assert(0); 
-        break; 
-    } 
-}
-
-/// use the fill port 
-void baseline_cache::bandwidth_management::use_fill_port(mem_fetch *mf)
-{
-    // assume filling the entire line with the returned request 
-    unsigned fill_cycles = m_config.m_line_sz / m_config.m_data_port_width; 
-    m_fill_port_occupied_cycles += fill_cycles; 
-}
-
-/// called every cache cycle to free up the ports 
-void baseline_cache::bandwidth_management::replenish_port_bandwidth()
-{
-    if (m_data_port_occupied_cycles > 0) {
-        m_data_port_occupied_cycles -= 1; 
-    }
-    assert(m_data_port_occupied_cycles >= 0); 
-
-    if (m_fill_port_occupied_cycles > 0) {
-        m_fill_port_occupied_cycles -= 1; 
-    }
-    assert(m_fill_port_occupied_cycles >= 0); 
-}
-
-/// query for data port availability 
-bool baseline_cache::bandwidth_management::data_port_free() const
-{
-    return (m_data_port_occupied_cycles == 0); 
-}
-
-/// query for fill port availability 
-bool baseline_cache::bandwidth_management::fill_port_free() const
-{
-    return (m_fill_port_occupied_cycles == 0); 
-}
-
 /// Sends next request to lower level of memory
 void baseline_cache::cycle(){
     if ( !m_miss_queue.empty() ) {
         mem_fetch *mf = m_miss_queue.front();
-        if ( !m_memport->full(mf->size(),mf->get_is_write()) ) {
+        if ( !m_memport->full(mf->get_data_size(),mf->get_is_write()) ) {
             m_miss_queue.pop_front();
             m_memport->push(mf);
+            n_simt_to_mem+=mf->get_num_flits(true); // Interconnect power stats
         }
     }
-    bool data_port_busy = !m_bandwidth_management.data_port_free(); 
-    bool fill_port_busy = !m_bandwidth_management.fill_port_free(); 
-    m_stats.sample_cache_port_utility(data_port_busy, fill_port_busy); 
-    m_bandwidth_management.replenish_port_bandwidth(); 
 }
 
 /// Interface for response from lower memory level (model bandwidth restictions in caller)
 void baseline_cache::fill(mem_fetch *mf, unsigned time){
-    extra_mf_fields_lookup::iterator e = m_extra_mf_fields.find(mf);
-    assert( e != m_extra_mf_fields.end() );
-    assert( e->second.m_valid );
-    mf->set_data_size( e->second.m_data_size );
-    if ( m_config.m_alloc_policy == ON_MISS )
-        m_tag_array->fill(e->second.m_cache_index,time);
-    else if ( m_config.m_alloc_policy == ON_FILL )
-        m_tag_array->fill(e->second.m_block_addr,time);
-    else abort();
-    bool has_atomic = false;
-    m_mshrs.mark_ready(e->second.m_block_addr, has_atomic);
-    if (has_atomic) {
-        assert(m_config.m_alloc_policy == ON_MISS);
-        cache_block_t &block = m_tag_array->get_block(e->second.m_cache_index);
-        block.m_status = MODIFIED; // mark line as dirty for atomic operation
-    }
-    m_extra_mf_fields.erase(mf);
-    m_bandwidth_management.use_fill_port(mf); 
+    //TODO: Rachata -> check if this is ok --> fill but modify the time to factor in the util
+//    if(!mf->m_bypassed)
+//    {
+	extra_mf_fields_lookup::iterator e = m_extra_mf_fields.find(mf);
+	assert( e != m_extra_mf_fields.end() );
+	assert( e->second.m_valid );
+	mf->set_data_size( e->second.m_data_size );
+// TODO: get_l2util should, by default, be 1
+	if ( m_config.m_alloc_policy == ON_MISS ) {
+//            if(m_config.)
+		m_tag_array->fill(e->second.m_cache_index,(unsigned)((float)time*mf->get_l2util()));
+//            else
+//		m_tag_array->fill(e->second.m_cache_index,time);
+        }
+	else if ( m_config.m_alloc_policy == ON_FILL ) {
+//            if(m_config.)
+		m_tag_array->fill(e->second.m_block_addr,(unsigned)((float)time*mf->get_l2util()));
+//            else
+//		m_tag_array->fill(e->second.m_block_addr,time);
+        }
+	else abort();
+	bool has_atomic = false;
+	m_mshrs.mark_ready(e->second.m_block_addr, has_atomic);
+	if (has_atomic) {
+		assert(m_config.m_alloc_policy == ON_MISS);
+		cache_block_t &block = m_tag_array->get_block(e->second.m_cache_index);
+		block.m_status = MODIFIED; // mark line as dirty for atomic operation
+	}
+	m_extra_mf_fields.erase(mf);
+//    }
 }
 
 /// Checks if mf is waiting to be filled by lower memory level
@@ -792,6 +512,7 @@
 	cache_block_t &block = m_tag_array->get_block(cache_index);
 	block.m_status = MODIFIED;
 
+	m_write_access++;
 	return HIT;
 }
 
@@ -808,6 +529,7 @@
 	// generate a write-through
 	send_write_request(mf, WRITE_REQUEST_SENT, time, events);
 
+	m_write_access++;
 	return HIT;
 }
 
@@ -823,6 +545,7 @@
 	// Invalidate block
 	block.m_status = INVALID;
 
+	m_write_access++;
 	return HIT;
 }
 
@@ -837,275 +560,210 @@
 
 /****** Write-miss functions (Set by config file) ******/
 
-/// Write-allocate miss: Send write request to lower level memory
-// and send a read request for the same block
-enum cache_request_status
-data_cache::wr_miss_wa( new_addr_type addr,
-                        unsigned cache_index, mem_fetch *mf,
-                        unsigned time, std::list<cache_event> &events,
-                        enum cache_request_status status )
-{
-    new_addr_type block_addr = m_config.block_addr(addr);
-
-    // Write allocate, maximum 3 requests (write miss, read request, write back request)
-    // Conservatively ensure the worst-case request can be handled this cycle
-    bool mshr_hit = m_mshrs.probe(block_addr);
-    bool mshr_avail = !m_mshrs.full(block_addr);
-    if(miss_queue_full(2) 
-        || (!(mshr_hit && mshr_avail) 
-        && !(!mshr_hit && mshr_avail 
-        && (m_miss_queue.size() < m_config.m_miss_queue_size))))
-        return RESERVATION_FAIL;
+/// Write-allocate miss: Send write request to lower level memory and send a read request for the same block
+enum cache_request_status data_cache::wr_miss_wa(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status) {
 
-    send_write_request(mf, WRITE_REQUEST_SENT, time, events);
-    // Tries to send write allocate request, returns true on success and false on failure
-    //if(!send_write_allocate(mf, addr, block_addr, cache_index, time, events))
-    //    return RESERVATION_FAIL;
-
-    const mem_access_t *ma = new  mem_access_t( m_wr_alloc_type,
-                        mf->get_addr(),
-                        mf->get_data_size(),
-                        false, // Now performing a read
-                        mf->get_access_warp_mask(),
-                        mf->get_access_byte_mask() );
-
-    mem_fetch *n_mf = new mem_fetch( *ma,
-                    NULL,
-                    mf->get_ctrl_size(),
-                    mf->get_wid(),
-                    mf->get_sid(),
-                    mf->get_tpc(),
-                    mf->get_mem_config());
+	new_addr_type block_addr = m_config.block_addr(addr);
 
-    bool do_miss = false;
-    bool wb = false;
-    cache_block_t evicted;
+	// Write allocate, maximum 3 requests (write miss, read request, write back request)
+	// Conservatively ensure the worst-case request can be handled this cycle
+	bool mshr_hit = m_mshrs.probe(block_addr);
+	bool mshr_avail = !m_mshrs.full(block_addr);
+	if(miss_queue_full(2) || (!(mshr_hit && mshr_avail) && !(!mshr_hit && mshr_avail && (m_miss_queue.size() < m_config.m_miss_queue_size))))
+		return RESERVATION_FAIL;
 
-    // Send read request resulting from write miss
-    send_read_request(addr, block_addr, cache_index, n_mf, time, do_miss, wb,
-        evicted, events, false, true);
-
-    if( do_miss ){
-        // If evicted block is modified and not a write-through
-        // (already modified lower level)
-        if( wb && (m_config.m_write_policy != WRITE_THROUGH) ) { 
-            mem_fetch *wb = m_memfetch_creator->alloc(evicted.m_block_addr,
-                m_wrbk_type,m_config.get_line_sz(),true);
-            m_miss_queue.push_back(wb);
-            wb->set_status(m_miss_queue_status,time);
-        }
-        return MISS;
-    }
+	send_write_request(mf, WRITE_REQUEST_SENT, time, events);
+	// Tries to send write allocate request, returns true on success and false on failure
+	//if(!send_write_allocate(mf, addr, block_addr, cache_index, time, events))
+	//	return RESERVATION_FAIL;
+
+	const mem_access_t *ma = new  mem_access_t( L2_WR_ALLOC_R,
+						mf->get_addr(),
+						mf->get_data_size(),
+						false, // Now performing a read
+						mf->get_access_warp_mask(),
+						mf->get_access_byte_mask() );
+
+	mem_fetch *n_mf = new mem_fetch( *ma,
+					NULL,
+					mf->get_ctrl_size(),
+					mf->get_wid(),
+					mf->get_sid(),
+					mf->get_tpc(),
+					mf->get_tid(),
+					mf->get_mem_config());
+
+	bool do_miss = false;
+	bool wb = false;
+	cache_block_t evicted;
+
+	// Send read request resulting from write miss
+	send_read_request(addr, block_addr, cache_index, n_mf, time, do_miss, wb, evicted, events, false, true);
+
+	if( wb && (m_config.m_write_policy != WRITE_THROUGH) ) { // If evicted block is modified and not a write-through (already modified lower level)
+		mem_fetch *wb = m_memfetch_creator->alloc(evicted.m_block_addr,L2_WRBK_ACC,m_config.get_line_sz(),true);
+		m_miss_queue.push_back(wb);
+		wb->set_status(m_miss_queue_status,time);
+	}
+	if( do_miss ){
+		m_write_access++;
+		m_write_miss++;
+		return MISS;
+	}
 
-    return RESERVATION_FAIL;
+	return RESERVATION_FAIL;
 }
 
 /// No write-allocate miss: Simply send write request to lower level memory
-enum cache_request_status
-data_cache::wr_miss_no_wa( new_addr_type addr,
-                           unsigned cache_index,
-                           mem_fetch *mf,
-                           unsigned time,
-                           std::list<cache_event> &events,
-                           enum cache_request_status status )
-{
-    if(miss_queue_full(0))
-        return RESERVATION_FAIL; // cannot handle request this cycle
+enum cache_request_status data_cache::wr_miss_no_wa(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status ){
+	if(miss_queue_full(0))
+		return RESERVATION_FAIL; // cannot handle request this cycle
 
-    // on miss, generate write through (no write buffering -- too many threads for that)
-    send_write_request(mf, WRITE_REQUEST_SENT, time, events);
+	// on miss, generate write through (no write buffering -- too many threads for that)
+	send_write_request(mf, WRITE_REQUEST_SENT, time, events);
 
-    return MISS;
+	m_write_access++;
+	m_write_miss++;
+	return MISS;
 }
 
 /****** Read hit functions (Set by config file) ******/
 
-/// Baseline read hit: Update LRU status of block.
-// Special case for atomic instructions -> Mark block as modified
-enum cache_request_status
-data_cache::rd_hit_base( new_addr_type addr,
-                         unsigned cache_index,
-                         mem_fetch *mf,
-                         unsigned time,
-                         std::list<cache_event> &events,
-                         enum cache_request_status status )
-{
-    new_addr_type block_addr = m_config.block_addr(addr);
-    m_tag_array->access(block_addr,time,cache_index);
-    // Atomics treated as global read/write requests - Perform read, mark line as
-    // MODIFIED
-    if(mf->isatomic()){ 
-        assert(mf->get_access_type() == GLOBAL_ACC_R);
-        cache_block_t &block = m_tag_array->get_block(cache_index);
+/// Baseline read hit: Update LRU status of block. Special case for atomic instructions -> Mark block as modified
+enum cache_request_status data_cache::rd_hit_base(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status ){
+	new_addr_type block_addr = m_config.block_addr(addr);
+	m_tag_array->access(block_addr,time,cache_index);
+	if(mf->isatomic()){ // Atomics treated as global read/write requests - Perform read, mark line as MODIFIED
+		assert(mf->get_access_type() == GLOBAL_ACC_R);
+		cache_block_t &block = m_tag_array->get_block(cache_index);
         block.m_status = MODIFIED;  // mark line as dirty
-    }
-    return HIT;
+	}
+
+	m_read_access++;
+	return HIT;
 }
 
 /****** Read miss functions (Set by config file) ******/
 
-/// Baseline read miss: Send read request to lower level memory,
-// perform write-back as necessary
-enum cache_request_status
-data_cache::rd_miss_base( new_addr_type addr,
-                          unsigned cache_index,
-                          mem_fetch *mf,
-                          unsigned time,
-                          std::list<cache_event> &events,
-                          enum cache_request_status status ){
-    if(miss_queue_full(1))
-        // cannot handle request this cycle
-        // (might need to generate two requests)
-        return RESERVATION_FAIL; 
+/// Baseline read miss: Send read request to lower level memory, perform write-back as necessary
+enum cache_request_status data_cache::rd_miss_base(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status ){
+	if(miss_queue_full(1))
+		return RESERVATION_FAIL; // cannot handle request this cycle (might need to generate two requests)
 
-    new_addr_type block_addr = m_config.block_addr(addr);
-    bool do_miss = false;
-    bool wb = false;
-    cache_block_t evicted;
-    send_read_request( addr,
-                       block_addr,
-                       cache_index,
-                       mf, time, do_miss, wb, evicted, events, false, false);
-
-    if( do_miss ){
-        // If evicted block is modified and not a write-through
-        // (already modified lower level)
-        if(wb && (m_config.m_write_policy != WRITE_THROUGH) ){ 
-            mem_fetch *wb = m_memfetch_creator->alloc(evicted.m_block_addr,
-                m_wrbk_type,m_config.get_line_sz(),true);
-        send_write_request(wb, WRITE_BACK_REQUEST_SENT, time, events);
-    }
-        return MISS;
-    }
-    return RESERVATION_FAIL;
+	new_addr_type block_addr = m_config.block_addr(addr);
+	bool do_miss = false;
+	bool wb = false;
+	cache_block_t evicted;
+	send_read_request(addr, block_addr, cache_index, mf, time, do_miss, wb, evicted, events, false, false);
+
+	if(wb && (m_config.m_write_policy != WRITE_THROUGH) ){ // If evicted block is modified and not a write-through (already modified lower level)
+		mem_fetch *wb = m_memfetch_creator->alloc(evicted.m_block_addr, L1_WRBK_ACC,m_config.get_line_sz(),true);
+		send_write_request(wb, WRITE_BACK_REQUEST_SENT, time, events);
+	}
+	if( do_miss ){
+		m_read_access++;
+		m_read_miss++;
+		return MISS;
+	}
+	return RESERVATION_FAIL;
 }
 
-/// Access cache for read_only_cache: returns RESERVATION_FAIL if
-// request could not be accepted (for any reason)
-enum cache_request_status
-read_only_cache::access( new_addr_type addr,
-                         mem_fetch *mf,
-                         unsigned time,
-                         std::list<cache_event> &events )
-{
-    assert( mf->get_data_size() <= m_config.get_line_sz());
-    assert(m_config.m_write_policy == READ_ONLY);
-    assert(!mf->get_is_write());
-    new_addr_type block_addr = m_config.block_addr(addr);
-    unsigned cache_index = (unsigned)-1;
-    enum cache_request_status status = m_tag_array->probe(block_addr,cache_index);
-    enum cache_request_status cache_status = RESERVATION_FAIL;
-
-    if ( status == HIT ) {
-        cache_status = m_tag_array->access(block_addr,time,cache_index); // update LRU state
-    }else if ( status != RESERVATION_FAIL ) {
-        if(!miss_queue_full(0)){
-            bool do_miss=false;
-            send_read_request(addr, block_addr, cache_index, mf, time, do_miss, events, true, false);
-            if(do_miss)
-                cache_status = MISS;
-            else
-                cache_status = RESERVATION_FAIL;
-        }else{
-            cache_status = RESERVATION_FAIL;
-        }
-    }
-
-    m_stats.inc_stats(mf->get_access_type(), m_stats.select_stats_status(status, cache_status));
-    return cache_status;
+/// Access cache for read_only_cache: returns RESERVATION_FAIL if request could not be accepted (for any reason)
+enum cache_request_status read_only_cache::access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events ) {
+	assert( mf->get_data_size() <= m_config.get_line_sz());
+	assert(m_config.m_write_policy == READ_ONLY);
+	assert(!mf->get_is_write());
+	new_addr_type block_addr = m_config.block_addr(addr);
+	unsigned cache_index = (unsigned)-1;
+	enum cache_request_status status = m_tag_array->probe(block_addr,cache_index);
+	if ( status == HIT ) {
+		m_tag_array->access(block_addr,time,cache_index); // update LRU state
+		return HIT;
+	}else if ( status != RESERVATION_FAIL ) {
+		if(!miss_queue_full(0)){
+			bool do_miss=false;
+			send_read_request(addr, block_addr, cache_index, mf, time, do_miss, events, true, false);
+			if(do_miss)
+				return MISS;
+		}
+	}
+	return RESERVATION_FAIL;
 }
 
-//! A general function that takes the result of a tag_array probe
-//  and performs the correspding functions based on the cache configuration
-//  The access fucntion calls this function
-enum cache_request_status
-data_cache::process_tag_probe( bool wr,
-                               enum cache_request_status probe_status,
-                               new_addr_type addr,
-                               unsigned cache_index,
-                               mem_fetch* mf,
-                               unsigned time,
-                               std::list<cache_event>& events )
-{
-    // Each function pointer ( m_[rd/wr]_[hit/miss] ) is set in the
-    // data_cache constructor to reflect the corresponding cache configuration
-    // options. Function pointers were used to avoid many long conditional
-    // branches resulting from many cache configuration options.
-    cache_request_status access_status = probe_status;
-    if(wr){ // Write
-        if(probe_status == HIT){
-            access_status = (this->*m_wr_hit)( addr,
-                                      cache_index,
-                                      mf, time, events, probe_status );
-        }else if ( probe_status != RESERVATION_FAIL ) {
-            access_status = (this->*m_wr_miss)( addr,
-                                       cache_index,
-                                       mf, time, events, probe_status );
-        }
-    }else{ // Read
-        if(probe_status == HIT){
-            access_status = (this->*m_rd_hit)( addr,
-                                      cache_index,
-                                      mf, time, events, probe_status );
-        }else if ( probe_status != RESERVATION_FAIL ) {
-            access_status = (this->*m_rd_miss)( addr,
-                                       cache_index,
-                                       mf, time, events, probe_status );
-        }
-    }
+/// This is meant to model the first level data cache in Fermi.
+/// It is write-evict (global) or write-back (local) at the granularity of individual blocks (Set by GPGPU-Sim configuration file)
+/// (the policy used in fermi according to the CUDA manual)
+enum cache_request_status l1_cache::access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events ){
+
+	assert( mf->get_data_size() <= m_config.get_line_sz());
+	bool wr = mf->get_is_write();
+	new_addr_type block_addr = m_config.block_addr(addr);
+	unsigned cache_index = (unsigned)-1;
+	enum cache_request_status status = m_tag_array->probe(block_addr,cache_index);
 
-    m_bandwidth_management.use_data_port(mf, access_status, events); 
-    return access_status;
+	// Each function pointer ( m_[rd/wr]_[hit/miss] ) is set in the data_cache constructor to reflect the corresponding cache configuration options.
+	// Function pointers were used to avoid many long conditional branches resulting from many cache configuration options.
+	if(wr){	// Write
+		if(status == HIT){
+			return (this->*m_wr_hit)(addr, cache_index, mf, time, events, status);
+		}else if ( status != RESERVATION_FAIL ) {
+			return (this->*m_wr_miss)(addr, cache_index,  mf, time, events, status);
+		}
+	}else{ // Read
+		if(status == HIT){
+			return (this->*m_rd_hit)(addr, cache_index,  mf, time, events, status);
+		}else if ( status != RESERVATION_FAIL ) {
+			return (this->*m_rd_miss)(addr, cache_index,  mf, time, events, status);
+		}
+	}
+	return RESERVATION_FAIL;
 }
 
-// Both the L1 and L2 currently use the same access function.
-// Differentiation between the two caches is done through configuration
-// of caching policies.
-// Both the L1 and L2 override this function to provide a means of
-// performing actions specific to each cache when such actions are implemnted.
-enum cache_request_status
-data_cache::access( new_addr_type addr,
-                    mem_fetch *mf,
-                    unsigned time,
-                    std::list<cache_event> &events )
-{
+/// Models second level shared cache with global write-back and write-allocate policies
+/// Currently the same as l1_cache, but separated to allow for different implementations
+enum cache_request_status l2_cache::access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events ){
 
-    assert( mf->get_data_size() <= m_config.get_line_sz());
-    bool wr = mf->get_is_write();
-    new_addr_type block_addr = m_config.block_addr(addr);
-    unsigned cache_index = (unsigned)-1;
-    enum cache_request_status probe_status
-        = m_tag_array->probe( block_addr, cache_index );
-    enum cache_request_status access_status
-        = process_tag_probe( wr, probe_status, addr, cache_index, mf, time, events );
-    m_stats.inc_stats(mf->get_access_type(),
-        m_stats.select_stats_status(probe_status, access_status));
-    return access_status;
-}
+	assert( mf->get_data_size() <= m_config.get_line_sz());
+	bool wr = mf->get_is_write();
+	new_addr_type block_addr = m_config.block_addr(addr);
+	unsigned cache_index = (unsigned)-1;
+	enum cache_request_status status = m_tag_array->probe(block_addr,cache_index);
 
-/// This is meant to model the first level data cache in Fermi.
-/// It is write-evict (global) or write-back (local) at the
-/// granularity of individual blocks (Set by GPGPU-Sim configuration file)
-/// (the policy used in fermi according to the CUDA manual)
-enum cache_request_status
-l1_cache::access( new_addr_type addr,
-                  mem_fetch *mf,
-                  unsigned time,
-                  std::list<cache_event> &events )
-{
-    return data_cache::access( addr, mf, time, events );
+	// Each function pointer ( m_[rd/wr]_[hit/miss] ) is set in the data_cache constructor to reflect the corresponding cache configuration options.
+	// Function pointers were used to avoid many long conditional branches resulting from many cache configuration options.
+        // Rachata: TODO: Check if I can just do this or not
+	if(wr){	// Write
+		if(status == HIT){
+			return (this->*m_wr_hit)(addr, cache_index,  mf, (unsigned)(mf->get_l2util()*(float)time), events, status);
+			// return (this->*m_wr_hit)(addr, cache_index,  mf, (unsigned)((1-mf->get_l2util())*(float)time), events, status);
+			// return (this->*m_wr_hit)(addr, cache_index,  mf, time, events, status);
+		}else if ( status != RESERVATION_FAIL ) {
+			return (this->*m_wr_miss)(addr, cache_index,  mf, (unsigned)(mf->get_l2util()*(float)time), events, status);
+			// return (this->*m_wr_miss)(addr, cache_index,  mf, (unsigned)((1-mf->get_l2util())*(float)time), events, status);
+			// return (this->*m_wr_miss)(addr, cache_index,  mf, time, events, status);
+		}
+	}else{ // Read
+		if(status == HIT){
+			return (this->*m_rd_hit)(addr, cache_index,  mf, (unsigned)(mf->get_l2util()*(float)time), events, status);
+			// return (this->*m_rd_hit)(addr, cache_index,  mf, (unsigned)((1-mf->get_l2util())*(float)time), events, status);
+			// return (this->*m_rd_hit)(addr, cache_index,  mf, time, events, status);
+		}else if ( status != RESERVATION_FAIL ) {
+			return (this->*m_rd_miss)(addr, cache_index,  mf, (unsigned)(mf->get_l2util()*(float)time), events, status);
+			// return (this->*m_rd_miss)(addr, cache_index,  mf, (unsigned)((1-mf->get_l2util())*(float)time), events, status);
+			// return (this->*m_rd_miss)(addr, cache_index,  mf, time, events, status);
+		}
+	}
+	return RESERVATION_FAIL;
 }
 
-// The l2 cache access function calls the base data_cache access
-// implementation.  When the L2 needs to diverge from L1, L2 specific
-// changes should be made here.
-enum cache_request_status
-l2_cache::access( new_addr_type addr,
-                  mem_fetch *mf,
-                  unsigned time,
-                  std::list<cache_event> &events )
-{
-    return data_cache::access( addr, mf, time, events );
+/// Shadow second level shared cache with global write-back and write-allocate policies for stat collections
+/// Currently the same as l1_cache, but separated to allow for different implementations
+enum cache_request_status l2_cache::shadow_access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events ){
+
+	assert( mf->get_data_size() <= m_config.get_line_sz());
+	bool wr = mf->get_is_write();
+	new_addr_type block_addr = m_config.block_addr(addr);
+	return m_tag_array->shadow_probe(block_addr);
 }
 
 /// Access function for tex_cache
@@ -1113,9 +771,7 @@
 /// otherwise returns HIT_RESERVED or MISS; NOTE: *never* returns HIT
 /// since unlike a normal CPU cache, a "HIT" in texture cache does not
 /// mean the data is ready (still need to get through fragment fifo)
-enum cache_request_status tex_cache::access( new_addr_type addr, mem_fetch *mf,
-    unsigned time, std::list<cache_event> &events )
-{
+enum cache_request_status tex_cache::access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events ) {
     if ( m_fragment_fifo.full() || m_request_fifo.full() || m_rob.full() )
         return RESERVATION_FAIL;
 
@@ -1125,7 +781,6 @@
     new_addr_type block_addr = m_config.block_addr(addr);
     unsigned cache_index = (unsigned)-1;
     enum cache_request_status status = m_tags.access(block_addr,time,cache_index);
-    enum cache_request_status cache_status = RESERVATION_FAIL;
     assert( status != RESERVATION_FAIL );
     assert( status != HIT_RESERVED ); // as far as tags are concerned: HIT or MISS
     m_fragment_fifo.push( fragment_entry(mf,cache_index,status==MISS,mf->get_data_size()) );
@@ -1138,13 +793,11 @@
         m_request_fifo.push(mf);
         mf->set_status(m_request_queue_status,time);
         events.push_back(READ_REQUEST_SENT);
-        cache_status = MISS;
+        return MISS;
     } else {
         // the value *will* *be* in the cache already
-        cache_status = HIT_RESERVED;
+        return HIT_RESERVED;
     }
-    m_stats.inc_stats(mf->get_access_type(), m_stats.select_stats_status(status, cache_status));
-    return cache_status;
 }
 
 void tex_cache::cycle(){
@@ -1154,6 +807,7 @@
         if ( !m_memport->full(mf->get_ctrl_size(),false) ) {
             m_request_fifo.pop();
             m_memport->push(mf);
+            n_simt_to_mem+=mf->get_num_flits(true); // Interconnect power stats
         }
     }
     // read ready lines from cache
@@ -1176,8 +830,7 @@
         } else {
             // hit:
             assert( m_cache[e.m_cache_index].m_valid );
-            assert( m_cache[e.m_cache_index].m_block_addr
-                == m_config.block_addr(e.m_request->get_addr()) );
+            assert( m_cache[e.m_cache_index].m_block_addr = m_config.block_addr(e.m_request->get_addr()) );
             m_result_fifo.push( e.m_request );
             m_fragment_fifo.pop();
         }
@@ -1204,19 +857,15 @@
 void tex_cache::display_state( FILE *fp ) const
 {
     fprintf(fp,"%s (texture cache) state:\n", m_name.c_str() );
-    fprintf(fp,"fragment fifo entries  = %u / %u\n",
-        m_fragment_fifo.size(), m_fragment_fifo.capacity() );
-    fprintf(fp,"reorder buffer entries = %u / %u\n",
-        m_rob.size(), m_rob.capacity() );
-    fprintf(fp,"request fifo entries   = %u / %u\n",
-        m_request_fifo.size(), m_request_fifo.capacity() );
+    fprintf(fp,"fragment fifo entries  = %u / %u\n", m_fragment_fifo.size(), m_fragment_fifo.capacity() );
+    fprintf(fp,"reorder buffer entries = %u / %u\n", m_rob.size(), m_rob.capacity() );
+    fprintf(fp,"request fifo entries   = %u / %u\n", m_request_fifo.size(), m_request_fifo.capacity() );
     if ( !m_rob.empty() )
         fprintf(fp,"reorder buffer contents:\n");
     for ( int n=m_rob.size()-1; n>=0; n-- ) {
         unsigned index = (m_rob.next_pop_index() + n)%m_rob.capacity();
         const rob_entry &r = m_rob.peek(index);
-        fprintf(fp, "tex rob[%3d] : %s ",
-            index, (r.m_ready?"ready  ":"pending") );
+        fprintf(fp, "tex rob[%3d] : %s ", index, (r.m_ready?"ready  ":"pending") );
         if ( r.m_ready )
             fprintf(fp,"@%6u", r.m_time );
         else
diff -Naur gpgpu-sim-baseline/gpu-cache.h gpgpu-sim/gpu-cache.h
--- gpgpu-sim-baseline/gpu-cache.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/gpu-cache.h	2015-10-13 06:38:50.534481857 -0400
@@ -45,11 +45,10 @@
 };
 
 enum cache_request_status {
-    HIT = 0,
+    HIT,
     HIT_RESERVED,
     MISS,
-    RESERVATION_FAIL, 
-    NUM_CACHE_REQUEST_STATUS
+    RESERVATION_FAIL
 };
 
 enum cache_event {
@@ -58,8 +57,6 @@
     WRITE_REQUEST_SENT
 };
 
-const char * cache_request_status_str(enum cache_request_status status); 
-
 struct cache_block_t {
     cache_block_t()
     {
@@ -69,6 +66,7 @@
         m_fill_time=0;
         m_last_access_time=0;
         m_status=INVALID;
+        m_l2util = 1;
     }
     void allocate( new_addr_type tag, new_addr_type block_addr, unsigned time )
     {
@@ -78,12 +76,14 @@
         m_last_access_time=time;
         m_fill_time=0;
         m_status=RESERVED;
+        m_l2util = 1;
     }
     void fill( unsigned time )
     {
         assert( m_status == RESERVED );
         m_status=VALID;
         m_fill_time=time;
+        m_l2util = 1;
     }
 
     new_addr_type    m_tag;
@@ -92,6 +92,8 @@
     unsigned         m_last_access_time;
     unsigned         m_fill_time;
     cache_block_state    m_status;
+    // Rachata -> Not being used at the moment
+    float            m_l2util;
 };
 
 enum replacement_policy_t {
@@ -123,11 +125,6 @@
     ASSOC // normal cache 
 };
 
-enum set_index_function{
-    FERMI_HASH_SET_FUNCTION = 0,
-    LINEAR_SET_FUNCTION,
-    CUSTOM_SET_FUNCTION
-};
 
 class cache_config {
 public:
@@ -136,26 +133,19 @@
         m_valid = false; 
         m_disabled = false;
         m_config_string = NULL; // set by option parser
-        m_config_stringPrefL1 = NULL;
-        m_config_stringPrefShared = NULL;
-        m_data_port_width = 0;
-        m_set_index_function = LINEAR_SET_FUNCTION;
     }
-    void init(char * config, FuncCache status)
+    void init()
     {
-    	cache_status= status;
-        assert( config );
-        char rp, wp, ap, mshr_type, wap, sif;
+        assert( m_config_string );
+        char rp, wp, ap, mshr_type, wap;
 
-
-        int ntok = sscanf(config,"%u:%u:%u,%c:%c:%c:%c:%c,%c:%u:%u,%u:%u,%u",
+        int ntok = sscanf(m_config_string,"%u:%u:%u,%c:%c:%c:%c,%c:%u:%u,%u:%u",
                           &m_nset, &m_line_sz, &m_assoc, &rp, &wp, &ap, &wap,
-                          &sif,&mshr_type,&m_mshr_entries,&m_mshr_max_merge,
-                          &m_miss_queue_size, &m_result_fifo_entries,
-                          &m_data_port_width);
+                          &mshr_type, &m_mshr_entries,&m_mshr_max_merge,
+                          &m_miss_queue_size,&m_result_fifo_entries);
 
-        if ( ntok < 11 ) {
-            if ( !strcmp(config,"none") ) {
+        if ( ntok < 10 ) {
+            if ( !strcmp(m_config_string,"none") ) {
                 m_disabled = true;
                 return;
             }
@@ -180,7 +170,7 @@
         default: exit_parse_error();
         }
         switch (mshr_type) {
-        case 'F': m_mshr_type = TEX_FIFO; assert(ntok==13); break;
+        case 'F': m_mshr_type = TEX_FIFO; assert(ntok==12); break;
         case 'A': m_mshr_type = ASSOC; break;
         default: exit_parse_error();
         }
@@ -207,19 +197,6 @@
             // are stalling each other.  
             assert(0 && "Invalid cache configuration: Writeback cache cannot allocate new line on fill. "); 
         }
-
-        // default: port to data array width and granularity = line size 
-        if (m_data_port_width == 0) {
-            m_data_port_width = m_line_sz; 
-        }
-        assert(m_line_sz % m_data_port_width == 0); 
-
-        switch(sif){
-        case 'H': m_set_index_function = FERMI_HASH_SET_FUNCTION; break;
-        case 'C': m_set_index_function = CUSTOM_SET_FUNCTION; break;
-        case 'L': m_set_index_function = LINEAR_SET_FUNCTION; break;
-        default: exit_parse_error();
-        }
     }
     bool disabled() const { return m_disabled;}
     unsigned get_line_sz() const
@@ -242,34 +219,24 @@
 
     virtual unsigned set_index( new_addr_type addr ) const
     {
-        if(m_set_index_function != LINEAR_SET_FUNCTION){
-            printf("\nGPGPU-Sim cache configuration error: Hashing or "
-                    "custom set index function selected in configuration "
-                    "file for a cache that has not overloaded the set_index "
-                    "function\n");
-            abort();
-        }
         return(addr >> m_line_sz_log2) & (m_nset-1);
     }
 
     new_addr_type tag( new_addr_type addr ) const
     {
-        // For generality, the tag includes both index and tag. This allows for more complex set index
-        // calculations that can result in different indexes mapping to the same set, thus the full
-        // tag + index is required to check for hit/miss. Tag is now identical to the block address.
+    	// For generality, the tag includes both index and tag. This allows for more complex set index
+    	// calculations that can result in different indexes mapping to the same set, thus the full
+    	// tag + index is required to check for hit/miss. Tag is now identical to the block address.
 
         //return addr >> (m_line_sz_log2+m_nset_log2);
-        return addr & ~(m_line_sz-1);
+    	return addr & ~(m_line_sz-1);
     }
     new_addr_type block_addr( new_addr_type addr ) const
     {
         return addr & ~(m_line_sz-1);
     }
-    FuncCache get_cache_status() {return cache_status;}
+
     char *m_config_string;
-    char *m_config_stringPrefL1;
-    char *m_config_stringPrefShared;
-    FuncCache cache_status;
 
 protected:
     void exit_parse_error()
@@ -306,8 +273,6 @@
         unsigned m_rob_entries;
     };
     unsigned m_result_fifo_entries;
-    unsigned m_data_port_width; //< number of byte the cache can access per cycle 
-    enum set_index_function m_set_index_function; // Hash, linear, or custom set index function
 
     friend class tag_array;
     friend class baseline_cache;
@@ -318,16 +283,13 @@
     friend class l2_cache;
 };
 
-class l1d_cache_config : public cache_config{
-public:
-	l1d_cache_config() : cache_config(){}
-	virtual unsigned set_index(new_addr_type addr) const;
-};
 
 class l2_cache_config : public cache_config {
 public:
 	l2_cache_config() : cache_config(){}
-	void init(linear_to_raw_address_translation *address_mapping);
+    // Saugata: adding number of banks as argument
+	// void init(linear_to_raw_address_translation *address_mapping);
+	void init(linear_to_raw_address_translation *address_mapping, unsigned int n_banks);
 	virtual unsigned set_index(new_addr_type addr) const;
 
 private:
@@ -337,10 +299,11 @@
 class tag_array {
 public:
     // Use this constructor
-    tag_array(cache_config &config, int core_id, int type_id );
+    tag_array( const cache_config &config, int core_id, int type_id );
     ~tag_array();
 
     enum cache_request_status probe( new_addr_type addr, unsigned &idx ) const;
+    enum cache_request_status shadow_probe( new_addr_type addr ) const;
     enum cache_request_status access( new_addr_type addr, unsigned time, unsigned &idx );
     enum cache_request_status access( new_addr_type addr, unsigned time, unsigned &idx, bool &wb, cache_block_t &evicted );
 
@@ -355,14 +318,13 @@
 
     void print( FILE *stream, unsigned &total_access, unsigned &total_misses ) const;
     float windowed_miss_rate( ) const;
-    void get_stats(unsigned &total_access, unsigned &total_misses, unsigned &total_hit_res, unsigned &total_res_fail) const;
+    void get_stats(unsigned &total_access, unsigned &total_misses) const;
 
-	void update_cache_parameters(cache_config &config);
 protected:
     // This constructor is intended for use only from derived classes that wish to
     // avoid unnecessary memory allocation that takes place in the
     // other tag_array constructor
-    tag_array( cache_config &config,
+    tag_array( const cache_config &config,
                int core_id,
                int type_id,
                cache_block_t* new_lines );
@@ -370,14 +332,13 @@
 
 protected:
 
-    cache_config &m_config;
+    const cache_config &m_config;
 
     cache_block_t *m_lines; /* nbanks x nset x assoc lines in total */
 
     unsigned m_access;
     unsigned m_miss;
     unsigned m_pending_hit; // number of cache miss that hit a line that is allocated but not filled
-    unsigned m_res_fail;
 
     // performance counters for calculating the amount of misses within a time window
     unsigned m_prev_snapshot_access;
@@ -388,6 +349,7 @@
     int m_type_id; // what kind of cache is this (normal, texture, constant)
 };
 
+
 class mshr_table {
 public:
     mshr_table( unsigned num_entries, unsigned max_merged )
@@ -415,12 +377,6 @@
     mem_fetch *next_access();
     void display( FILE *fp ) const;
 
-    void check_mshr_parameters( unsigned num_entries, unsigned max_merged )
-    {
-    	assert(m_num_entries==num_entries && "Change of MSHR parameters between kernels is not allowed");
-    	assert(m_max_merged==max_merged && "Change of MSHR parameters between kernels is not allowed");
-    }
-
 private:
 
     // finite sized, fully associative table, with a finite maximum number of merged requests
@@ -442,103 +398,11 @@
 
 
 /***************************************************************** Caches *****************************************************************/
-///
-/// Simple struct to maintain cache accesses, misses, pending hits, and reservation fails.
-///
-struct cache_sub_stats{
-    unsigned accesses;
-    unsigned misses;
-    unsigned pending_hits;
-    unsigned res_fails;
-
-    unsigned long long port_available_cycles; 
-    unsigned long long data_port_busy_cycles; 
-    unsigned long long fill_port_busy_cycles; 
-
-    cache_sub_stats(){
-        clear();
-    }
-    void clear(){
-        accesses = 0;
-        misses = 0;
-        pending_hits = 0;
-        res_fails = 0;
-        port_available_cycles = 0; 
-        data_port_busy_cycles = 0; 
-        fill_port_busy_cycles = 0; 
-    }
-    cache_sub_stats &operator+=(const cache_sub_stats &css){
-        ///
-        /// Overloading += operator to easily accumulate stats
-        ///
-        accesses += css.accesses;
-        misses += css.misses;
-        pending_hits += css.pending_hits;
-        res_fails += css.res_fails;
-        port_available_cycles += css.port_available_cycles; 
-        data_port_busy_cycles += css.data_port_busy_cycles; 
-        fill_port_busy_cycles += css.fill_port_busy_cycles; 
-        return *this;
-    }
-
-    cache_sub_stats operator+(const cache_sub_stats &cs){
-        ///
-        /// Overloading + operator to easily accumulate stats
-        ///
-        cache_sub_stats ret;
-        ret.accesses = accesses + cs.accesses;
-        ret.misses = misses + cs.misses;
-        ret.pending_hits = pending_hits + cs.pending_hits;
-        ret.res_fails = res_fails + cs.res_fails;
-        ret.port_available_cycles = port_available_cycles + cs.port_available_cycles; 
-        ret.data_port_busy_cycles = data_port_busy_cycles + cs.data_port_busy_cycles; 
-        ret.fill_port_busy_cycles = fill_port_busy_cycles + cs.fill_port_busy_cycles; 
-        return ret;
-    }
-
-    void print_port_stats(FILE *fout, const char *cache_name) const; 
-};
-
-///
-/// Cache_stats
-/// Used to record statistics for each cache.
-/// Maintains a record of every 'mem_access_type' and its resulting
-/// 'cache_request_status' : [mem_access_type][cache_request_status]
-///
-class cache_stats {
-public:
-    cache_stats();
-    void clear();
-    void inc_stats(int access_type, int access_outcome);
-    enum cache_request_status select_stats_status(enum cache_request_status probe, enum cache_request_status access) const;
-    unsigned &operator()(int access_type, int access_outcome);
-    unsigned operator()(int access_type, int access_outcome) const;
-    cache_stats operator+(const cache_stats &cs);
-    cache_stats &operator+=(const cache_stats &cs);
-    void print_stats(FILE *fout, const char *cache_name = "Cache_stats") const;
-
-    unsigned get_stats(enum mem_access_type *access_type, unsigned num_access_type, enum cache_request_status *access_status, unsigned num_access_status)  const;
-    void get_sub_stats(struct cache_sub_stats &css) const;
-
-    void sample_cache_port_utility(bool data_port_busy, bool fill_port_busy); 
-private:
-    bool check_valid(int type, int status) const;
-
-    std::vector< std::vector<unsigned> > m_stats;
-
-    unsigned long long m_cache_port_available_cycles; 
-    unsigned long long m_cache_data_port_busy_cycles; 
-    unsigned long long m_cache_fill_port_busy_cycles; 
-};
 
 class cache_t {
 public:
     virtual ~cache_t() {}
     virtual enum cache_request_status access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events ) =  0;
-
-    // accessors for cache bandwidth availability 
-    virtual bool data_port_free() const = 0; 
-    virtual bool fill_port_free() const = 0; 
 };
 
 bool was_write_sent( const std::list<cache_event> &events );
@@ -549,11 +413,9 @@
 /// Each subclass implements its own 'access' function
 class baseline_cache : public cache_t {
 public:
-    baseline_cache( const char *name, cache_config &config, int core_id, int type_id, mem_fetch_interface *memport,
+    baseline_cache( const char *name, const cache_config &config, int core_id, int type_id, mem_fetch_interface *memport,
                      enum mem_fetch_status status )
-    : m_config(config), m_tag_array(new tag_array(config,core_id,type_id)), 
-      m_mshrs(config.m_mshr_entries,config.m_mshr_max_merge), 
-      m_bandwidth_management(config) 
+    : m_config(config), m_tag_array(new tag_array(config,core_id,type_id)), m_mshrs(config.m_mshr_entries,config.m_mshr_max_merge)
     {
         init( name, config, memport, status );
     }
@@ -567,6 +429,11 @@
         assert(config.m_mshr_type == ASSOC);
         m_memport=memport;
         m_miss_queue_status = status;
+        m_read_access=0;
+        m_write_access=0;
+        m_read_miss=0;
+        m_write_miss=0;
+        n_simt_to_mem=0;
     }
 
     virtual ~baseline_cache()
@@ -574,13 +441,6 @@
         delete m_tag_array;
     }
 
-	void update_cache_parameters(cache_config &config)
-	{
-		m_config=config;
-		m_tag_array->update_cache_parameters(config);
-		m_mshrs.check_mshr_parameters(config.m_mshr_entries,config.m_mshr_max_merge);
-	}
-
     virtual enum cache_request_status access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events ) =  0;
     /// Sends next request to lower level of memory
     void cycle();
@@ -597,25 +457,25 @@
     void print(FILE *fp, unsigned &accesses, unsigned &misses) const;
     void display_state( FILE *fp ) const;
 
-    // Stat collection
-    const cache_stats &get_stats() const {
-        return m_stats;
+    virtual void get_data_stats(unsigned &read_access, unsigned &read_misses,unsigned &write_access,  unsigned &write_misses) const {
+    	read_access = m_read_access;
+    	write_access = m_write_access;
+    	read_misses = m_read_miss;
+    	write_misses = m_write_miss;
     }
-    unsigned get_stats(enum mem_access_type *access_type, unsigned num_access_type, enum cache_request_status *access_status, unsigned num_access_status)  const{
-        return m_stats.get_stats(access_type, num_access_type, access_status, num_access_status);
-    }
-    void get_sub_stats(struct cache_sub_stats &css) const {
-        m_stats.get_sub_stats(css);
+
+    void get_stats(unsigned &accesses, unsigned &misses) const {
+    	m_tag_array->get_stats(accesses, misses);
     }
 
-    // accessors for cache bandwidth availability 
-    bool data_port_free() const { return m_bandwidth_management.data_port_free(); } 
-    bool fill_port_free() const { return m_bandwidth_management.fill_port_free(); } 
+    void set_icnt_power_stats(unsigned &simt_to_mem) const{
+    	simt_to_mem = n_simt_to_mem;
+    }
 
 protected:
     // Constructor that can be used by derived classes with custom tag arrays
     baseline_cache( const char *name,
-                    cache_config &config,
+                    const cache_config &config,
                     int core_id,
                     int type_id,
                     mem_fetch_interface *memport,
@@ -623,15 +483,14 @@
                     tag_array* new_tag_array )
     : m_config(config),
       m_tag_array( new_tag_array ),
-      m_mshrs(config.m_mshr_entries,config.m_mshr_max_merge), 
-      m_bandwidth_management(config) 
+      m_mshrs(config.m_mshr_entries,config.m_mshr_max_merge)
     {
         init( name, config, memport, status );
     }
 
 protected:
     std::string m_name;
-    cache_config &m_config;
+    const cache_config &m_config;
     tag_array*  m_tag_array;
     mshr_table m_mshrs;
     std::list<mem_fetch*> m_miss_queue;
@@ -657,8 +516,6 @@
 
     extra_mf_fields_lookup m_extra_mf_fields;
 
-    cache_stats m_stats;
-
     /// Checks whether this request can be handled on this cycle. num_miss equals max # of misses to be handled on this cycle
     bool miss_queue_full(unsigned num_miss){
     	  return ( (m_miss_queue.size()+num_miss) >= m_config.m_miss_queue_size );
@@ -670,39 +527,19 @@
     void send_read_request(new_addr_type addr, new_addr_type block_addr, unsigned cache_index, mem_fetch *mf,
     		unsigned time, bool &do_miss, bool &wb, cache_block_t &evicted, std::list<cache_event> &events, bool read_only, bool wa);
 
-    /// Sub-class containing all metadata for port bandwidth management 
-    class bandwidth_management 
-    {
-    public: 
-        bandwidth_management(cache_config &config); 
-
-        /// use the data port based on the outcome and events generated by the mem_fetch request 
-        void use_data_port(mem_fetch *mf, enum cache_request_status outcome, const std::list<cache_event> &events); 
+    // Power stats
+    unsigned m_read_access;
+    unsigned m_write_access;
+    unsigned m_read_miss;
+    unsigned m_write_miss;
 
-        /// use the fill port 
-        void use_fill_port(mem_fetch *mf); 
-
-        /// called every cache cycle to free up the ports 
-        void replenish_port_bandwidth(); 
-
-        /// query for data port availability 
-        bool data_port_free() const; 
-        /// query for fill port availability 
-        bool fill_port_free() const; 
-    protected: 
-        const cache_config &m_config; 
-
-        int m_data_port_occupied_cycles; //< Number of cycle that the data port remains used 
-        int m_fill_port_occupied_cycles; //< Number of cycle that the fill port remains used 
-    }; 
-
-    bandwidth_management m_bandwidth_management; 
+    unsigned n_simt_to_mem; // Interconnect power stats
 };
 
 /// Read only cache
 class read_only_cache : public baseline_cache {
 public:
-    read_only_cache( const char *name, cache_config &config, int core_id, int type_id, mem_fetch_interface *memport, enum mem_fetch_status status )
+    read_only_cache( const char *name, const cache_config &config, int core_id, int type_id, mem_fetch_interface *memport, enum mem_fetch_status status )
     : baseline_cache(name,config,core_id,type_id,memport,status){}
 
     /// Access cache for read_only_cache: returns RESERVATION_FAIL if request could not be accepted (for any reason)
@@ -711,27 +548,24 @@
     virtual ~read_only_cache(){}
 
 protected:
-    read_only_cache( const char *name, cache_config &config, int core_id, int type_id, mem_fetch_interface *memport, enum mem_fetch_status status, tag_array* new_tag_array )
+    read_only_cache( const char *name, const cache_config &config, int core_id, int type_id, mem_fetch_interface *memport, enum mem_fetch_status status, tag_array* new_tag_array )
     : baseline_cache(name,config,core_id,type_id,memport,status, new_tag_array){}
 };
 
 /// Data cache - Implements common functions for L1 and L2 data cache
 class data_cache : public baseline_cache {
 public:
-    data_cache( const char *name, cache_config &config,
+    data_cache( const char *name, const cache_config &config,
     			int core_id, int type_id, mem_fetch_interface *memport,
-                mem_fetch_allocator *mfcreator, enum mem_fetch_status status,
-                mem_access_type wr_alloc_type, mem_access_type wrbk_type )
+                mem_fetch_allocator *mfcreator, enum mem_fetch_status status )
     			: baseline_cache(name,config,core_id,type_id,memport,status)
     {
         init( mfcreator );
-        m_wr_alloc_type = wr_alloc_type;
-        m_wrbk_type = wrbk_type;
     }
 
     virtual ~data_cache() {}
 
-    virtual void init( mem_fetch_allocator *mfcreator )
+    void init( mem_fetch_allocator *mfcreator )
     {
         m_memfetch_creator=mfcreator;
 
@@ -743,235 +577,115 @@
 
         // Set write hit function
         switch(m_config.m_write_policy){
-        // READ_ONLY is now a separate cache class, config is deprecated
-        case READ_ONLY:
-            assert(0 && "Error: Writable Data_cache set as READ_ONLY\n");
-            break; 
+        case READ_ONLY: assert(0 && "Error: Writable Data_cache set as READ_ONLY\n"); break; // READ_ONLY is now a separate cache class, config is deprecated
         case WRITE_BACK: m_wr_hit = &data_cache::wr_hit_wb; break;
         case WRITE_THROUGH: m_wr_hit = &data_cache::wr_hit_wt; break;
         case WRITE_EVICT: m_wr_hit = &data_cache::wr_hit_we; break;
-        case LOCAL_WB_GLOBAL_WT:
-            m_wr_hit = &data_cache::wr_hit_global_we_local_wb;
-            break;
-        default:
-            assert(0 && "Error: Must set valid cache write policy\n");
-            break; // Need to set a write hit function
+        case LOCAL_WB_GLOBAL_WT: m_wr_hit = &data_cache::wr_hit_global_we_local_wb; break;
+        default: assert(0 && "Error: Must set valid cache write policy\n"); break; // Need to set a write hit function
         }
 
         // Set write miss function
         switch(m_config.m_write_alloc_policy){
         case WRITE_ALLOCATE: m_wr_miss = &data_cache::wr_miss_wa; break;
         case NO_WRITE_ALLOCATE: m_wr_miss = &data_cache::wr_miss_no_wa; break;
-        default:
-            assert(0 && "Error: Must set valid cache write miss policy\n");
-            break; // Need to set a write miss function
+        default: assert(0 && "Error: Must set valid cache write miss policy\n"); break; // Need to set a write miss function
         }
     }
 
-    virtual enum cache_request_status access( new_addr_type addr,
-                                              mem_fetch *mf,
-                                              unsigned time,
-                                              std::list<cache_event> &events );
 protected:
     data_cache( const char *name,
-                cache_config &config,
-                int core_id,
+                const cache_config &config,
+    			int core_id,
                 int type_id,
                 mem_fetch_interface *memport,
                 mem_fetch_allocator *mfcreator,
                 enum mem_fetch_status status,
-                tag_array* new_tag_array,
-                mem_access_type wr_alloc_type,
-                mem_access_type wrbk_type)
+                tag_array* new_tag_array )
     : baseline_cache(name, config, core_id, type_id, memport,status, new_tag_array)
     {
         init( mfcreator );
-        m_wr_alloc_type = wr_alloc_type;
-        m_wrbk_type = wrbk_type;
     }
 
-    mem_access_type m_wr_alloc_type; // Specifies type of write allocate request (e.g., L1 or L2)
-    mem_access_type m_wrbk_type; // Specifies type of writeback request (e.g., L1 or L2)
-
-    //! A general function that takes the result of a tag_array probe
-    //  and performs the correspding functions based on the cache configuration
-    //  The access fucntion calls this function
-    enum cache_request_status
-        process_tag_probe( bool wr,
-                           enum cache_request_status status,
-                           new_addr_type addr,
-                           unsigned cache_index,
-                           mem_fetch* mf,
-                           unsigned time,
-                           std::list<cache_event>& events );
-
 protected:
     mem_fetch_allocator *m_memfetch_creator;
 
     // Functions for data cache access
     /// Sends write request to lower level memory (write or writeback)
-    void send_write_request( mem_fetch *mf,
-                             cache_event request,
-                             unsigned time,
-                             std::list<cache_event> &events);
+    void send_write_request(mem_fetch *mf, cache_event request, unsigned time, std::list<cache_event> &events);
+
 
-    // Member Function pointers - Set by configuration options
-    // to the functions below each grouping
+    // Member Function pointers - Set by configuration options to the functions below each grouping
     /******* Write-hit configs *******/
-    enum cache_request_status
-        (data_cache::*m_wr_hit)( new_addr_type addr,
-                                 unsigned cache_index,
-                                 mem_fetch *mf,
-                                 unsigned time,
-                                 std::list<cache_event> &events,
-                                 enum cache_request_status status );
+    enum cache_request_status (data_cache::*m_wr_hit)(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status);
     /// Marks block as MODIFIED and updates block LRU
-    enum cache_request_status
-        wr_hit_wb( new_addr_type addr,
-                   unsigned cache_index,
-                   mem_fetch *mf,
-                   unsigned time,
-                   std::list<cache_event> &events,
-                   enum cache_request_status status ); // write-back
-    enum cache_request_status
-        wr_hit_wt( new_addr_type addr,
-                   unsigned cache_index,
-                   mem_fetch *mf,
-                   unsigned time,
-                   std::list<cache_event> &events,
-                   enum cache_request_status status ); // write-through
-
+    enum cache_request_status wr_hit_wb(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status); // write-back
+    enum cache_request_status wr_hit_wt(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status); // write-through
     /// Marks block as INVALID and sends write request to lower level memory
-    enum cache_request_status
-        wr_hit_we( new_addr_type addr,
-                   unsigned cache_index,
-                   mem_fetch *mf,
-                   unsigned time,
-                   std::list<cache_event> &events,
-                   enum cache_request_status status ); // write-evict
-    enum cache_request_status
-        wr_hit_global_we_local_wb( new_addr_type addr,
-                                   unsigned cache_index,
-                                   mem_fetch *mf,
-                                   unsigned time,
-                                   std::list<cache_event> &events,
-                                   enum cache_request_status status );
-        // global write-evict, local write-back
+    enum cache_request_status wr_hit_we(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status); // write-evict
+    enum cache_request_status wr_hit_global_we_local_wb(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status); // global write-evict, local write-back
 
 
     /******* Write-miss configs *******/
-    enum cache_request_status
-        (data_cache::*m_wr_miss)( new_addr_type addr,
-                                  unsigned cache_index,
-                                  mem_fetch *mf,
-                                  unsigned time,
-                                  std::list<cache_event> &events,
-                                  enum cache_request_status status );
-    /// Sends read request, and possible write-back request,
-    //  to lower level memory for a write miss with write-allocate
-    enum cache_request_status
-        wr_miss_wa( new_addr_type addr,
-                    unsigned cache_index,
-                    mem_fetch *mf,
-                    unsigned time,
-                    std::list<cache_event> &events,
-                    enum cache_request_status status ); // write-allocate
-    enum cache_request_status
-        wr_miss_no_wa( new_addr_type addr,
-                       unsigned cache_index,
-                       mem_fetch *mf,
-                       unsigned time,
-                       std::list<cache_event> &events,
-                       enum cache_request_status status ); // no write-allocate
+    enum cache_request_status (data_cache::*m_wr_miss)(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status);
+    /// Sends read request, and possible write-back request, to lower level memory for a write miss with write-allocate
+    enum cache_request_status wr_miss_wa(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status); // write-allocate
+    enum cache_request_status wr_miss_no_wa(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status); // no write-allocate
 
     // Currently no separate functions for reads
     /******* Read-hit configs *******/
-    enum cache_request_status
-        (data_cache::*m_rd_hit)( new_addr_type addr,
-                                 unsigned cache_index,
-                                 mem_fetch *mf,
-                                 unsigned time,
-                                 std::list<cache_event> &events,
-                                 enum cache_request_status status );
-    enum cache_request_status
-        rd_hit_base( new_addr_type addr,
-                     unsigned cache_index,
-                     mem_fetch *mf,
-                     unsigned time,
-                     std::list<cache_event> &events,
-                     enum cache_request_status status );
+    enum cache_request_status (data_cache::*m_rd_hit)(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status);
+    enum cache_request_status rd_hit_base(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status);
 
     /******* Read-miss configs *******/
-    enum cache_request_status
-        (data_cache::*m_rd_miss)( new_addr_type addr,
-                                  unsigned cache_index,
-                                  mem_fetch *mf,
-                                  unsigned time,
-                                  std::list<cache_event> &events,
-                                  enum cache_request_status status );
-    enum cache_request_status
-        rd_miss_base( new_addr_type addr,
-                      unsigned cache_index,
-                      mem_fetch*mf,
-                      unsigned time,
-                      std::list<cache_event> &events,
-                      enum cache_request_status status );
+    enum cache_request_status (data_cache::*m_rd_miss)(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status);
+    enum cache_request_status rd_miss_base(new_addr_type addr, unsigned cache_index, mem_fetch *mf, unsigned time, std::list<cache_event> &events, enum cache_request_status status);
 
 };
 
 /// This is meant to model the first level data cache in Fermi.
-/// It is write-evict (global) or write-back (local) at
-/// the granularity of individual blocks
+/// It is write-evict (global) or write-back (local) at the granularity of individual blocks
 /// (the policy used in fermi according to the CUDA manual)
 class l1_cache : public data_cache {
 public:
-    l1_cache(const char *name, cache_config &config,
-            int core_id, int type_id, mem_fetch_interface *memport,
+	l1_cache(const char *name, const cache_config &config,
+			int core_id, int type_id, mem_fetch_interface *memport,
             mem_fetch_allocator *mfcreator, enum mem_fetch_status status )
-            : data_cache(name,config,core_id,type_id,memport,mfcreator,status, L1_WR_ALLOC_R, L1_WRBK_ACC){}
+			: data_cache(name,config,core_id,type_id,memport,mfcreator,status){}
 
     virtual ~l1_cache(){}
 
-    virtual enum cache_request_status
-        access( new_addr_type addr,
-                mem_fetch *mf,
-                unsigned time,
-                std::list<cache_event> &events );
+	virtual enum cache_request_status access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events );
 
 protected:
-    l1_cache( const char *name,
-              cache_config &config,
-              int core_id,
+	l1_cache( const char *name,
+              const cache_config &config,
+			  int core_id,
               int type_id,
               mem_fetch_interface *memport,
               mem_fetch_allocator *mfcreator,
               enum mem_fetch_status status,
               tag_array* new_tag_array )
-    : data_cache( name,
-                  config,
-                  core_id,type_id,memport,mfcreator,status, new_tag_array, L1_WR_ALLOC_R, L1_WRBK_ACC ){}
+	: data_cache(name,config,core_id,type_id,memport,mfcreator,status, new_tag_array){}
 
 };
 
-/// Models second level shared cache with global write-back
-/// and write-allocate policies
+/// Models second level shared cache with global write-back and write-allocate policies
 class l2_cache : public data_cache {
 public:
-    l2_cache(const char *name,  cache_config &config,
-            int core_id, int type_id, mem_fetch_interface *memport,
+	l2_cache(const char *name, const cache_config &config,
+			int core_id, int type_id, mem_fetch_interface *memport,
             mem_fetch_allocator *mfcreator, enum mem_fetch_status status )
-            : data_cache(name,config,core_id,type_id,memport,mfcreator,status, L2_WR_ALLOC_R, L2_WRBK_ACC){}
+			: data_cache(name,config,core_id,type_id,memport,mfcreator,status){}
 
     virtual ~l2_cache() {}
 
-    virtual enum cache_request_status
-        access( new_addr_type addr,
-                mem_fetch *mf,
-                unsigned time,
-                std::list<cache_event> &events );
+	virtual enum cache_request_status access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events );
+	enum cache_request_status shadow_access( new_addr_type addr, mem_fetch *mf, unsigned time, std::list<cache_event> &events );
+
 };
 
-/*****************************************************************************/
+/********************************************************************************************************************************************************/
 
 // See the following paper to understand this cache model:
 // 
@@ -980,7 +694,7 @@
 // http://www-graphics.stanford.edu/papers/texture_prefetch/
 class tex_cache : public cache_t {
 public:
-    tex_cache( const char *name, cache_config &config, int core_id, int type_id, mem_fetch_interface *memport,
+    tex_cache( const char *name, const cache_config &config, int core_id, int type_id, mem_fetch_interface *memport,
                enum mem_fetch_status request_status, 
                enum mem_fetch_status rob_status )
     : m_config(config), 
@@ -998,6 +712,7 @@
         m_cache = new data_block[ config.get_num_lines() ];
         m_request_queue_status = request_status;
         m_rob_status = rob_status;
+        n_simt_to_mem=0;
     }
 
     /// Access function for tex_cache
@@ -1015,22 +730,15 @@
     mem_fetch *next_access(){return m_result_fifo.pop();}
     void display_state( FILE *fp ) const;
 
-    // accessors for cache bandwidth availability - stubs for now 
-    bool data_port_free() const { return true; }
-    bool fill_port_free() const { return true; }
-
-    // Stat collection
-    const cache_stats &get_stats() const {
-        return m_stats;
-    }
-    unsigned get_stats(enum mem_access_type *access_type, unsigned num_access_type, enum cache_request_status *access_status, unsigned num_access_status) const{
-        return m_stats.get_stats(access_type, num_access_type, access_status, num_access_status);
+    void get_stats(unsigned &accesses, unsigned &misses) const{
+    	m_tags.get_stats(accesses, misses);
     }
 
-    void get_sub_stats(struct cache_sub_stats &css) const{
-        m_stats.get_sub_stats(css);
+    void set_icnt_power_stats(unsigned &simt_to_mem) const{
+    	simt_to_mem = n_simt_to_mem;
     }
-private:
+
+    private:
     std::string m_name;
     const cache_config &m_config;
 
@@ -1153,11 +861,12 @@
         unsigned m_rob_index;
     };
 
-    cache_stats m_stats;
-
     typedef std::map<mem_fetch*,extra_mf_fields> extra_mf_fields_lookup;
 
     extra_mf_fields_lookup m_extra_mf_fields;
+
+    // Interconnect power stats
+    unsigned n_simt_to_mem;
 };
 
 #endif
diff -Naur gpgpu-sim-baseline/gpu-sim.cc gpgpu-sim/gpu-sim.cc
--- gpgpu-sim-baseline/gpu-sim.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/gpu-sim.cc	2015-10-13 06:38:50.546481857 -0400
@@ -51,7 +51,8 @@
 #include "l2cache.h"
 
 #include "../cuda-sim/ptx-stats.h"
-#include "../statwrapper.h"
+#include "../intersim/statwraper.h"
+#include "../intersim/interconnect_interface.h"
 #include "../abstract_hardware_model.h"
 #include "../debug.h"
 #include "../gpgpusim_entrypoint.h"
@@ -85,6 +86,7 @@
 
 // performance counter for stalls due to congestion.
 unsigned int gpu_stall_dramfull = 0; 
+unsigned int gpu_stall_dramportbound = 0; // Saugata: tallies when ports are inadequate
 unsigned int gpu_stall_icnt2sh = 0;
 
 /* Clock Domains */
@@ -158,8 +160,21 @@
     option_parser_register(opp, "-gpgpu_n_mem", OPT_UINT32, &m_n_mem, 
                  "number of memory modules (e.g. memory controllers) in gpu",
                  "8");
-    option_parser_register(opp, "-gpgpu_n_sub_partition_per_mchannel", OPT_UINT32, &m_n_sub_partition_per_memory_channel, 
-                 "number of memory subpartition in each memory module",
+    // Saugata: added option to set the number of L2 pipeline stages
+    option_parser_register(opp, "-gpgpu_n_L2_pipeline_stages", OPT_UINT32, &m_n_L2_pipeline_stages,
+                 "number of pipeline stages in L2 cache bank",
+                 "1");
+    // Saugata: added option to set the number of input ports to each L2 cache bank
+    option_parser_register(opp, "-gpgpu_n_L2_bank_ports", OPT_UINT32, &m_n_L2_bank_ports,
+                 "number of input ports for each L2 cache bank",
+                 "1");
+    // Saugata: added option to set number of L2 cache banks per MPU
+    option_parser_register(opp, "-gpgpu_n_L2_banks_per_mchannel", OPT_UINT32, &m_n_L2_banks_per_memory_channel,
+                 "number of L2 cache banks in each memory module",
+                 "1");
+    // Saugata: added option to set number of input ports per MPU
+    option_parser_register(opp, "-gpgpu_n_ports_per_mchannel", OPT_UINT32, &m_n_ports_per_memory_channel,
+                 "number of input ports for each memory module",
                  "1");
     option_parser_register(opp, "-gpgpu_n_mem_per_ctrlr", OPT_UINT32, &gpu_n_mem_per_ctrlr, 
                  "number of memory chips per memory controller",
@@ -188,10 +203,111 @@
     option_parser_register(opp, "-rop_latency", OPT_UINT32, &rop_latency,
                      "ROP queue latency (default 85)",
                      "85");
+    // Saugata: added option to set L2 cache latency
+    option_parser_register(opp, "-l2_latency", OPT_UINT32, &L2_latency,
+                     "L2 cache access latency (default 10)",
+                     "10");
     option_parser_register(opp, "-dram_latency", OPT_UINT32, &dram_latency,
                      "DRAM latency (default 30)",
                      "30");
 
+    // Rachata
+    option_parser_register(opp, "-gpgpu_cache:cache_aware", OPT_BOOL, &cache_aware, 
+                           "DRAM is cache aware",
+                           "0");
+    option_parser_register(opp, "-gpgpu_cache:bypass_PCAL", OPT_BOOL, &l2_bypass_PCAL, 
+                           "PCAL bypassing policy", "0");
+ 
+    option_parser_register(opp, "-gpgpu_cache:PCAL_num_tokens", OPT_UINT32, &l2_bypass_PCAL_num_tokens, 
+                           "Number of tokens in PCAL", "128");
+ 
+    option_parser_register(opp, "-gpgpu_cache:bypass_rop", OPT_BOOL, &bypass_rop, 
+                           "DRAM is cache aware",
+                           "0");
+
+    option_parser_register(opp, "-gpgpu_cache:cache_aware_insertion", OPT_INT32, &cache_aware_insertion, 
+                           "DRAM is cache aware",
+                           "0");
+
+    option_parser_register(opp, "-gpgpu_cache:reuse_aware", OPT_BOOL, &reuse_aware, 
+                           "DRAM is reuse aware",
+                           "0");
+
+    // Rachata: EAF insertion policy
+    option_parser_register(opp, "-gpgpu_cache:eaf_insertion", OPT_BOOL, &eaf_insertion, 
+                           "DRAM is reuse aware",
+                           "0");
+
+
+    option_parser_register(opp, "-gpgpu_cache:prioritize_mf", OPT_BOOL, &prioritize_mf, 
+                           "DRAM is cache aware",
+                           "0");
+
+    option_parser_register(opp, "-gpgpu_cache:l2_warp_hit_ratio", OPT_FLOAT, &target_warp_hit_ratio, 
+                           "hit ratio for each warp",
+                           "0");
+
+    option_parser_register(opp, "-gpgpu_cache:l2_warp_hit_ratio_high", OPT_FLOAT, &target_warp_hit_ratio_high, 
+                           "hit ratio for each warp",
+                           "1");
+
+    option_parser_register(opp, "-gpgpu_cache:l2_warp_info_clear", OPT_UINT32, &l2_info_clear, 
+                           "Gap time between each clear() in l2_warp_info",
+                           "10000");
+
+    option_parser_register(opp, "-gpgpu_cache:l2_warmup", OPT_UINT32, &l2_warmup, 
+                           "Number of cache access before cache_aware kicks in",
+                           "100");
+
+    option_parser_register(opp, "-gpgpu_cache:l2_reuse_threshold", OPT_FLOAT, &l2_reuse_threshold, 
+                           "Number of accessed before automatically try to cache",
+                           "0.01");
+
+    option_parser_register(opp, "-gpgpu_cache:l2_bloom_filter_size", OPT_UINT32, &l2_bloom_filter_size, 
+                           "Size of the bloom filter at L2",
+                           "512");
+
+    // Rachata: Bypass based on PC
+    option_parser_register(opp, "-gpgpu_cache:l2_bypass_pc", OPT_BOOL, &l2_bypass_pc, 
+                           "Enables request bypassing of L2 cache based on PC",
+                           "0");
+
+    option_parser_register(opp, "-gpgpu_cache:l2_bypass_pc_range", OPT_UINT32, &l2_bypass_pc_range, 
+                           "The range of PC that we will keep track of (size of the table)",
+                           "65535");
+
+    option_parser_register(opp, "-gpgpu_cache:l2_bypass_combined", OPT_BOOL, &l2_bypass_combined, 
+                           "Enables request bypassing of L2 cache based on PC",
+                           "0");
+
+
+
+    // Saugata: added options to enable random cache bypassing
+    option_parser_register(opp, "-gpgpu_cache:l2_bypass_randomly", OPT_BOOL, &l2_bypass_randomly, 
+                           "Enables random request bypassing of L2 cache",
+                           "0");
+    option_parser_register(opp, "-gpgpu_cache:l2_random_bypass_rate", OPT_FLOAT, &l2_random_bypass_rate, 
+                           "Percentage of L2 requests that are randomly bypassed",
+                           "0.1");
+
+    // Saugata: added an option to perform bypassing after a request travels through the request queue
+    option_parser_register(opp, "-gpgpu_cache:l2_bypass_after_queue", OPT_BOOL, &l2_bypass_after_queue, 
+                           "Only performs L2 bypassing after a request travels through the request queue",
+                           "0");
+
+    // Saugata: added an option to enable warp message printing
+    option_parser_register(opp, "-gpgpu_cache:l2_print_warp_hit_rates", OPT_BOOL, &l2_print_warp_hit_rates, 
+                           "Enables a periodic message printing warp hit rates in the L2 cache",
+                           "0");
+
+    // Saugata: added an option to enable divergence message printing
+    option_parser_register(opp, "-gpgpu_cache:l2_print_divergence_stats", OPT_BOOL, &l2_print_divergence_stats, 
+                           "Enables a periodic message printing divergence stats in the L2 cache",
+                           "0");
+
+    option_parser_register(opp, "-l2_ideal", OPT_BOOL, &l2_ideal, 
+                           "Use a ideal L2 cache that always hit",
+                           "0");
     m_address_mapping.addrdec_setoption(opp);
 }
 
@@ -214,22 +330,10 @@
                    "shader L1 instruction cache config "
                    " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} ",
                    "4:256:4,L:R:f:N,A:2:32,4" );
-    option_parser_register(opp, "-gpgpu_cache:dl1", OPT_CSTR, &m_L1D_config.m_config_string,
+    option_parser_register(opp, "-gpgpu_cache:dl1", OPT_CSTR, &m_L1D_config.m_config_string, 
                    "per-shader L1 data cache config "
                    " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
                    "none" );
-    option_parser_register(opp, "-gpgpu_cache:dl1PrefL1", OPT_CSTR, &m_L1D_config.m_config_stringPrefL1,
-                   "per-shader L1 data cache config "
-                   " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
-                   "none" );
-    option_parser_register(opp, "-gpgpu_cache:dl1PreShared", OPT_CSTR, &m_L1D_config.m_config_stringPrefShared,
-                   "per-shader L1 data cache config "
-                   " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
-                   "none" );
-    option_parser_register(opp, "-gmem_skip_L1D", OPT_BOOL, &gmem_skip_L1D, 
-                   "global memory access skip L1D cache (implements -Xptxas -dlcm=cg, default=no skip)",
-                   "0");
-
     option_parser_register(opp, "-gpgpu_perfect_mem", OPT_BOOL, &gpgpu_perfect_mem, 
                  "enable perfect memory mode (no cache miss)",
                  "0");
@@ -248,9 +352,6 @@
     option_parser_register(opp, "-gpgpu_shader_cta", OPT_UINT32, &max_cta_per_core, 
                  "Maximum number of concurrent CTAs in shader (default 8)",
                  "8");
-    option_parser_register(opp, "-gpgpu_num_cta_barriers", OPT_UINT32, &max_barriers_per_cta,
-                 "Maximum number of named barriers per CTA (default 16)",
-                 "16");
     option_parser_register(opp, "-gpgpu_n_clusters", OPT_UINT32, &n_simt_clusters, 
                  "number of processing clusters",
                  "10");
@@ -263,16 +364,7 @@
     option_parser_register(opp, "-gpgpu_n_ldst_response_buffer_size", OPT_UINT32, &ldst_unit_response_queue_size, 
                  "number of response packets in ld/st unit ejection buffer",
                  "2");
-    option_parser_register(opp, "-gpgpu_shmem_size", OPT_UINT32, &gpgpu_shmem_size,
-                 "Size of shared memory per shader core (default 16kB)",
-                 "16384");
-    option_parser_register(opp, "-gpgpu_shmem_size", OPT_UINT32, &gpgpu_shmem_sizeDefault,
-                 "Size of shared memory per shader core (default 16kB)",
-                 "16384");
-    option_parser_register(opp, "-gpgpu_shmem_size_PrefL1", OPT_UINT32, &gpgpu_shmem_sizePrefL1,
-                 "Size of shared memory per shader core (default 16kB)",
-                 "16384");
-    option_parser_register(opp, "-gpgpu_shmem_size_PrefShared", OPT_UINT32, &gpgpu_shmem_sizePrefShared,
+    option_parser_register(opp, "-gpgpu_shmem_size", OPT_UINT32, &gpgpu_shmem_size, 
                  "Size of shared memory per shader core (default 16kB)",
                  "16384");
     option_parser_register(opp, "-gpgpu_shmem_num_banks", OPT_UINT32, &num_shmem_bank, 
@@ -373,6 +465,8 @@
     gpgpu_functional_sim_config::reg_options(opp);
     m_shader_config.reg_options(opp);
     m_memory_config.reg_options(opp);
+    // Rachata --> hack
+    m_memory_config.warp_size = m_shader_config.warp_size;
     power_config::reg_options(opp);
    option_parser_register(opp, "-gpgpu_max_cycle", OPT_INT32, &gpu_max_cycle_opt, 
                "terminates gpu simulation early (0 = no limit)",
@@ -386,9 +480,6 @@
    option_parser_register(opp, "-gpgpu_runtime_stat", OPT_CSTR, &gpgpu_runtime_stat, 
                   "display runtime statistics such as dram utilization {<freq>:<flag>}",
                   "10000:0");
-   option_parser_register(opp, "-liveness_message_freq", OPT_INT64, &liveness_message_freq, 
-               "Minimum number of seconds between simulation liveness messages (0 = always print)",
-               "1");
    option_parser_register(opp, "-gpgpu_flush_l1_cache", OPT_BOOL, &gpgpu_flush_l1_cache,
                 "Flush L1 cache at the end of each kernel call",
                 "0");
@@ -434,9 +525,6 @@
     option_parser_register(opp, "-trace_sampling_core", OPT_INT32, 
                           &Trace::sampling_core, "The core which is printed using CORE_DPRINTF. Default 0",
                           "0");
-    option_parser_register(opp, "-trace_sampling_memory_partition", OPT_INT32, 
-                          &Trace::sampling_memory_partition, "The memory partition which is printed using MEMPART_DPRINTF. Default -1 (i.e. all)",
-                          "-1");
    ptx_file_line_stats_options(opp);
 }
 
@@ -505,6 +593,7 @@
         unsigned idx = (n+m_last_issued_kernel+1)%m_config.max_concurrent_kernel;
         if( m_running_kernels[idx] && !m_running_kernels[idx]->no_more_ctas_to_run() ) {
             m_last_issued_kernel=idx;
+
             // record this kernel for stat print if it is the first time this kernel is selected for execution  
             unsigned launch_uid = m_running_kernels[idx]->get_uid(); 
             if (std::find(m_executed_kernel_uids.begin(), m_executed_kernel_uids.end(), launch_uid) == m_executed_kernel_uids.end()) {
@@ -572,17 +661,10 @@
         m_cluster[i] = new simt_core_cluster(this,i,m_shader_config,m_memory_config,m_shader_stats,m_memory_stats);
 
     m_memory_partition_unit = new memory_partition_unit*[m_memory_config->m_n_mem];
-    m_memory_sub_partition = new memory_sub_partition*[m_memory_config->m_n_mem_sub_partition];
-    for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
+    for (unsigned i=0;i<m_memory_config->m_n_mem;i++) 
         m_memory_partition_unit[i] = new memory_partition_unit(i, m_memory_config, m_memory_stats);
-        for (unsigned p = 0; p < m_memory_config->m_n_sub_partition_per_memory_channel; p++) {
-            unsigned submpid = i * m_memory_config->m_n_sub_partition_per_memory_channel + p; 
-            m_memory_sub_partition[submpid] = m_memory_partition_unit[i]->get_sub_partition(p); 
-        }
-    }
 
-    icnt_wrapper_init();
-    icnt_create(m_shader_config->n_simt_clusters,m_memory_config->m_n_mem_sub_partition);
+    icnt_init(m_shader_config->n_simt_clusters,m_memory_config->m_n_mem);
 
     time_vector_create(NUM_MEM_REQ_STAT);
     fprintf(stdout, "GPGPU-Sim uArch: performance model initialization complete.\n");
@@ -593,7 +675,8 @@
     *average_pipeline_duty_cycle=0;
     *active_sms=0;
 
-    last_liveness_message_time = 0;
+    // Saugata: added check to print stats once in case of simulator livelock
+    final_stats_printed = false;
 }
 
 int gpgpu_sim::shared_mem_size() const
@@ -659,8 +742,17 @@
 {
     if (m_config.gpu_max_cycle_opt && (gpu_tot_sim_cycle + gpu_sim_cycle) >= m_config.gpu_max_cycle_opt) 
        return false;
-    if (m_config.gpu_max_insn_opt && (gpu_tot_sim_insn + gpu_sim_insn) >= m_config.gpu_max_insn_opt) 
-       return false;
+    if (m_config.gpu_max_insn_opt && (gpu_tot_sim_insn + gpu_sim_insn) >= m_config.gpu_max_insn_opt) {
+        // Saugata: added in print calls just in case...
+        if(!final_stats_printed) {
+            printf("\n\n\n\n\n*** DONE with %llu instructions (%u desired) ***\n\n", gpu_tot_sim_insn + gpu_sim_insn, m_config.gpu_max_insn_opt);
+            print_stats();
+            printf("\n*** ABOVE RESULTS FOR SIMULATED INSTRUCTIONS ***\n\n\n\n\n");
+            final_stats_printed = true;
+            fflush(stdout);
+        }
+        return false;
+    }
     if (m_config.gpu_max_cta_opt && (gpu_tot_issued_cta >= m_config.gpu_max_cta_opt) )
        return false;
     if (m_config.gpu_deadlock_detect && gpu_deadlock) 
@@ -705,8 +797,8 @@
        set_spill_interval (m_config.gpgpu_cflog_interval * 40);
     }
 
-    if (g_network_mode)
-       icnt_init();
+    if (g_network_mode) 
+       icnt_init_grid(); 
 
     // McPAT initialization function. Called on first launch of GPU
 #ifdef GPGPUSIM_POWER_MODEL
@@ -724,14 +816,15 @@
 
 void gpgpu_sim::print_stats()
 {
+
     ptx_file_line_stats_write_file();
     gpu_print_stat();
 
     if (g_network_mode) {
-        printf("----------------------------Interconnect-DETAILS--------------------------------\n" );
-        icnt_display_stats();
-        icnt_display_overall_stats();
-        printf("----------------------------END-of-Interconnect-DETAILS-------------------------\n" );
+       interconnect_stats();
+       printf("----------------------------Interconnect-DETAILS---------------------------------" );
+       icnt_overal_stat();
+       printf("----------------------------END-of-Interconnect-DETAILS-------------------------" );
     }
 }
 
@@ -767,7 +860,7 @@
       }
       if( icnt_busy() ) {
          printf("GPGPU-Sim uArch DEADLOCK:  iterconnect contains traffic\n");
-         icnt_display_state( stdout );
+         display_icnt_state( stdout );
       }
       printf("\nRe-run the simulator in gdb and use debug routines in .gdbinit to debug this\n");
       fflush(stdout);
@@ -793,92 +886,13 @@
 
    return statout.str(); 
 }
-void gpgpu_sim::set_cache_config(std::string kernel_name,  FuncCache cacheConfig )
-{
-	m_special_cache_config[kernel_name]=cacheConfig ;
-}
-
-FuncCache gpgpu_sim::get_cache_config(std::string kernel_name)
-{
-	for (	std::map<std::string, FuncCache>::iterator iter = m_special_cache_config.begin(); iter != m_special_cache_config.end(); iter++){
-		    std::string kernel= iter->first;
-			if (kernel_name.compare(kernel) == 0){
-				return iter->second;
-			}
-	}
-	return (FuncCache)0;
-}
-
-bool gpgpu_sim::has_special_cache_config(std::string kernel_name)
-{
-	for (	std::map<std::string, FuncCache>::iterator iter = m_special_cache_config.begin(); iter != m_special_cache_config.end(); iter++){
-	    	std::string kernel= iter->first;
-			if (kernel_name.compare(kernel) == 0){
-				return true;
-			}
-	}
-	return false;
-}
-
-
-void gpgpu_sim::set_cache_config(std::string kernel_name)
-{
-	if(has_special_cache_config(kernel_name)){
-		change_cache_config(get_cache_config(kernel_name));
-	}else{
-		change_cache_config(FuncCachePreferNone);
-	}
-}
-
-
-void gpgpu_sim::change_cache_config(FuncCache cache_config)
-{
-	if(cache_config != m_shader_config->m_L1D_config.get_cache_status()){
-		printf("FLUSH L1 Cache at configuration change between kernels\n");
-		for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
-			m_cluster[i]->cache_flush();
-	    }
-	}
-
-	switch(cache_config){
-	case FuncCachePreferNone:
-		m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
-		m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizeDefault;
-		break;
-	case FuncCachePreferL1:
-		if((m_shader_config->m_L1D_config.m_config_stringPrefL1 == NULL) || (m_shader_config->gpgpu_shmem_sizePrefL1 == (unsigned)-1))
-		{
-			printf("WARNING: missing Preferred L1 configuration\n");
-			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
-			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizeDefault;
-
-		}else{
-			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_stringPrefL1, FuncCachePreferL1);
-			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizePrefL1;
-		}
-		break;
-	case FuncCachePreferShared:
-		if((m_shader_config->m_L1D_config.m_config_stringPrefShared == NULL) || (m_shader_config->gpgpu_shmem_sizePrefShared == (unsigned)-1))
-		{
-			printf("WARNING: missing Preferred L1 configuration\n");
-			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
-			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizeDefault;
-		}else{
-			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_stringPrefShared, FuncCachePreferShared);
-			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizePrefShared;
-		}
-		break;
-	default:
-		break;
-	}
-}
-
 
 void gpgpu_sim::clear_executed_kernel_info()
 {
-   m_executed_kernel_names.clear();
-   m_executed_kernel_uids.clear();
+   m_executed_kernel_names.clear(); 
+   m_executed_kernel_uids.clear(); 
 }
+
 void gpgpu_sim::gpu_print_stat() 
 {  
    FILE *statfout = stdout; 
@@ -898,6 +912,7 @@
 
    // performance counter for stalls due to congestion.
    printf("gpu_stall_dramfull = %d\n", gpu_stall_dramfull);
+   printf("gpu_stall_dramportbound = %d\n", gpu_stall_dramportbound); // Saugata: tallies cycles when some requests succeed but the DRAM fills up
    printf("gpu_stall_icnt2sh    = %d\n", gpu_stall_icnt2sh );
 
    time_t curr_time;
@@ -905,64 +920,24 @@
    unsigned long long elapsed_time = MAX( curr_time - g_simulation_starttime, 1 );
    printf( "gpu_total_sim_rate=%u\n", (unsigned)( ( gpu_tot_sim_insn + gpu_sim_insn ) / elapsed_time ) );
 
-   //shader_print_l1_miss_stat( stdout );
-   shader_print_cache_stats(stdout);
-
-   cache_stats core_cache_stats;
-   core_cache_stats.clear();
-   for(unsigned i=0; i<m_config.num_cluster(); i++){
-       m_cluster[i]->get_cache_stats(core_cache_stats);
-   }
-   printf("\nTotal_core_cache_stats:\n");
-   core_cache_stats.print_stats(stdout, "Total_core_cache_stats_breakdown");
-   shader_print_scheduler_stat( stdout, false );
+   shader_print_l1_miss_stat( stdout );
+   shader_print_scheduler_stat( stdout, true );
 
    m_shader_stats->print(stdout);
 #ifdef GPGPUSIM_POWER_MODEL
    if(m_config.g_power_simulation_enabled){
-	   m_gpgpusim_wrapper->print_power_kernel_stats(gpu_sim_cycle, gpu_tot_sim_cycle, gpu_tot_sim_insn + gpu_sim_insn, kernel_info_str, true );
-	   mcpat_reset_perf_count(m_gpgpusim_wrapper);
+	   m_gpgpusim_wrapper->print_power_kernel_stats(gpu_sim_cycle, gpu_tot_sim_cycle, gpu_tot_sim_insn + gpu_sim_insn, kernel_info_str );
+	   mcpat_reset_perf_count(m_gpgpusim_wrapper, true);
    }
 #endif
 
    // performance counter that are not local to one shader
    m_memory_stats->memlatstat_print(m_memory_config->m_n_mem,m_memory_config->nbk);
-   for (unsigned i=0;i<m_memory_config->m_n_mem;i++)
+   m_memory_stats->print(stdout);
+   for (unsigned i=0;i<m_memory_config->m_n_mem;i++) 
       m_memory_partition_unit[i]->print(stdout);
-
-   // L2 cache stats
-   if(!m_memory_config->m_L2_config.disabled()){
-       cache_stats l2_stats;
-       struct cache_sub_stats l2_css;
-       struct cache_sub_stats total_l2_css;
-       l2_stats.clear();
-       l2_css.clear();
-       total_l2_css.clear();
-
-       printf("\n========= L2 cache stats =========\n");
-       for (unsigned i=0;i<m_memory_config->m_n_mem_sub_partition;i++){
-           m_memory_sub_partition[i]->accumulate_L2cache_stats(l2_stats);
-           m_memory_sub_partition[i]->get_L2cache_sub_stats(l2_css);
-
-           fprintf( stdout, "L2_cache_bank[%d]: Access = %u, Miss = %u, Miss_rate = %.3lf, Pending_hits = %u, Reservation_fails = %u\n",
-                    i, l2_css.accesses, l2_css.misses, (double)l2_css.misses / (double)l2_css.accesses, l2_css.pending_hits, l2_css.res_fails);
-
-           total_l2_css += l2_css;
-       }
-       if (!m_memory_config->m_L2_config.disabled() && m_memory_config->m_L2_config.get_num_lines()) {
-          //L2c_print_cache_stat();
-          printf("L2_total_cache_accesses = %u\n", total_l2_css.accesses);
-          printf("L2_total_cache_misses = %u\n", total_l2_css.misses);
-          if(total_l2_css.accesses > 0)
-              printf("L2_total_cache_miss_rate = %.4lf\n", (double)total_l2_css.misses/(double)total_l2_css.accesses);
-          printf("L2_total_cache_pending_hits = %u\n", total_l2_css.pending_hits);
-          printf("L2_total_cache_reservation_fails = %u\n", total_l2_css.res_fails);
-          printf("L2_total_cache_breakdown:\n");
-          l2_stats.print_stats(stdout, "L2_cache_stats_breakdown");
-          total_l2_css.print_port_stats(stdout, "L2_cache");
-       }
-   }
-
+   if (!m_memory_config->m_L2_config.disabled() && m_memory_config->m_L2_config.get_num_lines())
+      L2c_print_cache_stat();
    if (m_config.gpgpu_cflog_interval != 0) {
       spill_log_to_file (stdout, 1, gpu_sim_cycle);
       insn_warp_occ_print(stdout);
@@ -978,19 +953,30 @@
    }
 #endif
 
+   // Saugata: print warp hit rate stats
+   for(unsigned i = 0; i < m_memory_config->m_n_mem; ++i) { 
+      m_memory_partition_unit[i]->print_warp_hit_rates(stdout);
+   }
+
+   // Saugata: print divergence stats
+   shader_print_divergence_stat(stdout);
+
 
    // Interconnect power stat print
-   long total_simt_to_mem=0;
-   long total_mem_to_simt=0;
-   long temp_stm=0;
-   long temp_mts = 0;
+   unsigned total_mem_to_simt=0;
+   unsigned total_simt_to_mem=0;
+   for (unsigned i=0;i<m_memory_config->m_n_mem;i++){
+      unsigned temp=0;
+      m_memory_partition_unit[i]->set_icnt_power_stats(temp);
+      total_mem_to_simt += temp;
+   }
    for(unsigned i=0; i<m_config.num_cluster(); i++){
-	   m_cluster[i]->get_icnt_stats(temp_stm, temp_mts);
-	   total_simt_to_mem += temp_stm;
-	   total_mem_to_simt += temp_mts;
+	   unsigned temp=0;
+	   m_cluster[i]->set_icnt_stats(temp);
+	   total_simt_to_mem += temp;
    }
-   printf("\nicnt_total_pkts_mem_to_simt=%ld\n", total_mem_to_simt);
-   printf("icnt_total_pkts_simt_to_mem=%ld\n", total_simt_to_mem);
+   printf("\nicnt_total_pkts_mem_to_simt=%u\n", total_mem_to_simt);
+   printf("icnt_total_pkts_simt_to_mem=%u\n\n", total_simt_to_mem);
 
    time_vector_print();
    fflush(stdout);
@@ -1165,8 +1151,8 @@
    }
     if (clock_mask & ICNT) {
         // pop from memory controller to interconnect
-        for (unsigned i=0;i<m_memory_config->m_n_mem_sub_partition;i++) {
-            mem_fetch* mf = m_memory_sub_partition[i]->top();
+        for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
+            mem_fetch* mf = m_memory_partition_unit[i]->top();
             if (mf) {
                 unsigned response_size = mf->get_is_write()?mf->get_ctrl_size():mf->size();
                 if ( ::icnt_has_buffer( m_shader_config->mem2device(i), response_size ) ) {
@@ -1174,12 +1160,12 @@
                        mf->set_return_timestamp(gpu_sim_cycle+gpu_tot_sim_cycle);
                     mf->set_status(IN_ICNT_TO_SHADER,gpu_sim_cycle+gpu_tot_sim_cycle);
                     ::icnt_push( m_shader_config->mem2device(i), mf->get_tpc(), mf, response_size );
-                    m_memory_sub_partition[i]->pop();
+                    m_memory_partition_unit[i]->pop();
                 } else {
                     gpu_stall_icnt2sh++;
                 }
             } else {
-               m_memory_sub_partition[i]->pop();
+               m_memory_partition_unit[i]->pop();
             }
         }
     }
@@ -1188,26 +1174,43 @@
       for (unsigned i=0;i<m_memory_config->m_n_mem;i++){
          m_memory_partition_unit[i]->dram_cycle(); // Issue the dram command (scheduler + delay model)
          // Update performance counters for DRAM
-         m_memory_partition_unit[i]->set_dram_power_stats(m_power_stats->pwr_mem_stat->n_cmd[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_activity[CURRENT_STAT_IDX][i],
-                        m_power_stats->pwr_mem_stat->n_nop[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_act[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_pre[CURRENT_STAT_IDX][i],
-                        m_power_stats->pwr_mem_stat->n_rd[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_wr[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_req[CURRENT_STAT_IDX][i]);
+         m_memory_partition_unit[i]->set_dram_power_stats(m_power_stats->pwr_mem_stat->n_cmd[0][i], m_power_stats->pwr_mem_stat->n_activity[0][i],
+                        m_power_stats->pwr_mem_stat->n_nop[0][i], m_power_stats->pwr_mem_stat->n_act[0][i], m_power_stats->pwr_mem_stat->n_pre[0][i],
+                        m_power_stats->pwr_mem_stat->n_rd[0][i], m_power_stats->pwr_mem_stat->n_wr[0][i], m_power_stats->pwr_mem_stat->n_req[0][i]);
       }
    }
 
    // L2 operations follow L2 clock domain
    if (clock_mask & L2) {
-       m_power_stats->pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].clear();
-      for (unsigned i=0;i<m_memory_config->m_n_mem_sub_partition;i++) {
+      for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
           //move memory request from interconnect into memory partition (if not backed up)
           //Note:This needs to be called in DRAM clock domain if there is no L2 cache in the system
-          if ( m_memory_sub_partition[i]->full() ) {
-             gpu_stall_dramfull++;
-          } else {
-              mem_fetch* mf = (mem_fetch*) icnt_pop( m_shader_config->mem2device(i) );
-              m_memory_sub_partition[i]->push( mf, gpu_sim_cycle + gpu_tot_sim_cycle );
+
+          // Saugata: make sure the MPU first pulls any eligible requests out of the requests queued at the input port; assumes 1 cycle delay
+          m_memory_partition_unit[i]->bank_arbitrate_cycle(gpu_sim_cycle+gpu_tot_sim_cycle);
+
+          // Saugata: pull off one request per MPU input port; these are queued inside the MPU
+          for(unsigned int port = 0; port < m_memory_config->m_n_ports_per_memory_channel; ++port) {
+              if ( m_memory_partition_unit[i]->full() ) {
+                 if(!port) {
+                     // Saugata: only tally this if the MPU doesn't accept any loads
+                     gpu_stall_dramfull++;
+                 }
+                 else {
+                     ++gpu_stall_dramportbound;
+                 }
+                 // Saugata: added break to ensure that stall cycles aren't counted more than once
+                 break;
+              } else {
+                  mem_fetch* mf = (mem_fetch*) icnt_pop( m_shader_config->mem2device(i) );
+                  m_memory_partition_unit[i]->push( mf, gpu_sim_cycle + gpu_tot_sim_cycle );
+              }
           }
-          m_memory_sub_partition[i]->cache_cycle(gpu_sim_cycle+gpu_tot_sim_cycle);
-          m_memory_sub_partition[i]->accumulate_L2cache_stats(m_power_stats->pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX]);
+          m_memory_partition_unit[i]->cache_cycle(gpu_sim_cycle+gpu_tot_sim_cycle);
+          m_memory_partition_unit[i]->set_L2cache_power_stats(m_power_stats->pwr_mem_stat->n_l2_read_access[0][i], m_power_stats->pwr_mem_stat->n_l2_read_miss[0][i],
+          m_power_stats->pwr_mem_stat->n_l2_write_access[0][i], m_power_stats->pwr_mem_stat->n_l2_write_miss[0][i]);
+
+          m_memory_partition_unit[i]->set_icnt_power_stats(m_power_stats->pwr_mem_stat->n_mem_to_simt[0][i]);
        }
    }
 
@@ -1216,16 +1219,15 @@
    }
 
    if (clock_mask & CORE) {
-      // L1 cache + shader core pipeline stages
-      m_power_stats->pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].clear();
+      // L1 cache + shader core pipeline stages 
       for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
          if (m_cluster[i]->get_not_completed() || get_more_cta_left() ) {
                m_cluster[i]->core_cycle();
                *active_sms+=m_cluster[i]->get_n_active_sms();
+
+               // Interconnect power stats: SIMT->MEM
+               m_cluster[i]->set_icnt_stats(m_power_stats->pwr_mem_stat->n_simt_to_mem[0][i]);
          }
-         // Update core icnt/cache stats for GPUWattch
-         m_cluster[i]->get_icnt_stats(m_power_stats->pwr_mem_stat->n_simt_to_mem[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_mem_to_simt[CURRENT_STAT_IDX][i]);
-         m_cluster[i]->get_cache_stats(m_power_stats->pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX]);
       }
       float temp=0;
       for (unsigned i=0;i<m_shader_config->num_shader();i++){
@@ -1278,7 +1280,7 @@
             if (m_memory_config->m_L2_config.get_num_lines()) {
                int dlc = 0;
                for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
-                  dlc = m_memory_sub_partition[i]->flushL2();
+                  dlc = m_memory_partition_unit[i]->flushL2();
                   assert (dlc == 0); // need to model actual writes to DRAM here
                   printf("Dirty lines flushed from L2 %d is %d\n", i, dlc  );
                }
@@ -1291,20 +1293,17 @@
          time_t curr_time;
          time(&curr_time);
          unsigned long long  elapsed_time = MAX(curr_time - g_simulation_starttime, 1);
-         if ( (elapsed_time - last_liveness_message_time) >= m_config.liveness_message_freq ) {
-            days    = elapsed_time/(3600*24);
-            hrs     = elapsed_time/3600 - 24*days;
-            minutes = elapsed_time/60 - 60*(hrs + 24*days);
-            sec = elapsed_time - 60*(minutes + 60*(hrs + 24*days));
-            printf("GPGPU-Sim uArch: cycles simulated: %lld  inst.: %lld (ipc=%4.1f) sim_rate=%u (inst/sec) elapsed = %u:%u:%02u:%02u / %s", 
-                   gpu_tot_sim_cycle + gpu_sim_cycle, gpu_tot_sim_insn + gpu_sim_insn, 
-                   (double)gpu_sim_insn/(double)gpu_sim_cycle,
-                   (unsigned)((gpu_tot_sim_insn+gpu_sim_insn) / elapsed_time),
-                   (unsigned)days,(unsigned)hrs,(unsigned)minutes,(unsigned)sec,
-                   ctime(&curr_time));
-            fflush(stdout);
-            last_liveness_message_time = elapsed_time; 
-         }
+         days    = elapsed_time/(3600*24);
+         hrs     = elapsed_time/3600 - 24*days;
+         minutes = elapsed_time/60 - 60*(hrs + 24*days);
+         sec = elapsed_time - 60*(minutes + 60*(hrs + 24*days));
+         printf("GPGPU-Sim uArch: cycles simulated: %lld  inst.: %lld (ipc=%4.1f) sim_rate=%u (inst/sec) elapsed = %u:%u:%02u:%02u / %s", 
+                gpu_tot_sim_cycle + gpu_sim_cycle, gpu_tot_sim_insn + gpu_sim_insn, 
+                (double)gpu_sim_insn/(double)gpu_sim_cycle,
+                (unsigned)((gpu_tot_sim_insn+gpu_sim_insn) / elapsed_time),
+                (unsigned)days,(unsigned)hrs,(unsigned)minutes,(unsigned)sec,
+                ctime(&curr_time));
+         fflush(stdout);
          visualizer_printstat();
          m_memory_stats->memlatstat_lat_pw();
          if (m_config.gpgpu_runtime_stat && (m_config.gpu_runtime_stat_flag != 0) ) {
@@ -1321,6 +1320,18 @@
             if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_SCHED) 
                shader_print_scheduler_stat( stdout, false );
          }
+
+         // Saugata: added print calls for warp hit rate
+         if(m_memory_config->l2_print_warp_hit_rates) {
+            for(unsigned i = 0; i < m_memory_config->m_n_mem; ++i) { 
+               m_memory_partition_unit[i]->print_warp_hit_rates(stdout);
+            }
+         }
+
+         // Saugata: added print calls for divergence stats
+         if(m_memory_config->l2_print_divergence_stats) {
+             shader_print_divergence_stat(stdout);
+         }
       }
 
       if (!(gpu_sim_cycle % 20000)) {
@@ -1401,3 +1412,8 @@
    return *m_cluster;
 }
 
+void memory_partition_unit::visualizer_print( gzFile visualizer_file )
+{
+   m_dram->visualizer_print(visualizer_file);
+}
+
diff -Naur gpgpu-sim-baseline/gpu-sim.h gpgpu-sim/gpu-sim.h
--- gpgpu-sim-baseline/gpu-sim.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/gpu-sim.h	2015-10-13 06:38:50.558481857 -0400
@@ -35,6 +35,7 @@
 #include "shader.h"
 #include <iostream>
 #include <fstream>
+
 #include <list>
 #include <stdio.h>
 
@@ -72,7 +73,6 @@
 };
 
 
-
 struct power_config {
 	power_config()
 	{
@@ -190,35 +190,30 @@
          option_parser_destroy(dram_opp); 
       }
 
-      int nbkt = nbk/nbkgrp;
-      unsigned i;
-      for (i=0; nbkt>0; i++) {
-          nbkt = nbkt>>1;
-      }
-      bk_tag_length = i;
+        int nbkt = nbk/nbkgrp;
+        unsigned i;
+        for (i=0; nbkt>0; i++) {
+            nbkt = nbkt>>1;
+        }
+        bk_tag_length = i;
       assert(nbkgrp>0 && "Number of bank groups cannot be zero");
       tRCDWR = tRCD-(WL+1);
       tRTW = (CL+(BL/data_command_freq_ratio)+2-WL);
       tWTR = (WL+(BL/data_command_freq_ratio)+tCDLR); 
       tWTP = (WL+(BL/data_command_freq_ratio)+tWR);
       dram_atom_size = BL * busW * gpu_n_mem_per_ctrlr; // burst length x bus width x # chips per partition 
-
-      assert( m_n_sub_partition_per_memory_channel > 0 ); 
-      assert( (nbk % m_n_sub_partition_per_memory_channel == 0) 
-              && "Number of DRAM banks must be a perfect multiple of memory sub partition"); 
-      m_n_mem_sub_partition = m_n_mem * m_n_sub_partition_per_memory_channel; 
-      fprintf(stdout, "Total number of memory sub partition = %u\n", m_n_mem_sub_partition); 
-
-      m_address_mapping.init(m_n_mem, m_n_sub_partition_per_memory_channel);
-      m_L2_config.init(&m_address_mapping);
-
+      // Saugata: added L2 bank ID into address mapping
+      // m_address_mapping.init(m_n_mem);
+      m_address_mapping.init(m_n_mem, m_n_L2_banks_per_memory_channel);
+      // Saugata: L2 cache splits ways amongst number of banks now
+      m_L2_config.init(&m_address_mapping, m_n_L2_banks_per_memory_channel);
       m_valid = true;
       icnt_flit_size = 32; // Default 32
    }
    void reg_options(class OptionParser * opp);
 
    bool m_valid;
-   mutable l2_cache_config m_L2_config;
+   l2_cache_config m_L2_config;
    bool m_L2_texure_only;
 
    char *gpgpu_dram_timing_opt;
@@ -229,11 +224,17 @@
    enum dram_ctrl_t scheduler_type;
    bool gpgpu_memlatency_stat;
    unsigned m_n_mem;
-   unsigned m_n_sub_partition_per_memory_channel;
-   unsigned m_n_mem_sub_partition;
+   // Saugata: added number of pipeline stages and ports per bank for L2
+   unsigned m_n_L2_pipeline_stages;
+   unsigned m_n_L2_bank_ports;
+   // Saugata: added number of L2 banks and number of ports per MPU
+   unsigned m_n_L2_banks_per_memory_channel;
+   unsigned m_n_ports_per_memory_channel;
    unsigned gpu_n_mem_per_ctrlr;
 
    unsigned rop_latency;
+   // Saugata: added L2 cache latency
+   unsigned L2_latency;
    unsigned dram_latency;
 
    // DRAM parameters
@@ -270,6 +271,37 @@
    linear_to_raw_address_translation m_address_mapping;
 
    unsigned icnt_flit_size;
+
+   // Rachata
+   bool cache_aware;
+   // PCAT bypassing policy
+   bool l2_bypass_PCAL;
+   unsigned l2_bypass_PCAL_num_tokens;
+   // LMNO bypassing policy
+   bool bypass_rop;
+   int cache_aware_insertion;
+   bool reuse_aware;
+   bool eaf_insertion;
+   bool prioritize_mf;
+   unsigned warp_size;
+   float target_warp_hit_ratio;
+   float target_warp_hit_ratio_high;
+   unsigned l2_info_clear;
+   unsigned l2_warmup;
+   unsigned l2_reuse_threshold;
+   unsigned l2_bloom_filter_size;
+   // Saugata: added options to allow for random cache bypassing
+   bool l2_bypass_randomly;
+   bool l2_bypass_pc;
+   bool l2_bypass_combined;
+   unsigned l2_bypass_pc_range;
+   float l2_random_bypass_rate;
+   // Saugata: added option to only perform bypassing after the request travels through the request queue
+   bool l2_bypass_after_queue;
+   // Saugata: added control on whether to print warp hit rates
+   bool l2_print_warp_hit_rates;
+   // Saugata: added control on whether to print divergence stats
+   bool l2_print_divergence_stats;
 };
 
 // global counters and flags (please try not to add to this list!!!)
@@ -357,7 +389,6 @@
 
 
 
-    unsigned long long liveness_message_freq; 
 
     friend class gpgpu_sim;
 };
@@ -380,6 +411,15 @@
    void update_stats();
    void deadlock_check();
 
+   // Saugata: hack to force exit when instructions completed
+   bool exit_now() {
+       if (m_config.gpu_max_insn_opt && (gpu_tot_sim_insn + gpu_sim_insn) >= m_config.gpu_max_insn_opt) {
+           return true;
+       }
+
+       return false;
+   }
+
    void get_pdom_stack_top_info( unsigned sid, unsigned tid, unsigned *pc, unsigned *rpc );
 
    int shared_mem_size() const;
@@ -425,11 +465,13 @@
    void reinit_clock_domains(void);
    int  next_clock_domain(void);
    void issue_block2core();
-   void print_dram_stats(FILE *fout) const;
+   void print_dram_L2_stats(FILE *fout) const;
+   void L2c_print_cache_stat() const;
    void shader_print_runtime_stat( FILE *fout );
    void shader_print_l1_miss_stat( FILE *fout ) const;
-   void shader_print_cache_stats( FILE *fout ) const;
    void shader_print_scheduler_stat( FILE* fout, bool print_dynamic_info ) const;
+   // Saugata: added divergence stat print call
+   void shader_print_divergence_stat( FILE *fout) const;
    void visualizer_printstat();
    void print_shader_cycle_distro( FILE *fout ) const;
 
@@ -439,7 +481,6 @@
 
    class simt_core_cluster **m_cluster;
    class memory_partition_unit **m_memory_partition_unit;
-   class memory_sub_partition **m_memory_sub_partition;
 
    std::vector<kernel_info_t*> m_running_kernels;
    unsigned m_last_issued_kernel;
@@ -473,9 +514,8 @@
    unsigned long long  gpu_tot_issued_cta;
    unsigned long long  last_gpu_sim_insn;
 
-   unsigned long long  last_liveness_message_time; 
-
-   std::map<std::string, FuncCache> m_special_cache_config;
+   // Saugata: added check to print stats once in case of simulator livelock
+   bool final_stats_printed;
 
    std::vector<std::string> m_executed_kernel_names; //< names of kernel for stat printout 
    std::vector<unsigned> m_executed_kernel_uids; //< uids of kernel launches for stat printout
@@ -490,11 +530,6 @@
 
 
 
-   FuncCache get_cache_config(std::string kernel_name);
-   void set_cache_config(std::string kernel_name, FuncCache cacheConfig );
-   bool has_special_cache_config(std::string kernel_name);
-   void change_cache_config(FuncCache cache_config);
-   void set_cache_config(std::string kernel_name);
 
 };
 
diff -Naur gpgpu-sim-baseline/icnt_wrapper.cc gpgpu-sim/icnt_wrapper.cc
--- gpgpu-sim-baseline/icnt_wrapper.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/icnt_wrapper.cc	2015-10-13 06:38:50.546481857 -0400
@@ -27,110 +27,41 @@
 
 #include "icnt_wrapper.h"
 #include <assert.h>
-#include "../intersim2/globals.hpp"
-#include "../intersim2/interconnect_interface.hpp"
+#include "../intersim/interconnect_interface.h"
 
-icnt_create_p                icnt_create;
-icnt_init_p                  icnt_init;
-icnt_has_buffer_p            icnt_has_buffer;
-icnt_push_p                  icnt_push;
-icnt_pop_p                   icnt_pop;
-icnt_transfer_p              icnt_transfer;
-icnt_busy_p                  icnt_busy;
-icnt_display_stats_p         icnt_display_stats;
-icnt_display_overall_stats_p icnt_display_overall_stats;
-icnt_display_state_p         icnt_display_state;
-icnt_get_flit_size_p         icnt_get_flit_size;
+icnt_has_buffer_p icnt_has_buffer;
+icnt_push_p       icnt_push;
+icnt_pop_p        icnt_pop;
+icnt_transfer_p   icnt_transfer;
+icnt_busy_p       icnt_busy;
+icnt_get_flit_size_p icnt_get_flit_size;
 
 int   g_network_mode;
 char* g_network_config_filename;
 
 #include "../option_parser.h"
 
-// Wrapper to intersim2 to accompany old icnt_wrapper
-// TODO: use delegate/boost/c++11<funtion> instead
-
-static void intersim2_create(unsigned int n_shader, unsigned int n_mem)
-{
-   g_icnt_interface->CreateInterconnect(n_shader, n_mem);
-}
-
-static void intersim2_init()
-{
-   g_icnt_interface->Init();
-}
-
-static bool intersim2_has_buffer(unsigned input, unsigned int size)
-{
-   return g_icnt_interface->HasBuffer(input, size);
-}
-
-static void intersim2_push(unsigned input, unsigned output, void* data, unsigned int size)
-{
-   g_icnt_interface->Push(input, output, data, size);
-}
-
-static void* intersim2_pop(unsigned output)
-{
-   return g_icnt_interface->Pop(output);
-}
-
-static void intersim2_transfer()
-{
-   g_icnt_interface->Advance();
-}
-
-static bool intersim2_busy()
-{
-   return g_icnt_interface->Busy();
-}
-
-static void intersim2_display_stats()
-{
-   g_icnt_interface->DisplayStats();
-}
-
-static void intersim2_display_overall_stats()
-{
-   g_icnt_interface->DisplayOverallStats();
-}
-
-static void intersim2_display_state(FILE *fp)
-{
-   g_icnt_interface->DisplayState(fp);
-}
-
-static unsigned intersim2_get_flit_size()
-{
-   return g_icnt_interface->GetFlitSize();
-}
-
 void icnt_reg_options( class OptionParser * opp )
 {
    option_parser_register(opp, "-network_mode", OPT_INT32, &g_network_mode, "Interconnection network mode", "1");
    option_parser_register(opp, "-inter_config_file", OPT_CSTR, &g_network_config_filename, "Interconnection network config file", "mesh");
 }
 
-void icnt_wrapper_init()
+void icnt_init( unsigned int n_shader, unsigned int n_mem )
 {
    switch (g_network_mode) {
-      case INTERSIM:
-         //FIXME: delete the object: may add icnt_done wrapper
-         g_icnt_interface = InterconnectInterface::New(g_network_config_filename);
-         icnt_create     = intersim2_create;
-         icnt_init       = intersim2_init;
-         icnt_has_buffer = intersim2_has_buffer;
-         icnt_push       = intersim2_push;
-         icnt_pop        = intersim2_pop;
-         icnt_transfer   = intersim2_transfer;
-         icnt_busy       = intersim2_busy;
-         icnt_display_stats = intersim2_display_stats;
-         icnt_display_overall_stats = intersim2_display_overall_stats;
-         icnt_display_state = intersim2_display_state;
-         icnt_get_flit_size = intersim2_get_flit_size;
-         break;
-      default:
-         assert(0);
-         break;
+   case INTERSIM:
+      init_interconnect(g_network_config_filename, n_shader, n_mem );
+      icnt_has_buffer = interconnect_has_buffer;
+      icnt_push       = interconnect_push;
+      icnt_pop        = interconnect_pop;
+      icnt_transfer   = advance_interconnect;
+      icnt_busy       = interconnect_busy;
+      icnt_get_flit_size = interconnect_get_flit_size;
+     break;
+
+   default:
+      assert(0);
+      break;
    }
 }
diff -Naur gpgpu-sim-baseline/icnt_wrapper.h gpgpu-sim/icnt_wrapper.h
--- gpgpu-sim-baseline/icnt_wrapper.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/icnt_wrapper.h	2015-10-13 06:38:50.566481857 -0400
@@ -31,31 +31,21 @@
 #include <stdio.h>
 
 // functional interface to the interconnect
-
-typedef void (*icnt_create_p)(unsigned n_shader,  unsigned n_mem);
-typedef void (*icnt_init_p)( );
 typedef bool (*icnt_has_buffer_p)(unsigned input, unsigned int size);
 typedef void (*icnt_push_p)(unsigned input, unsigned output, void* data, unsigned int size);
 typedef void* (*icnt_pop_p)(unsigned output);
 typedef void (*icnt_transfer_p)( );
-typedef bool (*icnt_busy_p)( );
+typedef unsigned (*icnt_busy_p)( );
 typedef void (*icnt_drain_p)( );
-typedef void (*icnt_display_stats_p)( );
-typedef void (*icnt_display_overall_stats_p)( );
-typedef void (*icnt_display_state_p)(FILE* fp);
 typedef unsigned (*icnt_get_flit_size_p)();
 
-extern icnt_create_p     icnt_create;
-extern icnt_init_p       icnt_init;
+
 extern icnt_has_buffer_p icnt_has_buffer;
 extern icnt_push_p       icnt_push;
 extern icnt_pop_p        icnt_pop;
 extern icnt_transfer_p   icnt_transfer;
 extern icnt_busy_p       icnt_busy;
 extern icnt_drain_p      icnt_drain;
-extern icnt_display_stats_p icnt_display_stats;
-extern icnt_display_overall_stats_p icnt_display_overall_stats;
-extern icnt_display_state_p icnt_display_state;
 extern icnt_get_flit_size_p icnt_get_flit_size;
 extern int g_network_mode;
 
@@ -64,7 +54,8 @@
    N_NETWORK_MODE
 };
 
-void icnt_wrapper_init();
+void icnt_init( unsigned int n_shader, unsigned int n_mem );
 void icnt_reg_options( class OptionParser * opp );
+void display_icnt_state( FILE *fp );
 
 #endif
diff -Naur gpgpu-sim-baseline/l2cache.cc gpgpu-sim/l2cache.cc
--- gpgpu-sim-baseline/l2cache.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/l2cache.cc	2015-10-13 06:38:50.578481858 -0400
@@ -38,14 +38,64 @@
 #include "gpu-cache.h"
 #include "histogram.h"
 #include "l2cache.h"
-#include "../statwrapper.h"
+#include "../intersim/statwraper.h"
 #include "../abstract_hardware_model.h"
 #include "gpu-sim.h"
 #include "shader.h"
 #include "mem_latency_stat.h"
-#include "l2cache_trace.h"
 
 
+//Rachata --> constructor for l2 warp info
+l2_warp_info::l2_warp_info(unsigned wID, const struct memory_config *config)
+{
+    warpID = wID;
+    t0_address = wID*config->warp_size;
+    hit_ratio = 0.5;
+    total_accessed = 0;
+    total_hit = 0;
+    timeCleared = 0;
+    unrecognized_tid = 0;
+    last_accessed_time = 0;
+    // TODO
+    for(unsigned int i=0;i<config->warp_size;i++)
+    {
+        hit_count[i] = 0;
+        accessed[i] = 0;
+        last_mask[i] = 0;
+        last_accessed[i] = 0;
+        last_hit[i] = 0;
+    }
+}
+
+void l2_warp_info::clear()
+{
+    hit_ratio = 0.5;
+    total_accessed = 0;
+    total_hit = 0;
+    unrecognized_tid = 0;
+    timeCleared++;
+}
+
+address_info::address_info(unsigned wID, const struct memory_config *config)
+{
+    m_config = config; 
+    warpID = wID;
+    total_accessed = 0;
+    total_hit = 0;
+    timeCleared = 0;
+    last_accessed_time = 0;
+}
+
+
+
+void address_info::clear(mem_fetch* mf)
+{
+    total_accessed = 0;
+    total_hit = 0;
+    timeCleared++;
+    last_accessed_time = mf->get_timestamp();
+}
+
 mem_fetch * partition_mf_allocator::alloc(new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const 
 {
     assert( wr );
@@ -56,6 +106,7 @@
                                    -1, 
                                    -1, 
                                    -1,
+                                   -1, // TODO: Rachata --> tid
                                    m_memory_config );
     return mf;
 }
@@ -63,411 +114,713 @@
 memory_partition_unit::memory_partition_unit( unsigned partition_id, 
                                               const struct memory_config *config,
                                               class memory_stats_t *stats )
-: m_id(partition_id), m_config(config), m_stats(stats), m_arbitration_metadata(config) 
 {
+    m_id = partition_id;
+    m_config=config;
+    m_stats=stats;
     m_dram = new dram_t(m_id,m_config,m_stats,this);
 
-    m_sub_partition = new memory_sub_partition*[m_config->m_n_sub_partition_per_memory_channel]; 
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        unsigned sub_partition_id = m_id * m_config->m_n_sub_partition_per_memory_channel + p; 
-        m_sub_partition[p] = new memory_sub_partition(sub_partition_id, m_config, stats); 
-    }
-}
-
-memory_partition_unit::~memory_partition_unit() 
-{
-    delete m_dram; 
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        delete m_sub_partition[p]; 
-    } 
-    delete[] m_sub_partition; 
-}
-
-memory_partition_unit::arbitration_metadata::arbitration_metadata(const struct memory_config *config) 
-: m_last_borrower(config->m_n_sub_partition_per_memory_channel - 1), 
-  m_private_credit(config->m_n_sub_partition_per_memory_channel, 0), 
-  m_shared_credit(0) 
-{
-    // each sub partition get at least 1 credit for forward progress 
-    // the rest is shared among with other partitions 
-    m_private_credit_limit = 1; 
-    m_shared_credit_limit = config->gpgpu_frfcfs_dram_sched_queue_size 
-                            + config->gpgpu_dram_return_queue_size 
-                            - (config->m_n_sub_partition_per_memory_channel - 1); 
-    if (config->gpgpu_frfcfs_dram_sched_queue_size == 0 
-        or config->gpgpu_dram_return_queue_size == 0) 
-    {
-        m_shared_credit_limit = 0; // no limit if either of the queue has no limit in size 
-    }
-    assert(m_shared_credit_limit >= 0); 
-}
+    char L2c_name[32];
+    // Saugata: multiple L2 banks share a common interface because they have a common DRAM request queue
+    // snprintf(L2c_name, 32, "L2_bank_%03d", m_id);
+    m_L2_num_banks = m_config->m_n_L2_banks_per_memory_channel;
+    m_L2interface = new L2interface(this);
+    m_mf_allocator = new partition_mf_allocator(config);
 
-bool memory_partition_unit::arbitration_metadata::has_credits(int inner_sub_partition_id) const 
-{
-    int spid = inner_sub_partition_id; 
-    if (m_private_credit[spid] < m_private_credit_limit) {
-        return true; 
-    } else if (m_shared_credit_limit == 0 || m_shared_credit < m_shared_credit_limit) {
-        return true; 
-    } else {
-        return false; 
+    if(!m_config->m_L2_config.disabled()) {
+       // Saugata: added multiple L2 cache banks
+       m_L2cache = new l2_cache *[m_L2_num_banks];
+
+       for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+          snprintf(L2c_name, 32, "L2_bank_%03d_%03d", m_id, i);
+          m_L2cache[i] = new l2_cache(L2c_name,m_config->m_L2_config,-1,-1,m_L2interface,m_mf_allocator,IN_PARTITION_L2_MISS_QUEUE);
+       }
     }
-}
 
-void memory_partition_unit::arbitration_metadata::borrow_credit(int inner_sub_partition_id) 
-{
-    int spid = inner_sub_partition_id; 
-    if (m_private_credit[spid] < m_private_credit_limit) {
-        m_private_credit[spid] += 1; 
-    } else if (m_shared_credit_limit == 0 || m_shared_credit < m_shared_credit_limit) {
-        m_shared_credit += 1; 
-    } else {
-        assert(0 && "DRAM arbitration error: Borrowing from depleted credit!"); 
+    n_mem_to_simt=0;
+    unsigned int icnt_L2;
+    unsigned int L2_dram;
+    unsigned int dram_L2;
+    unsigned int L2_icnt;
+    sscanf(m_config->gpgpu_L2_queue_config,"%u:%u:%u:%u", &icnt_L2,&L2_dram,&dram_L2,&L2_icnt );
+    // Saugata: added histograms to track queuing latencies
+    mpu_icnt_L2_latency_hist = new contention_histogram<unsigned int>(16, 20);
+    icnt_L2_latency_hist = new contention_histogram<unsigned int> *[m_L2_num_banks];
+    // Saugata: each cache bank now has its own interconnect->L2 queue and DRAM->L2 queue
+    m_icnt_L2_queue = new fifo_pipeline<mem_fetch> *[m_L2_num_banks];
+    m_dram_L2_queue = new fifo_pipeline<mem_fetch> *[m_L2_num_banks];
+    for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+        // Saugata: added histogram to track queuing latencies
+        icnt_L2_latency_hist[i] = new contention_histogram<unsigned int>(16, 20);
+        // Saugata TODO: check to see if we need unique names here...
+        m_icnt_L2_queue[i] = new fifo_pipeline<mem_fetch>("icnt-to-L2",0,icnt_L2); 
+        m_dram_L2_queue[i] = new fifo_pipeline<mem_fetch>("dram-to-L2",0,dram_L2);
     }
-    m_last_borrower = spid; 
-}
-
-void memory_partition_unit::arbitration_metadata::return_credit(int inner_sub_partition_id) 
-{
-    int spid = inner_sub_partition_id; 
-    if (m_private_credit[spid] > 0) {
-        m_private_credit[spid] -= 1; 
-    } else {
-        m_shared_credit -= 1; 
-    } 
-    assert((m_shared_credit >= 0) && "DRAM arbitration error: Returning more than available credits!"); 
-}
+    m_L2_dram_queue = new fifo_pipeline<mem_fetch>("L2-to-dram",0,L2_dram);
+    m_L2_icnt_queue = new fifo_pipeline<mem_fetch>("L2-to-icnt",0,L2_icnt);
+    wb_addr=-1;
 
-void memory_partition_unit::arbitration_metadata::print( FILE *fp ) const 
-{
-    fprintf(fp, "private_credit = "); 
-    for (unsigned p = 0; p < m_private_credit.size(); p++) {
-        fprintf(fp, "%d ", m_private_credit[p]); 
-    }
-    fprintf(fp, "(limit = %d)\n", m_private_credit_limit); 
-    fprintf(fp, "shared_credit = %d (limit = %d)\n", m_shared_credit, m_shared_credit_limit); 
-}
+    // Saugata: added input ports to MPU
+    input_ports = new mem_fetch * [m_config->m_n_ports_per_memory_channel];
+    input_port_enqueue_time = new unsigned long long[m_config->m_n_ports_per_memory_channel];
+
+    for(unsigned int p = 0; p < m_config->m_n_ports_per_memory_channel; ++p) {
+        input_ports[p] = NULL;
+    }
+
+    // Saugata: stats for input port queuing
+    num_bank_arbitrations = 0;
+    total_input_stall_cycles = 0;
+    max_input_stall_time = 0;
+    total_pending_input_requests = 0;
+    num_bank_arbitration_cycles = 0;
+    num_incomplete_bank_arbitration_cycles = 0;
+    max_slot_to_write = 0;
+
+    // Saugata: added contention model for L2 cache
+    assert(m_config->L2_latency > 0);
+    assert(m_config->m_n_L2_pipeline_stages > 0);
+    assert(m_config->L2_latency >= m_config->m_n_L2_pipeline_stages);
+
+    if(!m_config->m_L2_config.disabled()) {
+        unsigned int L2cache_serialized_latency = ((m_config->L2_latency - 1) / m_config->m_n_L2_pipeline_stages) + 1;
+        L2cache_contention = new queue_contention_model * [m_L2_num_banks];
 
-bool memory_partition_unit::busy() const 
-{
-    bool busy = false; 
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        if (m_sub_partition[p]->busy()) {
-            busy = true; 
+        for(unsigned int b = 0; b < m_L2_num_banks; ++b) {
+            L2cache_contention[b] = new queue_contention_model(m_config->m_n_L2_bank_ports, L2cache_serialized_latency);
         }
     }
-    return busy; 
-}
 
-void memory_partition_unit::cache_cycle(unsigned cycle) 
-{
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        m_sub_partition[p]->cache_cycle(cycle); 
+    bloom_filter_size = 0;
+    for(int i=0;i<1000000;i++)
+        bloom_filter[i] = false;
+    m_counter_at_queue = 0;
+    m_counter_at_queue2 = 0;
+    m_counter_cleared_cycles = 0;
+    m_counter_cleared_cycles2 = 0;
+}
+
+memory_partition_unit::~memory_partition_unit()
+{
+    // Saugata: need to delete the per-bank structures first...
+    for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+       delete m_L2cache[i];
+       delete m_icnt_L2_queue[i];
+       delete m_dram_L2_queue[i];
+       delete icnt_L2_latency_hist[i];
+       delete L2cache_contention[i];
     }
-}
+    delete [] m_icnt_L2_queue;
+    delete m_L2_dram_queue;
+    delete [] m_dram_L2_queue;
+    delete m_L2_icnt_queue;
+    delete m_L2cache;
+    delete m_L2interface;
 
-void memory_partition_unit::visualizer_print( gzFile visualizer_file ) const 
-{
-    m_dram->visualizer_print(visualizer_file);
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        m_sub_partition[p]->visualizer_print(visualizer_file); 
+    // Saugata: need to delete the input port structures
+    delete [] input_ports;
+    delete [] input_port_enqueue_time;
+
+    // Saugata: need to delete the L2 queue contention model
+    delete [] L2cache_contention;
+
+    // Saugata: need to delete the queue tracking histograms
+    delete [] icnt_L2_latency_hist;
+    delete mpu_icnt_L2_latency_hist;
+}
+
+//void memory_partition_unit::set_new_info(l2_warp_info* new_info, mem_fetch* mf)
+//{
+//    //Initializing
+////    new_info->set_t0((mf->get_tid()/m_config->warp_size)*m_config->warp_size);
+//    memory_partition_unit::update_warp_info(new_info, mf);
+//}
+
+//bool memory_partition_unit::update_warp_info(l2_warp_info** new_info, mem_fetch* mf)
+//{
+//    return update_warp_info(new_info,mf);
+//}
+
+bool memory_partition_unit::update_warp_info(l2_warp_info* new_info, mem_fetch* mf)
+{
+//    //Updating info
+    new_info->add_accessed();
+////    printf("HERE %d", m_config->warp_size);
+//    if(mf->get_tid() >= 0)
+//        new_info->add_accessed(mf->get_tid()%m_config->warp_size);
+//    else
+//        new_info->unrecognized_tid_add();
+//    //If hit, add hit. This part shadow the cache access
+    std::list<cache_event> events;
+    enum cache_request_status status = m_L2cache[mf->get_L2_bank_id()]->shadow_access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);
+    if(status == HIT || status == HIT_RESERVED)
+    {
+        new_info->add_hit();
+//        if(mf->get_tid() >= 0)
+//            new_info->add_hit(mf->get_tid()%m_config->warp_size); 
+    }  
+//
+//
+//// TODO: Rachata --> Another mechanism that is based on the instructions that use the cache
+//// So the mapping will both look at wid and inst
+//// Right now it only checks the hit rate given t0 
+//    
+    //(m_stats->L2_util_histogram[(int)(new_info->get_hit_rate())])++;
+    (m_stats->L2_util_histogram[(int)(new_info->get_hit_rate()*10)])++;
+
+    //clear if the last accessed is a while ago
+    if((mf->get_timestamp() - new_info->get_last_accessed()) > m_config->l2_info_clear)
+    {
+        new_info->clear();
+        m_stats->L2_info_cleared++;
+        assign_PCAL_tokens();
+        return false;
     }
-}
 
-// determine whether a given subpartition can issue to DRAM 
-bool memory_partition_unit::can_issue_to_dram(int inner_sub_partition_id) 
-{
-    int spid = inner_sub_partition_id; 
-    bool sub_partition_contention = m_sub_partition[spid]->dram_L2_queue_full(); 
-    bool has_dram_resource = m_arbitration_metadata.has_credits(spid); 
+    new_info->set_last_accessed(mf->get_timestamp());
 
-    MEMPART_DPRINTF("sub partition %d sub_partition_contention=%c has_dram_resource=%c\n", 
-                    spid, (sub_partition_contention)? 'T':'F', (has_dram_resource)? 'T':'F'); 
+    // set the hit util --> this is used when the cache is filled into l2
+    if(m_config->cache_aware_insertion == 1)
+        mf->set_l2util(new_info->get_hit_rate());
+    if(m_config->cache_aware_insertion == 2)
+        mf->set_l2util(1-new_info->get_hit_rate());
 
-    return (has_dram_resource && !sub_partition_contention); 
+    if(new_info->get_hit_rate() < m_config->target_warp_hit_ratio)
+    {
+        return true;
+    }
+    else
+    {
+        if((new_info->get_hit_rate() > m_config->target_warp_hit_ratio_high) && (new_info->get_hit_rate() < 1.0))
+        {
+//            m_stats->DRAM_high_prio++;
+            if(m_config->prioritize_mf)
+                mf->m_priority = 1;
+        }
+        return false;
+    }
 }
 
-int memory_partition_unit::global_sub_partition_id_to_local_id(int global_sub_partition_id) const
+bool memory_partition_unit::update_addr_info(address_info* new_info, mem_fetch* mf)
 {
-    return (global_sub_partition_id - m_id * m_config->m_n_sub_partition_per_memory_channel); 
-}
-
-void memory_partition_unit::dram_cycle() 
-{ 
-    // pop completed memory request from dram and push it to dram-to-L2 queue 
-    // of the original sub partition 
-    mem_fetch* mf_return = m_dram->return_queue_top();
-    if (mf_return) {
-        unsigned dest_global_spid = mf_return->get_sub_partition_id(); 
-        int dest_spid = global_sub_partition_id_to_local_id(dest_global_spid); 
-        assert(m_sub_partition[dest_spid]->get_id() == dest_global_spid); 
-        if (!m_sub_partition[dest_spid]->dram_L2_queue_full()) {
-            if( mf_return->get_access_type() == L1_WRBK_ACC ) {
-                m_sub_partition[dest_spid]->set_done(mf_return); 
-                delete mf_return;
-            } else {
-                m_sub_partition[dest_spid]->dram_L2_queue_push(mf_return);
-                mf_return->set_status(IN_PARTITION_DRAM_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-                m_arbitration_metadata.return_credit(dest_spid); 
-                MEMPART_DPRINTF("mem_fetch request %p return from dram to sub partition %d\n", mf_return, dest_spid); 
-            }
-            m_dram->return_queue_pop(); 
-        }
-    } else {
-        m_dram->return_queue_pop(); 
+    new_info->add_accessed();
+    if(new_info->update_reused(mf) < m_config->l2_reuse_threshold)
+        return true;
+    return false;
+}
+
+//TODO: Rewrite -- EAF cache
+//unsigned memory_partition_unit::get_hash_index(new_addr_type addr)
+//{
+//    unsigned address = (unsigned)addr;
+//    return (hashFunc1(addr)+hashFunc2(addr)*hashFunc3(addr))%m_config->l2_bloom_filter_bin_size;
+//}
+//
+//unsigned memory_partition_unit::get_bloom_filter_index(mem_fetch* mf)
+//{
+//    return get_hash_index(mf->get_addr());    
+//}
+//
+//
+
+void memory_partition_unit::bloom_filter_clear(){
+//    if(bloom_filter_size > 256){
+    if(bloom_filter_size > (m_config->l2_bloom_filter_size/2)){
+//       for(int i=0;i<512;i++)
+       for(unsigned int i=0;i< (m_config->l2_bloom_filter_size);i++)
+           bloom_filter[i] = false;
+       bloom_filter_size = 0;
+       m_stats->L2_bloom_filter_cleared++;
+    }
+}
+
+void memory_partition_unit::set_bloom_filter(mem_fetch* mf){
+    set_bloom_filter_loc(((int)hashFunc1(mf->get_addr()) & 0x7FFFF) %  m_config->l2_bloom_filter_size);
+    set_bloom_filter_loc(((int)hashFunc2(mf->get_addr()) & 0x7FFFF) %  m_config->l2_bloom_filter_size);
+    set_bloom_filter_loc(((int)hashFunc3(mf->get_addr()) & 0x7FFFF) %  m_config->l2_bloom_filter_size);
+
+//    set_bloom_filter_loc(hashFunc1((unsigned)mf->get_addr()) % 512);
+//    set_bloom_filter_loc(hashFunc2((unsigned)mf->get_addr()) % 512);
+//    set_bloom_filter_loc(hashFunc3((unsigned)mf->get_addr()) % 512);
+    bloom_filter_size++;
+}
+
+bool memory_partition_unit::bloom_filter_contain(new_addr_type addr){
+//    if( bloom_filter[hashFunc1((unsigned)addr) % 512] && 
+//        bloom_filter[hashFunc2((unsigned)addr) % 512] && 
+//        bloom_filter[hashFunc3((unsigned)addr) % 512] ) return true;
+//    else return false;
+
+    if( bloom_filter[((int)hashFunc1(addr) & 0x7FFFF) %  m_config->l2_bloom_filter_size] && 
+        bloom_filter[((int)hashFunc2(addr) & 0x7FFFF) %  m_config->l2_bloom_filter_size] && 
+        bloom_filter[((int)hashFunc3(addr) & 0x7FFFF) %  m_config->l2_bloom_filter_size] ) return true;
+    else return false;
+}
+
+// This function return true of the address should not be bypassed with EAF mechanism
+// By default, this will return false.
+// This function is only called during the actual bypassing mechanism. Returning the false
+// value will make sure the decision here do not meddle with the warp-level bypassing decision
+bool memory_partition_unit::bypass_addr(mem_fetch* mf, bool warp_level_decision){
+    //Clear the bloom filter if the size is too large
+    bloom_filter_clear();
+    bool return_val = false;
+    std::list<cache_event> events;
+    enum cache_request_status status = m_L2cache[mf->get_L2_bank_id()]->shadow_access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);
+    // Set the bloom filter if the cache access is a miss, but only choose
+    // not to bypass if this same block is in the bloom filter
+    if(status == MISS && warp_level_decision)
+    {
+        return_val = bloom_filter_contain(mf->get_addr());
+        set_bloom_filter(mf);
     }
-    
-    m_dram->cycle(); 
-    m_dram->dram_log(SAMPLELOG);   
-
-    if( !m_dram->full() ) {
-        // L2->DRAM queue to DRAM latency queue
-        // Arbitrate among multiple L2 subpartitions 
-        int last_issued_partition = m_arbitration_metadata.last_borrower(); 
-        for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-            int spid = (p + last_issued_partition + 1) % m_config->m_n_sub_partition_per_memory_channel; 
-            if (!m_sub_partition[spid]->L2_dram_queue_empty() && can_issue_to_dram(spid)) {
-                mem_fetch *mf = m_sub_partition[spid]->L2_dram_queue_top();
-                m_sub_partition[spid]->L2_dram_queue_pop();
-                MEMPART_DPRINTF("Issue mem_fetch request %p from sub partition %d to dram\n", mf, spid); 
-                dram_delay_t d;
-                d.req = mf;
-                d.ready_cycle = gpu_sim_cycle+gpu_tot_sim_cycle + m_config->dram_latency;
-                m_dram_latency_queue.push_back(d);
-                mf->set_status(IN_PARTITION_DRAM_LATENCY_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-                m_arbitration_metadata.borrow_credit(spid); 
-                break;  // the DRAM should only accept one request per cycle 
-            }
+    // Update EAF Status for the insertion policy
+    if(m_config->eaf_insertion)
+    {
+        //If we do EAF insertion policy, set the cache to the MRU position
+        if(return_val)
+            mf->set_l2util(1);
+        else
+            mf->set_l2util(0);
+    }
+    // TODO: This shit gives segfault sometimes :(
+    if(!m_config->reuse_aware)
+        return false;
+    else 
+        return return_val;
+}
+
+//bool memory_partition_unit::bypass_addr(mem_fetch* mf)
+//{
+//    bool return_val=false;
+//    if(m_config->reuse_aware)
+//    {
+//        address_info* new_info;
+//        unsigned index = get_bloom_filter_index(mf);
+//        //new_info = new address_info(index, m_config);
+//        std::map<unsigned, address_info*>::iterator r = addr_info.find(index);
+//        if(r==addr_info.end())
+//        {
+//            new_info = new address_info(index, m_config);
+//            update_addr_info(new_info, mf);
+//            addr_info.insert(std::pair<unsigned,address_info*>(index, new_info)); 
+//        }
+//        else
+//        {
+//            new_info = r->second;
+//            return_val = update_addr_info(new_info,mf);
+//            addr_info.erase(r);
+//            addr_info.insert(std::pair<unsigned,address_info*>(index, new_info));
+//        }
+//    }
+//    if(return_val == true)
+//        m_stats->L2_reuse_bypassed++;
+//    return return_val;
+//}
+
+bool memory_partition_unit::bypass_cache_rop(mem_fetch *mf){
+    // Bypass based on PC or WID
+    unsigned mf_wid = m_config->l2_bypass_combined?((unsigned)hashFunc1(mf->get_pc())%m_config->l2_bypass_pc_range)&mf->get_wid():(m_config->l2_bypass_pc?((unsigned)hashFunc1(mf->get_pc())%m_config->l2_bypass_pc_range):mf->get_wid());
+//        // By default, do not bypass the cache
+    bool return_val = false;
+    l2_warp_info* new_info;
+
+    // Saugata: for random bypassing, just use the bypass_cache prediction
+    // Saugata TODO: this won't work for bypass-after-queue
+    if(m_config->l2_bypass_randomly) {
+        return mf->m_bypassed;
+    }
+
+    // Rachata: PCAL HPCA'15
+    if(m_config->l2_bypass_PCAL) {
+        return mf->m_bypassed;
+    }
+
+//        // Search for the associated warpID in the stats
+//        // If not found, don't bypass the cache
+    std::map<unsigned, l2_warp_info*>::iterator r = warp_info.find(mf_wid);
+    if(r==warp_info.end())
+    {
+        return false;
+    }
+    else
+    {
+        new_info = r->second;
+        if(new_info->get_hit_rate() < m_config->target_warp_hit_ratio)
+        {
+            return_val = true;
         }
     }
 
-    // DRAM latency queue
-    if( !m_dram_latency_queue.empty() && ( (gpu_sim_cycle+gpu_tot_sim_cycle) >= m_dram_latency_queue.front().ready_cycle ) && !m_dram->full() ) {
-        mem_fetch* mf = m_dram_latency_queue.front().req;
-        m_dram_latency_queue.pop_front();
-        m_dram->push(mf);
+    // Return the decision
+    if(m_config->cache_aware && (new_info->get_total_accessed()>m_config->l2_warmup))
+    {
+        if(return_val == true)
+            m_stats->bypassed_rop_count_in_bypass++;
+        return return_val;
     }
+    else
+        return false;
 }
 
-void memory_partition_unit::set_done( mem_fetch *mf )
+// Rachata: PCAL --> Assign new set of tokens to latest used warps at every epoch
+void memory_partition_unit::assign_PCAL_tokens()
 {
-    unsigned global_spid = mf->get_sub_partition_id(); 
-    int spid = global_sub_partition_id_to_local_id(global_spid); 
-    assert(m_sub_partition[spid]->get_id() == global_spid); 
-    if (mf->get_access_type() == L1_WRBK_ACC || mf->get_access_type() == L2_WRBK_ACC) {
-        m_arbitration_metadata.return_credit(spid); 
-        MEMPART_DPRINTF("mem_fetch request %p return from dram to sub partition %d\n", mf, spid); 
+    unsigned count = 0;
+    token_list_PCAL.clear();
+    while(count<m_config->l2_bypass_PCAL_num_tokens && (!warp_queue_PCAL.empty()))
+    {
+        token_list_PCAL.insert(warp_queue_PCAL.front());
+        warp_queue_PCAL.pop_front();
+        count++;
     }
-    m_sub_partition[spid]->set_done(mf); 
+    leftover_tokens = m_config->l2_bypass_PCAL_num_tokens - count;
 }
 
-void memory_partition_unit::set_dram_power_stats(unsigned &n_cmd,
-                                                 unsigned &n_activity,
-                                                 unsigned &n_nop,
-                                                 unsigned &n_act,
-                                                 unsigned &n_pre,
-                                                 unsigned &n_rd,
-                                                 unsigned &n_wr,
-                                                 unsigned &n_req) const
+// Update the of latest used warps during the time warps get active
+void memory_partition_unit::update_latest_used_warps(unsigned wid)
 {
-    m_dram->set_dram_power_stats(n_cmd, n_activity, n_nop, n_act, n_pre, n_rd, n_wr, n_req);
+    warp_queue_PCAL.remove(wid);
+    warp_queue_PCAL.push_front(wid);
 }
 
-void memory_partition_unit::print( FILE *fp ) const
+//Rachata : Bypassing mechanism in PCAL. Based on the warp ID, the recently used
+//warps get the token first. All the counter here share the same
+//counter as LMNO so the counters get refreshed at the same interval
+bool memory_partition_unit::bypass_PCAL(mem_fetch *mf)
 {
-    fprintf(fp, "Memory Partition %u: \n", m_id); 
-    for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
-        m_sub_partition[p]->print(fp); 
-    }
-    fprintf(fp, "In Dram Latency Queue (total = %zd): \n", m_dram_latency_queue.size()); 
-    for (std::list<dram_delay_t>::const_iterator mf_dlq = m_dram_latency_queue.begin(); 
-         mf_dlq != m_dram_latency_queue.end(); ++mf_dlq) {
-        mem_fetch *mf = mf_dlq->req; 
-        fprintf(fp, "Ready @ %llu - ", mf_dlq->ready_cycle); 
-        if (mf) 
-            mf->print(fp); 
-        else 
-            fprintf(fp, " <NULL mem_fetch?>\n"); 
+    // Get the warpID
+    unsigned mf_wid = mf->get_wid();
+    // Update the last of last used warps
+    update_latest_used_warps(mf_wid);
+    // Check if the wid has the token -> If there is no token then bypass the warp
+    if(token_list_PCAL.find(mf_wid)!=token_list_PCAL.end())
+        return false; //Note: false means not bypass, true means bypass the warp
+    else
+    {
+        if(leftover_tokens > 0)
+        {
+            token_list_PCAL.insert(mf_wid);
+            leftover_tokens--;
+            return false;
+        }
+        return true; 
     }
-    m_dram->print(fp); 
 }
 
-memory_sub_partition::memory_sub_partition( unsigned sub_partition_id, 
-                                            const struct memory_config *config,
-                                            class memory_stats_t *stats )
+
+bool memory_partition_unit::bypass_cache(mem_fetch *mf)
 {
-    m_id = sub_partition_id;
-    m_config=config;
-    m_stats=stats;
+    // Saugata: commented out because it's currently unused
+    // unsigned target_ratio = m_config->target_warp_hit_ratio;
+    // Bypass based on PC or WID
+    unsigned mf_wid = m_config->l2_bypass_combined?((unsigned)hashFunc1(mf->get_pc())%m_config->l2_bypass_pc_range)&mf->get_wid():(m_config->l2_bypass_pc?((unsigned)hashFunc1(mf->get_pc())%m_config->l2_bypass_pc_range):mf->get_wid());
+//        // By default, do not bypass the cache
+    bool return_val = false;
+    l2_warp_info* new_info;
+//        // Search for the associated warpID in the stats
+//        // If not found, don't bypass the cache
+    std::map<unsigned, l2_warp_info*>::iterator r = warp_info.find(mf_wid);
+    if(r==warp_info.end())
+    {
+// Rachata: delete this object at the end, in case of memory leaks -- a few runs look ok
+        new_info = new l2_warp_info(mf_wid, m_config);
+        update_warp_info(new_info, mf);
+        warp_info.insert(std::pair<unsigned,l2_warp_info*>(mf_wid, new_info));
+    }
+    else
+    {
+      // If found, figure if we should use the cache
+        new_info = r->second;
+        return_val = update_warp_info(new_info, mf);
+        warp_info.erase(r);
+        warp_info.insert(std::pair<unsigned,l2_warp_info*>(mf_wid, new_info));
+        //update_warp_info(&(r->second), mf);
+    }
 
-    assert(m_id < m_config->m_n_mem_sub_partition); 
+    // Saugata: for random bypassing, override the decision based on the shadow access
+    if(m_config->l2_bypass_PCAL) {
+        return_val = bypass_PCAL(mf);
+    }
 
-    char L2c_name[32];
-    snprintf(L2c_name, 32, "L2_bank_%03d", m_id);
-    m_L2interface = new L2interface(this);
-    m_mf_allocator = new partition_mf_allocator(config);
 
-    if(!m_config->m_L2_config.disabled())
-       m_L2cache = new l2_cache(L2c_name,m_config->m_L2_config,-1,-1,m_L2interface,m_mf_allocator,IN_PARTITION_L2_MISS_QUEUE);
 
-    unsigned int icnt_L2;
-    unsigned int L2_dram;
-    unsigned int dram_L2;
-    unsigned int L2_icnt;
-    sscanf(m_config->gpgpu_L2_queue_config,"%u:%u:%u:%u", &icnt_L2,&L2_dram,&dram_L2,&L2_icnt );
-    m_icnt_L2_queue = new fifo_pipeline<mem_fetch>("icnt-to-L2",0,icnt_L2); 
-    m_L2_dram_queue = new fifo_pipeline<mem_fetch>("L2-to-dram",0,L2_dram);
-    m_dram_L2_queue = new fifo_pipeline<mem_fetch>("dram-to-L2",0,dram_L2);
-    m_L2_icnt_queue = new fifo_pipeline<mem_fetch>("L2-to-icnt",0,L2_icnt);
-    wb_addr=-1;
-}
+    // Saugata: for random bypassing, override the decision based on the shadow access
+    if(m_config->l2_bypass_randomly) {
+        if(rand() < (RAND_MAX * m_config->l2_random_bypass_rate)) {
+            return_val = true;
+        }
+        else {
+            return_val = false;
+        }
+    }
 
-memory_sub_partition::~memory_sub_partition()
-{
-    delete m_icnt_L2_queue;
-    delete m_L2_dram_queue;
-    delete m_dram_L2_queue;
-    delete m_L2_icnt_queue;
-    delete m_L2cache;
-    delete m_L2interface;
+    // Saugata: random bypassing avoids the warm-up period
+    if(return_val == true && ((new_info->get_total_accessed()>m_config->l2_warmup) || m_config->l2_bypass_randomly || m_config->l2_bypass_PCAL))
+    {
+        // Saugata: no longer setting mf bypassing status until the end
+        // mf->m_bypassed = true;
+        m_stats->L2_bypassed++;
+    }
+
+// UPDATE address info
+    // Saugata: not called for random bypass
+    if(!m_config->l2_bypass_randomly && bypass_addr(mf, return_val))
+    {
+        //Do not bypass if the address is used often
+        m_stats->L2_reuse_bypassed++;
+        // Saugata: no longer setting mf bypassing status until the end
+        // mf->m_bypassed = false;
+        return_val = false;
+    }
+
+    // Return the decision
+    // Saugata: random bypassing avoids the warm-up period
+    if(m_config->cache_aware && ((new_info->get_total_accessed()>m_config->l2_warmup) || m_config->l2_bypass_randomly || m_config->l2_bypass_PCAL)) {
+        // Saugata: set mf bypassing status at the end
+        mf->m_bypassed = return_val;
+        return return_val;
+    }
+    else
+        return false;
 }
 
-void memory_sub_partition::cache_cycle( unsigned cycle )
+void memory_partition_unit::cache_cycle( unsigned cycle )
 {
     // L2 fill responses
     if( !m_config->m_L2_config.disabled()) {
-       if ( m_L2cache->access_ready() && !m_L2_icnt_queue->full() ) {
-           mem_fetch *mf = m_L2cache->next_access();
-           if(mf->get_access_type() != L2_WR_ALLOC_R){ // Don't pass write allocate read request back to upper level cache
-				mf->set_reply();
-				mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-				m_L2_icnt_queue->push(mf);
-           }else{
-				m_request_tracker.erase(mf);
-				delete mf;
+        // Saugata: now checking each bank every cycle
+        for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+           unsigned int bank_id = (i + m_L2_current_bank) % m_L2_num_banks;
+
+           // Saugata: update the clock within the contention model
+           L2cache_contention[bank_id]->cycle(cycle);
+
+           if ( m_L2cache[bank_id]->access_ready() && !m_L2_icnt_queue->full() ) {
+               mem_fetch *mf = m_L2cache[bank_id]->next_access();
+               // TODO: Skip the fill if bypassed L2
+               // if(m_config->cache_aware && !mf->m_bypassed)
+               // {
+               if(mf->get_access_type() != L2_WR_ALLOC_R){ // Don't pass write allocate read request back to upper level cache
+                    mf->set_reply();
+                    mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+                    m_L2_icnt_queue->push(mf);
+                    n_mem_to_simt+=mf->get_num_flits(false); // Interconnect power stats (# of flits sent to the SMs)
+               }else{
+                    m_request_tracker.erase(mf);
+                    delete mf;
+               }
+               // }
            }
        }
+       // Saugata: fake bank access randomness by rotating through the starting bank
+       m_L2_current_bank = (m_L2_current_bank + 1) % m_L2_num_banks;
     }
 
     // DRAM to L2 (texture) and icnt (not texture)
-    if ( !m_dram_L2_queue->empty() ) {
-        mem_fetch *mf = m_dram_L2_queue->top();
-        if ( !m_config->m_L2_config.disabled() && m_L2cache->waiting_for_fill(mf) ) {
-            if (m_L2cache->fill_port_free()) {
-                mf->set_status(IN_PARTITION_L2_FILL_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-                m_L2cache->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
-                m_dram_L2_queue->pop();
-            }
-        } else if ( !m_L2_icnt_queue->full() ) {
-            mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-            m_L2_icnt_queue->push(mf);
-            m_dram_L2_queue->pop();
+    // Saugata: now checking each bank every cycle
+    for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+        unsigned int bank_id = (i + m_L2_current_bank) % m_L2_num_banks;
+        if ( !m_dram_L2_queue[bank_id]->empty() ) {
+            mem_fetch *mf = m_dram_L2_queue[bank_id]->top();
+            // if ( (!m_config->cache_aware || !mf->m_bypassed) && !m_config->m_L2_config.disabled() && m_L2cache[bank_id]->waiting_for_fill(mf) ) {
+            if ( !m_config->m_L2_config.disabled() && m_L2cache[bank_id]->waiting_for_fill(mf) ) {
+                if(!m_config->cache_aware || !mf->m_bypassed) {
+                    mf->set_status(IN_PARTITION_L2_FILL_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+                    m_L2cache[bank_id]->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
+                    m_dram_L2_queue[bank_id]->pop();
+                }
+                // Saugata: bypassing returning requests to prevent invoking a fill when cache allows bypassing
+                else if(m_config->cache_aware && !m_L2_icnt_queue->full()) {
+                    if(mf->get_access_type() != L2_WR_ALLOC_R) { // Don't pass write allocate read request back to upper level cache
+                        mf->set_reply();
+                        mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+                        m_L2_icnt_queue->push(mf);
+                        m_dram_L2_queue[bank_id]->pop();
+                        n_mem_to_simt+=mf->get_num_flits(false); // Interconnect power stats (# of flits sent to the SMs)
+                    }
+                    else {
+                        m_request_tracker.erase(mf);
+                        delete mf;
+                    }
+                }
+            } else if ( !m_L2_icnt_queue->full() ) {
+                mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+                m_L2_icnt_queue->push(mf);
+                m_dram_L2_queue[bank_id]->pop();
+                n_mem_to_simt+=mf->get_num_flits(false); // Interconnect power stats (# of flits sent to the SMs)
+            }
         }
     }
+    // Saugata: fake bank access randomness by rotating through the starting bank
+    m_L2_current_bank = (m_L2_current_bank + 1) % m_L2_num_banks;
+
+    // Saugata: NOTE that old version of DRAM -> L2/icnt not copied from Rachata's code
 
     // prior L2 misses inserted into m_L2_dram_queue here
-    if( !m_config->m_L2_config.disabled() )
-       m_L2cache->cycle();
+    if( !m_config->m_L2_config.disabled() ) {
+        for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+            unsigned int bank_id = (i + m_L2_current_bank) % m_L2_num_banks;
+            m_L2cache[bank_id]->cycle();
+        }
+        // Saugata: fake bank access randomness by rotating through the starting bank
+        m_L2_current_bank = (m_L2_current_bank + 1) % m_L2_num_banks;
+    }
 
-    // new L2 texture accesses and/or non-texture accesses
-    if ( !m_L2_dram_queue->full() && !m_icnt_L2_queue->empty() ) {
-        mem_fetch *mf = m_icnt_L2_queue->top();
-        if ( !m_config->m_L2_config.disabled() &&
-              ( (m_config->m_L2_texure_only && mf->istexture()) || (!m_config->m_L2_texure_only) )
-           ) {
-            // L2 is enabled and access is for L2
-            bool output_full = m_L2_icnt_queue->full(); 
-            bool port_free = m_L2cache->data_port_free(); 
-            if ( !output_full && port_free ) {
-                std::list<cache_event> events;
-                enum cache_request_status status = m_L2cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);
-                bool write_sent = was_write_sent(events);
-                bool read_sent = was_read_sent(events);
-
-                if ( status == HIT ) {
-                    if( !write_sent ) {
-                        // L2 cache replies
-                        assert(!read_sent);
-                        if( mf->get_access_type() == L1_WRBK_ACC ) {
-                            m_request_tracker.erase(mf);
-                            delete mf;
+    // Saugata: now checking each bank every cycle
+    for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+        unsigned int bank_id = (i + m_L2_current_bank) % m_L2_num_banks;
+
+        // new L2 texture accesses and/or non-texture accesses
+        if ( !m_L2_dram_queue->full() && !m_icnt_L2_queue[bank_id]->empty() ) {
+            mem_fetch *mf = m_icnt_L2_queue[bank_id]->top();
+
+            // Saugata: read time that request entered queue
+            unsigned long long mf_entry_time = mf->at_status_since();
+
+            // Rachata --> will inject the l2_filter here
+            // Saugata: bypassing here now only called when queue is not bypassed
+            // Saugata: bringing the bypass call out so we collect stats no matter what
+            bool bypass_current_request = false;
+            assert(!m_config->cache_aware || !mf->m_bypassed);
+            if(m_config->l2_bypass_after_queue) {
+                bypass_current_request = bypass_cache(mf);
+            }
+
+            // if ( !m_config->m_L2_config.disabled() && 
+            if ( (!m_config->cache_aware || !bypass_current_request) && !m_config->m_L2_config.disabled() &&
+                  ( (m_config->m_L2_texure_only && mf->istexture()) || (!m_config->m_L2_texure_only) )
+               ) {
+                // L2 is enabled and access is for L2
+                // Saugata: added contention to only access L2 when it's available
+                // if ( !m_L2_icnt_queue->full() ) 
+                if(!m_L2_icnt_queue->full() && L2cache_contention[bank_id]->available()) {
+                    std::list<cache_event> events;
+                    enum cache_request_status status = m_L2cache[bank_id]->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);
+                    bool write_sent = was_write_sent(events);
+                    bool read_sent = was_read_sent(events);
+     
+                    // TODO: If high priority, but not a hit --> set the priority at the MC
+                    // This is done at dram_sched.cc when the request is added. mem_fetch already have the priority data
+                    if ( status == HIT ) {
+                        if( !write_sent ) {
+                            // L2 cache replies
+                            assert(!read_sent);
+                            if( mf->get_access_type() == L1_WRBK_ACC ) {
+                                m_request_tracker.erase(mf);
+                                delete mf;
+                            } else {
+                                mf->set_reply();
+                                mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+                                m_L2_icnt_queue->push(mf);
+                                n_mem_to_simt+=mf->get_num_flits(false); // Interconnect power stats (# of flits sent to the SMs)
+                            }
+                            // Saugata: sample latency into histogram
+                            icnt_L2_latency_hist[bank_id]->sample(gpu_sim_cycle + gpu_tot_sim_cycle - mf_entry_time);
+                            mpu_icnt_L2_latency_hist->sample(gpu_sim_cycle + gpu_tot_sim_cycle - mf_entry_time);
+                            m_icnt_L2_queue[bank_id]->pop();
+                            // Saugata: cache has been accessed; simulate contention
+                            L2cache_contention[bank_id]->schedule();
                         } else {
-                            mf->set_reply();
-                            mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-                            m_L2_icnt_queue->push(mf);
+                            assert(write_sent);
+                            // Saugata: sample latency into histogram
+                            icnt_L2_latency_hist[bank_id]->sample(gpu_sim_cycle + gpu_tot_sim_cycle - mf_entry_time);
+                            mpu_icnt_L2_latency_hist->sample(gpu_sim_cycle + gpu_tot_sim_cycle - mf_entry_time);
+                            m_icnt_L2_queue[bank_id]->pop();
+                            // Saugata: cache has been accessed; simulate contention
+                            L2cache_contention[bank_id]->schedule();
                         }
-                        m_icnt_L2_queue->pop();
+                    } else if ( status != RESERVATION_FAIL ) {
+                        // L2 cache accepted request
+                        // Saugata: sample latency into histogram
+                        icnt_L2_latency_hist[bank_id]->sample(gpu_sim_cycle + gpu_tot_sim_cycle - mf_entry_time);
+                        mpu_icnt_L2_latency_hist->sample(gpu_sim_cycle + gpu_tot_sim_cycle - mf_entry_time);
+                        m_icnt_L2_queue[bank_id]->pop();
+                        // Saugata: cache has been accessed; simulate contention
+                        L2cache_contention[bank_id]->schedule();
                     } else {
-                        assert(write_sent);
-                        m_icnt_L2_queue->pop();
+                        assert(!write_sent);
+                        assert(!read_sent);
+                        // L2 cache lock-up: will try again next cycle
+                        // Saugata: since cache is locked, assume here that it is not processing a full request and therefore doesn't need to be contended for...
                     }
-                } else if ( status != RESERVATION_FAIL ) {
-                    // L2 cache accepted request
-                    m_icnt_L2_queue->pop();
-                } else {
-                    assert(!write_sent);
-                    assert(!read_sent);
-                    // L2 cache lock-up: will try again next cycle
                 }
+            } else {
+                // L2 is disabled or non-texture access to texture-only L2
+                mf->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+                m_L2_dram_queue->push(mf);
+                // Saugata: sample latency into histogram
+                icnt_L2_latency_hist[bank_id]->sample(gpu_sim_cycle + gpu_tot_sim_cycle - mf_entry_time);
+                mpu_icnt_L2_latency_hist->sample(gpu_sim_cycle + gpu_tot_sim_cycle - mf_entry_time);
+                m_icnt_L2_queue[bank_id]->pop();
             }
-        } else {
-            // L2 is disabled or non-texture access to texture-only L2
-            mf->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
-            m_L2_dram_queue->push(mf);
-            m_icnt_L2_queue->pop();
         }
     }
+    // Saugata: fake bank access randomness by rotating through the starting bank
+    m_L2_current_bank = (m_L2_current_bank + 1) % m_L2_num_banks;
 
     // ROP delay queue
-    if( !m_rop.empty() && (cycle >= m_rop.front().ready_cycle) && !m_icnt_L2_queue->full() ) {
+    // Saugata: modified to check proper bank queue
+    // if( !m_rop.empty() && (cycle >= m_rop.front().ready_cycle) && !m_icnt_L2_queue->full() ) {
+    if( !m_rop.empty() && (cycle >= m_rop.front().ready_cycle) ) {
         mem_fetch* mf = m_rop.front().req;
-        m_rop.pop();
-        m_icnt_L2_queue->push(mf);
-        mf->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+        if(mf) {
+            // Saugata: now, if the request is tagged for bypass, we will bypass the queue directly and send to DRAM
+            if(m_config->cache_aware && mf->m_bypassed) {
+                if(!m_L2_dram_queue->full()) {
+                    m_rop.pop();
+                    mf->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+                    m_L2_dram_queue->push(mf);
+                }
+            }
+            else if(!m_icnt_L2_queue[mf->get_L2_bank_id()]->full()) {
+                m_rop.pop();
+                m_icnt_L2_queue[mf->get_L2_bank_id()]->push(mf);
+                mf->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+            }
+        }
     }
 }
 
-bool memory_sub_partition::full() const
-{
-    return m_icnt_L2_queue->full();
-}
-
-bool memory_sub_partition::L2_dram_queue_empty() const
-{
-   return m_L2_dram_queue->empty(); 
-}
-
-class mem_fetch* memory_sub_partition::L2_dram_queue_top() const
-{
-   return m_L2_dram_queue->top(); 
-}
-
-void memory_sub_partition::L2_dram_queue_pop() 
+bool memory_partition_unit::full() const
 {
-   m_L2_dram_queue->pop(); 
-}
+    // Saugata: no longer checking interconnect->L2 queue - this is now done when we perform bank arbitration
+    // return m_icnt_L2_queue->full();
 
-bool memory_sub_partition::dram_L2_queue_full() const
-{
-   return m_dram_L2_queue->full(); 
+    // Saugata: check to see if there is an open port available
+    // Saugata: input ports are compacted, so just check the last slot
+    return (bool)(input_ports[m_config->m_n_ports_per_memory_channel - 1]);
 }
 
-void memory_sub_partition::dram_L2_queue_push( class mem_fetch* mf )
+// Saugata: added function to print warp hit rates
+void memory_partition_unit::print_warp_hit_rates(FILE *fp) const
 {
-   m_dram_L2_queue->push(mf); 
+    fprintf(fp, "\nWarp Hit Rates in MPU %u:\n", m_id);
+    for(std::map<unsigned, l2_warp_info*>::const_iterator it = warp_info.begin(); it != warp_info.end(); ++it) {
+        assert(it->second);
+        fprintf(fp, "-- MPU %u, Warp %u: %f\n", m_id, it->first, it->second->get_hit_rate());
+    }
+    fprintf(fp, "\n");
 }
 
-void memory_sub_partition::print_cache_stat(unsigned &accesses, unsigned &misses) const
+void memory_partition_unit::print_cache_stat(unsigned &accesses, unsigned &misses) const
 {
     FILE *fp = stdout;
-    if( !m_config->m_L2_config.disabled() )
-       m_L2cache->print(fp,accesses,misses);
+    if( !m_config->m_L2_config.disabled() ) {
+        // Saugata: each bank prints its own stats
+        for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+            m_L2cache[i]->print(fp,accesses,misses);
+        }
+    }
 }
 
-void memory_sub_partition::print( FILE *fp ) const
+void memory_partition_unit::print( FILE *fp ) const
 {
+    // Saugata: separating outputs for clarity
+    fprintf(fp, "\n\nMemory Partition %u: \n", m_id); 
+
     if ( !m_request_tracker.empty() ) {
-        fprintf(fp,"Memory Sub Parition %u: pending memory requests:\n", m_id);
+        fprintf(fp,"Memory Parition %u: pending memory requests:\n", m_id);
         for ( std::set<mem_fetch*>::const_iterator r=m_request_tracker.begin(); r != m_request_tracker.end(); ++r ) {
             mem_fetch *mf = *r;
             if ( mf )
@@ -476,22 +829,101 @@
                 fprintf(fp," <NULL mem_fetch?>\n");
         }
     }
-    if( !m_config->m_L2_config.disabled() )
-       m_L2cache->display_state(fp);
+    if( !m_config->m_L2_config.disabled() ) {
+        // Saugata: each bank displays its own state
+        for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+            m_L2cache[i]->display_state(fp);
+        }
+    }
+    m_dram->print(fp); 
+
+    // Saugata: added print calls for icnt->L2 latency histogram
+    char hist_name[32];
+    for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+        snprintf(hist_name, 32, "MPU %u, Bank %u", m_id, i);
+        fprintf(fp, "Interconnect -> L2 Queue Latency Histogram for L2 Cache Bank %u\n", i);
+        icnt_L2_latency_hist[i]->print(fp, hist_name);
+    }
+    snprintf(hist_name, 32, "MPU %u, Total", m_id);
+    fprintf(fp, "Overall Interconnect -> L2 Queue Latency Histogram for All L2 Banks\n");
+    mpu_icnt_L2_latency_hist->print(fp, hist_name);
+
+    // Saugata: added print calls for input port stats
+    fprintf(fp, "Input Port Queuing Behavior\n");
+    fprintf(fp, "-- MPU %u Number of Requests Arbitrated    : %llu\n", m_id, num_bank_arbitrations);
+    fprintf(fp, "-- MPU %u Number of Cycles Called          : %llu\n", m_id, num_bank_arbitration_cycles);
+    fprintf(fp, "-- MPU %u Number of Cycles Requests Stalled: %llu\n", m_id, num_incomplete_bank_arbitration_cycles);
+    fprintf(fp, "-- MPU %u Request Arbitration Delay\n", m_id);
+    fprintf(fp, "---- MPU %u Arbitration Average: ", m_id);
+    if(num_bank_arbitrations) {
+        fprintf(fp, "%f", (double)(total_input_stall_cycles) / num_bank_arbitrations);
+    }
+    else {
+        fprintf(fp, "0");
+    }
+    fprintf(fp, " cycles\n");
+    fprintf(fp, "---- MPU %u Arbitration Maximum: %u cycles\n", m_id, max_input_stall_time);
+    fprintf(fp, "-- MPU %u Requests Stalled (Not Issued to Bank) Per Cycle\n", m_id);
+    fprintf(fp, "---- MPU %u Per Cycle Average (incl. stall-free cycles): ", m_id);
+    if(num_bank_arbitration_cycles) {
+        fprintf(fp, "%f\n", (double)(total_pending_input_requests) / num_bank_arbitration_cycles);
+    }
+    else {
+        fprintf(fp, "0\n");
+    }
+    fprintf(fp, "---- MPU %u Per Cycle Average (excl. stall-free cycles): ", m_id);
+    if(num_incomplete_bank_arbitration_cycles) {
+        fprintf(fp, "%f\n", (double)(total_pending_input_requests) / num_incomplete_bank_arbitration_cycles);
+    }
+    else {
+        fprintf(fp, "0\n");
+    }
+    fprintf(fp, "---- MPU %u Per Cycle Maximum: %u\n", m_id, max_slot_to_write);
+    fprintf(fp, "\n");
+
+    // Saugata: separating outputs for clarity
+    fprintf(fp, "\n\n");
+}
+
+void memory_stats_t::print( FILE *fp )
+{
+
+    fprintf(fp,"gpgpu_l2_write_miss = %d\n", L2_write_miss);
+    fprintf(fp,"gpgpu_l2_write_access = %d\n", L2_write_access);
+    fprintf(fp,"gpgpu_l2_read_miss = %d\n", L2_read_miss);
+    fprintf(fp,"gpgpu_l2_read_access = %d\n", L2_read_access);
+
+    fprintf(fp,"gpgpu_l2_warp_bypassed_total = %d\n", L2_bypassed);
+    fprintf(fp,"gpgpu_l2_address_bypassed_total = %d\n", L2_reuse_bypassed);
+    fprintf(fp,"gpgpu_l2_rop_bypassed_total = %d\n", bypassed_rop_count);
+    fprintf(fp,"gpgpu_l2_non_rop_bypassed_total = %d\n", non_bypassed_rop_count);
+    fprintf(fp,"gpgpu_l2_rop_bypassed_total_in_bypass = %d\n", bypassed_rop_count_in_bypass);
+    fprintf(fp,"gpgpu_l2_bloom_filter_cleared = %d\n", L2_bloom_filter_cleared);
+    fprintf(fp,"gpgpu_l2_warp_info_cleared = %d\n", L2_info_cleared);
+    fprintf(fp,"gpgpu_DRAM_high_priority_request_counts = %d\n", DRAM_high_prio);
+    fprintf(fp,"gpgpu_DRAM_request_scheduled_from_high_priority_request_counts = %d\n", sched_from_high_prio);
+    fprintf(fp,"gpgpu_l2_warp_utility_histogram = {%lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu}\n", L2_util_histogram[0],L2_util_histogram[1],L2_util_histogram[2],L2_util_histogram[3],L2_util_histogram[4],L2_util_histogram[5],L2_util_histogram[6],L2_util_histogram[7],L2_util_histogram[8],L2_util_histogram[9],L2_util_histogram[10]);
+
+    // Saugata: these two sets of counters are now irrelevant
+    // fprintf(fp,"gpgpu_rop_bin_count = {%lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu}\n", rop_bin_count[0],rop_bin_count[1],rop_bin_count[2],rop_bin_count[3],rop_bin_count[4],rop_bin_count[5],rop_bin_count[6],rop_bin_count[7],rop_bin_count[8],rop_bin_count[9],rop_bin_count[10],rop_bin_count[11],rop_bin_count[12],rop_bin_count[13]);
+
+    // fprintf(fp,"gpgpu_bypass_ratio_bins = {%lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu}\n", bypassed_ratio[0],bypassed_ratio[1],bypassed_ratio[2],bypassed_ratio[3],bypassed_ratio[4],bypassed_ratio[5],bypassed_ratio[6],bypassed_ratio[7],bypassed_ratio[8],bypassed_ratio[9], bypassed_ratio[10]);
+
 }
 
 void memory_stats_t::visualizer_print( gzFile visualizer_file )
 {
-   // gzprintf(visualizer_file, "Ltwowritemiss: %d\n", L2_write_miss);
-   // gzprintf(visualizer_file, "Ltwowritehit: %d\n",  L2_write_access-L2_write_miss);
-   // gzprintf(visualizer_file, "Ltworeadmiss: %d\n", L2_read_miss);
-   // gzprintf(visualizer_file, "Ltworeadhit: %d\n", L2_read_access-L2_read_miss);
+   gzprintf(visualizer_file, "Ltwowritemiss: %d\n", L2_write_miss);
+   gzprintf(visualizer_file, "Ltwowritehit: %d\n",  L2_write_access-L2_write_miss);
+   gzprintf(visualizer_file, "Ltworeadmiss: %d\n", L2_read_miss);
+   gzprintf(visualizer_file, "Ltworeadhit: %d\n", L2_read_access-L2_read_miss);
    if (num_mfs)
       gzprintf(visualizer_file, "averagemflatency: %lld\n", mf_total_lat/num_mfs);
 }
 
-void gpgpu_sim::print_dram_stats(FILE *fout) const
+void gpgpu_sim::print_dram_L2_stats(FILE *fout) const
 {
+
 	unsigned cmd=0;
 	unsigned activity=0;
 	unsigned nop=0;
@@ -500,6 +932,10 @@
 	unsigned rd=0;
 	unsigned wr=0;
 	unsigned req=0;
+	unsigned l2_read_access=0;
+	unsigned l2_read_miss=0;
+	unsigned l2_write_access=0;
+	unsigned l2_write_miss=0;
 	unsigned tot_cmd=0;
 	unsigned tot_nop=0;
 	unsigned tot_act=0;
@@ -507,9 +943,14 @@
 	unsigned tot_rd=0;
 	unsigned tot_wr=0;
 	unsigned tot_req=0;
+	unsigned tot_l2_read_access=0;
+	unsigned tot_l2_read_miss=0;
+	unsigned tot_l2_write_access=0;
+	unsigned tot_l2_write_miss=0;
 
 	for (unsigned i=0;i<m_memory_config->m_n_mem;i++){
 		m_memory_partition_unit[i]->set_dram_power_stats(cmd,activity,nop,act,pre,rd,wr,req);
+		m_memory_partition_unit[i]->set_L2cache_power_stats(l2_read_access,l2_read_miss,l2_write_access,l2_write_miss);
 		tot_cmd+=cmd;
 		tot_nop+=nop;
 		tot_act+=act;
@@ -517,48 +958,308 @@
 		tot_rd+=rd;
 		tot_wr+=wr;
 		tot_req+=req;
+		tot_l2_read_access+=l2_read_access;
+		tot_l2_read_miss+=l2_read_miss;
+		tot_l2_write_access+=l2_write_access;
+		tot_l2_write_miss+=l2_write_miss;
 	}
+	fprintf(fout,"gpgpu_n_l2_cache_read_access = %d\n",tot_l2_read_access );
+	fprintf(fout,"gpgpu_n_l2_cache_read_miss = %d\n",tot_l2_read_miss );
+	fprintf(fout,"gpgpu_n_l2_cache_write_access = %d\n",tot_l2_write_access );
+	fprintf(fout,"gpgpu_n_l2_cache_write_miss = %d\n",tot_l2_write_miss );
     fprintf(fout,"gpgpu_n_dram_reads = %d\n",tot_rd );
     fprintf(fout,"gpgpu_n_dram_writes = %d\n",tot_wr );
     fprintf(fout,"gpgpu_n_dram_activate = %d\n",tot_act );
-    fprintf(fout,"gpgpu_n_dram_commands = %d\n",tot_cmd);
-    fprintf(fout,"gpgpu_n_dram_noops = %d\n",tot_nop );
-    fprintf(fout,"gpgpu_n_dram_precharges = %d\n",tot_pre );
-    fprintf(fout,"gpgpu_n_dram_requests = %d\n",tot_req );
+
+}
+void gpgpu_sim::L2c_print_cache_stat() const
+{
+    unsigned i, j, k;
+    for (i=0,j=0,k=0;i<m_memory_config->m_n_mem;i++)
+        m_memory_partition_unit[i]->print_cache_stat(k,j);
+    printf("L2 Cache Total Miss Rate = %0.3f\n", (float)j/k);
 }
 
-unsigned memory_sub_partition::flushL2() 
+unsigned memory_partition_unit::flushL2() 
 { 
-    if (!m_config->m_L2_config.disabled()) {
-        m_L2cache->flush(); 
+    // Saugata: flush each bank
+    for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+        m_L2cache[i]->flush(); 
     }
     return 0; // L2 is read only in this version
 }
 
-bool memory_sub_partition::busy() const 
+bool memory_partition_unit::busy() const 
 {
     return !m_request_tracker.empty();
 }
 
-void memory_sub_partition::push( mem_fetch* req, unsigned long long cycle ) 
+// unsigned memory_partition_unit::get_rop(bool bypassed, unsigned long long cycle)
+// {
+//     if(m_config->bypass_rop)
+//         return 1;
+// // 48 threads, 8 cycles latency, 2 banks (12 banks total), 2 ports 
+//     unsigned mpkc_threads[14] = {50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700};
+// //    unsigned latency_48threads[14] = {53, 59, 64, 72, 82, 95, 110, 137, 181, 222, 300,300,300,300};
+//     unsigned latency_16threads[14] = {25, 29, 32, 37, 44, 52, 73, 93, 421, 500, 500, 500, 500, 500};
+//     unsigned latency_32threads[14] = {46, 49, 59, 67, 88, 102, 147, 170, 500, 500, 500, 500, 500, 500};
+// //done
+//     unsigned latency_48threads[14] = {68, 78, 89, 102, 126, 160, 222, 500, 500, 500, 500, 500, 500, 500};
+//     unsigned latency_64threads[14] = {90, 90, 105, 131, 170, 172, 228, 500, 500, 500, 500, 500, 500, 500};
+//     unsigned latency_96threads[14] = {136, 137, 174, 177, 254, 249, 500, 500, 500, 500, 500, 500, 500,500};
+//     unsigned latency_128threads[14] = {184, 184, 184, 263, 267, 270, 500, 500, 500, 500, 500, 500, 500,500};
+//     unsigned latency_144threads[14] = {205, 208, 205, 322, 317, 328, 500, 500, 500, 500, 500, 500, 500,500};
+//     unsigned latency_176threads[14] = {258, 260, 258, 255, 570, 571, 564, 579, 600, 600, 600, 600, 600,600};
+//     unsigned latency_192threads[14] = {287, 287, 286, 280, 600, 600, 600, 600, 600, 600, 600, 600, 600,600};
+//     unsigned latency_240threads[14] = {379, 378, 379, 377, 377, 600, 600, 600, 600, 600, 600, 600, 600,600};
+// //    unsigned latency_48threads[14] = {53, 59, 64, 72, 82, 95, 110, 137, 181, 222, 13458, 53015, 85086, 115338};
+// 
+// 
+//     // Bypassed + non bypasses request counter
+//     m_counter_at_queue3++;
+//     if(bypassed)
+//     {
+//         //Increment bypass
+//         m_stats->bypassed_rop_count++;
+//         m_stats->bypassed_ratio[10]++;
+//         //the request is ready (to get sent to the MC in the next cycle
+//         return 1;
+//     }
+//     else
+//     {
+//         m_counter_at_queue++;
+//         m_counter_at_queue2++;
+//         int mpkc = int((float(m_counter_at_queue)/(float)(1+cycle-m_counter_cleared_cycles))*1000);
+// //        float bypassed_ratio = float(m_counter_at_queue2)/float(m_counter_at_queue3);
+//         if(cycle - m_counter_cleared_cycles > 50000)
+//         {
+//             m_counter_at_queue = 0;
+//             m_counter_cleared_cycles = cycle;
+//         }
+//         if(cycle - m_counter_cleared_cycles2 > 100)
+//         {
+//             m_counter_at_queue2 = 0;
+//             m_counter_at_queue3 = 0;
+//             m_counter_cleared_cycles2 = cycle;
+//         }
+//         m_stats->non_bypassed_rop_count++;
+//         int i;
+//         for(i=0;i<13;i++)
+//             if(mpkc < mpkc_threads[i]) break;
+//         m_stats->rop_bin_count[i]++;
+//         if(m_counter_at_queue2 < 16)
+//         {
+//             m_stats->bypassed_ratio[0]++;
+//             return latency_16threads[i];
+//         }
+//         else if(m_counter_at_queue2 < 32)
+//         {
+//             m_stats->bypassed_ratio[1]++;
+//             return latency_32threads[i];
+//         }
+//         else if(m_counter_at_queue2 < 48)
+//         {
+//             m_stats->bypassed_ratio[2]++;
+//             return latency_48threads[i];
+//         }
+//         else if(m_counter_at_queue2 < 64)
+//         {
+//             m_stats->bypassed_ratio[3]++;
+//             return latency_64threads[i];
+//         }
+//         else if(m_counter_at_queue2 < 96)
+//         {
+//             m_stats->bypassed_ratio[4]++;
+//             return latency_96threads[i];
+//         }
+//         else if(m_counter_at_queue2 < 128)
+//         {
+//             m_stats->bypassed_ratio[5]++;
+//             return latency_128threads[i];
+//         }
+//         else if(m_counter_at_queue2 < 144)
+//         {
+//             m_stats->bypassed_ratio[6]++;
+//             return latency_144threads[i];
+//         }
+//         else if(m_counter_at_queue2 < 176)
+//         {
+//             m_stats->bypassed_ratio[7]++;
+//             return latency_176threads[i];
+//         }
+//         else if(m_counter_at_queue2 < 192)
+//         {
+//             m_stats->bypassed_ratio[8]++;
+//             return latency_192threads[i];
+//         }
+//         else
+//         {
+//             m_stats->bypassed_ratio[9]++;
+//             return latency_240threads[i];
+//         }
+// 
+// //        if(bypassed_ratio < 0.25)
+// //        {
+// //            m_stats->bypassed_ratio[0]++;
+// //            return latency_48threads[i];
+// //        }
+// //        else if(bypassed_ratio < 0.5)
+// //        {
+// //            m_stats->bypassed_ratio[1]++;
+// //            return latency_96threads[i];
+// //        }
+// //        else if(bypassed_ratio < 0.75)
+// //        {
+// //            m_stats->bypassed_ratio[2]++;
+// //            return latency_144threads[i];
+// //        }
+// //        else if(bypassed_ratio < 1)
+// //        {
+// //            m_stats->bypassed_ratio[3]++;
+// //            return latency_192threads[i];
+// //        }
+// //        else
+// //        {
+// //            m_stats->bypassed_ratio[4]++;
+// //            return latency_240threads[i];
+// //        }
+//     }
+// 
+// }
+
+// Saugata: push() now adds the requests to the input port
+void memory_partition_unit::push( mem_fetch* req, unsigned long long cycle ) 
 {
-    if (req) {
+    if(req) {
         m_request_tracker.insert(req);
         m_stats->memlatstat_icnt2mem_pop(req);
+
+        unsigned int free_port = 0;
+
+        while(free_port < m_config->m_n_ports_per_memory_channel && input_ports[free_port]) {
+            ++free_port;
+        }
+
+        assert(free_port < m_config->m_n_ports_per_memory_channel);
+
+        input_ports[free_port] = req;
+        input_port_enqueue_time[free_port] = cycle;
+        if(!m_config->l2_bypass_after_queue) {
+            bypass_cache(req);
+        }
+    }
+}
+
+// Saugata: every cycle, check to see if requests can go to their appropriate queues
+void memory_partition_unit::bank_arbitrate_cycle(unsigned long long cycle)
+{
+    bool destination_queue_available;
+
+    for(unsigned int port_id = 0; port_id < m_config->m_n_ports_per_memory_channel; ++port_id) {
+        if(input_ports[port_id]) {
+            new_addr_type current_L2_bank = input_ports[port_id]->get_L2_bank_id();
+
+            // Saugata: if a request is bypassed, check if the L2-to-DRAM queue is full instead of the interconnect-to-L2 queue
+            if(m_config->cache_aware && input_ports[port_id]->m_bypassed) {
+                destination_queue_available = !m_L2_dram_queue->full();
+            }
+            else {
+                destination_queue_available = !m_icnt_L2_queue[current_L2_bank]->full();
+            }
+
+            if(destination_queue_available) {
+                send_to_bank(input_ports[port_id], cycle);
+                
+                ++num_bank_arbitrations;
+                unsigned int input_stall_time = cycle - input_port_enqueue_time[port_id];
+                total_input_stall_cycles += input_stall_time;
+
+                if(input_stall_time > max_input_stall_time) {
+                    max_input_stall_time = input_stall_time;
+                }
+
+                input_ports[port_id] = NULL;
+            }
+        }
+    }
+
+    // Saugata: need to perform compaction to ensure proper ordering when queues are almost full - prevents starvation
+    unsigned int slot_to_write = 0;
+
+    for(unsigned int port_id = 0; port_id < m_config->m_n_ports_per_memory_channel; ++port_id) {
+        if(input_ports[port_id]) {
+            input_ports[slot_to_write] = input_ports[port_id];
+            input_port_enqueue_time[slot_to_write] = input_port_enqueue_time[port_id];
+            ++slot_to_write;
+        }
+    }
+
+    // Saugata: collect stats based on slot_to_write
+    total_pending_input_requests += slot_to_write;
+    ++num_bank_arbitration_cycles;
+    if(slot_to_write) {
+        ++num_incomplete_bank_arbitration_cycles;
+    }
+
+    if(slot_to_write > max_slot_to_write) {
+        max_slot_to_write = slot_to_write;
+    }
+
+    // Saugata: make sure remaining entries (which have been moved already) are cleared
+    while(slot_to_write < m_config->m_n_ports_per_memory_channel) {
+        input_ports[slot_to_write] = NULL;
+        ++slot_to_write;
+    }
+}
+
+// Saugata: old push() functionality now being done once a request travels through the port
+void memory_partition_unit::send_to_bank(mem_fetch * req, unsigned long long cycle)
+{
+    if (req) {
         if( req->istexture() ) {
-            m_icnt_L2_queue->push(req);
-            req->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+            // Saugata: now, if the request is tagged for bypass, we will bypass the queue directly and send to DRAM
+            if(m_config->cache_aware && req->m_bypassed) {
+                m_L2_dram_queue->push(req);
+                req->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+            }
+            else {
+                // Saugata: modified to push request into correct bank
+                m_icnt_L2_queue[req->get_L2_bank_id()]->push(req);
+                req->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+            }
         } else {
             rop_delay_t r;
             r.req = req;
-            r.ready_cycle = cycle + m_config->rop_latency;
+            // Saugata: instead of using get_rop function, bypass the ROP if it has a real latency
+            // req->m_rop_latency = get_rop(bypass_cache_rop(req), cycle);
+            // r.ready_cycle = cycle + req->m_rop_latency;
+            if((m_config->rop_latency > 1) && (m_config->bypass_rop || bypass_cache_rop(req))) {
+                r.ready_cycle = cycle + 1;
+                ++m_stats->bypassed_rop_count;
+            }
+            else {
+                r.ready_cycle = cycle + m_config->rop_latency;
+                ++m_stats->non_bypassed_rop_count;
+            }
+//            if(m_config->bypass_rop && bypass_cache_rop(req))
+//            {
+//                m_stats->bypassed_rop_count++;
+//                r.ready_cycle = cycle + 1;
+//            }
+//            else if(m_config->bypass_rop)
+//            {
+//                m_stats->non_bypassed_rop_count++;
+//                r.ready_cycle = cycle + (unsigned)((float)m_config->rop_latency/4);
+//            }
+//            else
+//                r.ready_cycle = cycle + m_config->rop_latency;
             m_rop.push(r);
             req->set_status(IN_PARTITION_ROP_DELAY,gpu_sim_cycle+gpu_tot_sim_cycle);
         }
     }
 }
 
-mem_fetch* memory_sub_partition::pop() 
+mem_fetch* memory_partition_unit::pop() 
 {
     mem_fetch* mf = m_L2_icnt_queue->pop();
     m_request_tracker.erase(mf);
@@ -571,7 +1272,7 @@
     return mf;
 }
 
-mem_fetch* memory_sub_partition::top() 
+mem_fetch* memory_partition_unit::top() 
 {
     mem_fetch *mf = m_L2_icnt_queue->top();
     if( mf && (mf->get_access_type() == L2_WRBK_ACC || mf->get_access_type() == L1_WRBK_ACC) ) {
@@ -583,25 +1284,79 @@
     return mf;
 }
 
-void memory_sub_partition::set_done( mem_fetch *mf )
+void memory_partition_unit::set_done( mem_fetch *mf )
 {
     m_request_tracker.erase(mf);
 }
 
-void memory_sub_partition::accumulate_L2cache_stats(class cache_stats &l2_stats) const {
-    if (!m_config->m_L2_config.disabled()) {
-        l2_stats += m_L2cache->get_stats();
+void memory_partition_unit::dram_cycle() 
+{ 
+    // pop completed memory request from dram and push it to dram-to-L2 queue 
+    // Saugata: modified to first check request at top of DRAM to find correct DRAM->L2 queue
+    if(m_dram->top()) {
+        if ( !m_dram_L2_queue[m_dram->top()->get_L2_bank_id()]->full() ) {
+            mem_fetch* mf = m_dram->pop();
+            if( mf->get_access_type() == L1_WRBK_ACC ) {
+                m_request_tracker.erase(mf);
+                delete mf;
+            } else {
+                // Saugata: modified to push to a per-bank queue
+                m_dram_L2_queue[mf->get_L2_bank_id()]->push(mf);
+                mf->set_status(IN_PARTITION_DRAM_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
+            }
+        }
+    }
+    m_dram->cycle(); 
+    m_dram->dram_log(SAMPLELOG);   
+
+    if( !m_dram->full() && !m_L2_dram_queue->empty() ) {
+        // L2->DRAM queue to DRAM latency queue
+        mem_fetch *mf = m_L2_dram_queue->pop();
+        dram_delay_t d;
+        d.req = mf;
+        // Saugata: added in L2 latency
+        // d.ready_cycle = gpu_sim_cycle+gpu_tot_sim_cycle + m_config->dram_latency;
+        d.ready_cycle = gpu_sim_cycle+gpu_tot_sim_cycle + m_config->L2_latency + m_config->dram_latency;
+        m_dram_latency_queue.push(d);
+        mf->set_status(IN_PARTITION_DRAM_LATENCY_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
     }
-}
 
-void memory_sub_partition::get_L2cache_sub_stats(struct cache_sub_stats &css) const{
-    if (!m_config->m_L2_config.disabled()) {
-        m_L2cache->get_sub_stats(css);
+    // DRAM latency queue
+    if( !m_dram_latency_queue.empty() && ( (gpu_sim_cycle+gpu_tot_sim_cycle) >= m_dram_latency_queue.front().ready_cycle ) && !m_dram->full() ) {
+        mem_fetch* mf = m_dram_latency_queue.front().req;
+        m_dram_latency_queue.pop();
+        m_dram->push(mf);
     }
 }
 
-void memory_sub_partition::visualizer_print( gzFile visualizer_file )
-{
-    // TODO: Add visualizer stats for L2 cache 
+void memory_partition_unit::set_dram_power_stats(unsigned &n_cmd,
+												unsigned &n_activity,
+												unsigned &n_nop,
+												unsigned &n_act,
+												unsigned &n_pre,
+												unsigned &n_rd,
+												unsigned &n_wr,
+												unsigned &n_req) const{
+	m_dram->set_dram_power_stats(n_cmd, n_activity, n_nop, n_act, n_pre, n_rd, n_wr, n_req);
+}
+
+void memory_partition_unit::set_L2cache_power_stats(unsigned &n_read_access,
+												unsigned &n_read_miss,
+												unsigned &n_write_access,
+												unsigned &n_write_miss) const{
+    // Saugata: currently unchanged for our banked cache, assuming that partitioning is by ways only
+    unsigned read_access, read_miss, write_access, write_miss;
+    n_read_access = n_read_miss = n_write_access = n_write_miss = 0;
+
+    for(unsigned int i = 0; i < m_L2_num_banks; ++i) {
+    	m_L2cache[i]->get_data_stats(read_access,read_miss,write_access,write_miss);
+        n_read_access += read_access;
+        n_read_miss += read_miss;
+        n_write_access += write_access;
+        n_write_miss += write_miss;
+    }
 }
 
+void memory_partition_unit::set_icnt_power_stats(unsigned &mem_to_simt) const{
+	mem_to_simt = n_mem_to_simt;
+}
diff -Naur gpgpu-sim-baseline/l2cache.h gpgpu-sim/l2cache.h
--- gpgpu-sim-baseline/l2cache.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/l2cache.h	2015-10-13 06:38:50.546481857 -0400
@@ -30,12 +30,98 @@
 
 #include "dram.h"
 #include "../abstract_hardware_model.h"
+#include "contention.h" // Saugata: contains contention-related structures
 
 #include <list>
 #include <queue>
 
 class mem_fetch;
 
+//Rachata -- Stats for each warp l2 access
+class l2_warp_info{
+public:
+    l2_warp_info(unsigned wID, const struct memory_config *config);
+    ~l2_warp_info();
+
+    // TODO: Rachata --> Helper functions
+
+    unsigned get_t0() { return t0_address; }
+    void clear();
+    void set_t0(unsigned t0) { t0_address = t0; }
+    unsigned get_wid() { return warpID; }
+    unsigned get_hit_ratio() { return t0_address; }
+    unsigned get_total_accessed() const { return total_accessed; }
+    unsigned get_total_hit() { return total_hit; }
+    unsigned get_last_accessed() { return last_accessed_time; }
+    float get_hit_rate() { 
+        if(total_accessed == 0 ) return 0; 
+        else return (float)total_hit/(float)total_accessed; 
+    }
+    void set_last_accessed(unsigned time) { last_accessed_time = time; }
+    void add_accessed(){ total_accessed++; }
+    void add_accessed(int i){ accessed[i]++; }
+    void add_hit(){ total_hit++; }
+    void add_hit(int i){ hit_count[i]++; }
+    void set_mask(){ total_hit++; }
+    void unrecognized_tid_add(){ unrecognized_tid++; }
+
+    
+private:
+    unsigned timeCleared;
+    unsigned warpID;
+    unsigned int total_accessed;
+    unsigned int total_hit;
+    unsigned int unrecognized_tid;
+    unsigned int hit_count[128];
+    unsigned int accessed[128];
+    float hit_ratio;
+    unsigned last_mask[128];
+    unsigned last_accessed[128];
+    unsigned last_hit[128];
+    unsigned t0_address;
+    unsigned last_accessed_time;
+
+
+};
+
+//Rachata -- Stats for each address --> Should be just number of hits/reuses
+class address_info{
+public:
+    address_info(unsigned wID, const struct memory_config *config);
+    ~address_info();
+
+    // TODO: Rachata --> Helper functions
+
+    void clear(mem_fetch * mf);
+    unsigned get_wid() { return warpID; }
+    float update_reused(mem_fetch* mf){
+        //TODO: Detect burstiness
+        if(mf->get_timestamp()==0)
+            return 100000;
+        else
+            burstiness = ((float)total_accessed)/((float)mf->get_timestamp());
+        return burstiness;
+    }
+    unsigned get_total_hit() { return total_hit; }
+    unsigned get_last_accessed() { return last_accessed_time; }
+    float get_hit_rate() const { // Saugata: modified to const function
+        if(total_accessed == 0 ) return 0; 
+        else return (float)total_hit/(float)total_accessed; 
+    }
+    void set_last_accessed(unsigned time) { last_accessed_time = time; }
+    void add_accessed(){ total_accessed++; }
+    void add_hit(){ total_hit++; }
+    const memory_config* m_config;
+    
+private:
+    unsigned timeCleared;
+    unsigned warpID;
+    unsigned int last_accessed_time;
+    unsigned int total_accessed;
+    unsigned int total_hit;
+    unsigned int burstiness;
+};
+
 class partition_mf_allocator : public mem_fetch_allocator {
 public:
     partition_mf_allocator( const memory_config *config )
@@ -52,136 +138,109 @@
     const memory_config *m_memory_config;
 };
 
-// Memory partition unit contains all the units assolcated with a single DRAM channel. 
-// - It arbitrates the DRAM channel among multiple sub partitions.  
-// - It does not connect directly with the interconnection network. 
-class memory_partition_unit
+class memory_partition_unit 
 {
-public: 
+public:
    memory_partition_unit( unsigned partition_id, const struct memory_config *config, class memory_stats_t *stats );
    ~memory_partition_unit(); 
 
+//   void set_new_info(l2_warp_info* new_info, mem_fetch* mf);
+   bool update_warp_info(l2_warp_info* new_info, mem_fetch* mf);
+   bool update_addr_info(address_info* new_info, mem_fetch* mf);
+
+   std::hash<unsigned> hashFunc1;
+   std::hash<unsigned> hashFunc2;
+   std::hash<unsigned> hashFunc3;
+ 
    bool busy() const;
 
    void cache_cycle( unsigned cycle );
    void dram_cycle();
 
-   void set_done( mem_fetch *mf );
-
-   void visualizer_print( gzFile visualizer_file ) const;
-   void print_stat( FILE *fp ) { m_dram->print_stat(fp); }
-   void visualize() const { m_dram->visualize(); }
-   void print( FILE *fp ) const;
-
-   class memory_sub_partition * get_sub_partition(int sub_partition_id) 
-   {
-      return m_sub_partition[sub_partition_id]; 
-   }
-
-   // Power model
-   void set_dram_power_stats(unsigned &n_cmd,
-                             unsigned &n_activity,
-                             unsigned &n_nop,
-                             unsigned &n_act,
-                             unsigned &n_pre,
-                             unsigned &n_rd,
-                             unsigned &n_wr,
-                             unsigned &n_req) const;
-
-   int global_sub_partition_id_to_local_id(int global_sub_partition_id) const; 
-
-   unsigned get_mpid() const { return m_id; }
-
-private: 
-
-   unsigned m_id;
-   const struct memory_config *m_config;
-   class memory_stats_t *m_stats;
-   class memory_sub_partition **m_sub_partition; 
-   class dram_t *m_dram;
-
-   class arbitration_metadata
-   {
-   public: 
-      arbitration_metadata(const struct memory_config *config); 
-
-      // check if a subpartition still has credit 
-      bool has_credits(int inner_sub_partition_id) const; 
-      // borrow a credit for a subpartition 
-      void borrow_credit(int inner_sub_partition_id); 
-      // return a credit from a subpartition 
-      void return_credit(int inner_sub_partition_id); 
-
-      // return the last subpartition that borrowed credit 
-      int last_borrower() const { return m_last_borrower; } 
-
-      void print( FILE *fp ) const; 
-   private: 
-      // id of the last subpartition that borrowed credit 
-      int m_last_borrower; 
-
-      int m_shared_credit_limit; 
-      int m_private_credit_limit; 
-
-      // credits borrowed by the subpartitions
-      std::vector<int> m_private_credit; 
-      int m_shared_credit; 
-   }; 
-   arbitration_metadata m_arbitration_metadata; 
-
-   // determine wheither a given subpartition can issue to DRAM 
-   bool can_issue_to_dram(int inner_sub_partition_id); 
-
-   // model DRAM access scheduler latency (fixed latency between L2 and DRAM)
-   struct dram_delay_t
-   {
-      unsigned long long ready_cycle;
-      class mem_fetch* req;
-   };
-   std::list<dram_delay_t> m_dram_latency_queue;
-};
-
-class memory_sub_partition
-{
-public:
-   memory_sub_partition( unsigned sub_partition_id, const struct memory_config *config, class memory_stats_t *stats );
-   ~memory_sub_partition(); 
-
-   unsigned get_id() const { return m_id; } 
-
-   bool busy() const;
-
-   void cache_cycle( unsigned cycle );
-
    bool full() const;
    void push( class mem_fetch* mf, unsigned long long clock_cycle );
+   // Saugata: added functions to handle input port arbitration
+   void bank_arbitrate_cycle(unsigned long long cycle);
+   void send_to_bank(mem_fetch * req, unsigned long long cycle);
    class mem_fetch* pop(); 
    class mem_fetch* top();
    void set_done( mem_fetch *mf );
 
    unsigned flushL2();
 
-   // interface to L2_dram_queue
-   bool L2_dram_queue_empty() const; 
-   class mem_fetch* L2_dram_queue_top() const; 
-   void L2_dram_queue_pop(); 
-
-   // interface to dram_L2_queue
-   bool dram_L2_queue_full() const; 
-   void dram_L2_queue_push( class mem_fetch* mf ); 
+
+   void assign_PCAL_tokens();
+   void update_latest_used_warps(unsigned wid);
+   bool bypass_PCAL(mem_fetch *mf);
 
    void visualizer_print( gzFile visualizer_file );
    void print_cache_stat(unsigned &accesses, unsigned &misses) const;
+   void print_stat( FILE *fp ) { m_dram->print_stat(fp); }
+   void visualize() const { m_dram->visualize(); }
    void print( FILE *fp ) const;
 
-   void accumulate_L2cache_stats(class cache_stats &l2_stats) const;
-   void get_L2cache_sub_stats(struct cache_sub_stats &css) const;
+   // Rachata --> check if we should bypass the cache or not
+   bool bypass_cache(mem_fetch *mf);
+   bool bypass_cache_rop(mem_fetch *mf);
+   bool bypass_addr(mem_fetch *mf, bool warp_level_decision);
+   unsigned get_hash_index(new_addr_type addr);
+   unsigned get_bloom_filter_index(mem_fetch* mf);
+   unsigned get_rop(bool bypassed, unsigned long long cycle);
+   // Saugata: added function to print warp hit rates
+   void print_warp_hit_rates(FILE *fp) const;
+
+   unsigned m_counter_at_queue;
+   unsigned m_counter_at_queue2;
+   unsigned m_counter_at_queue3;
+   unsigned m_counter_cleared_cycles;
+   unsigned m_counter_cleared_cycles2;
+
+
+   // Power model
+   void set_dram_power_stats(unsigned &n_cmd,
+			unsigned &n_activity,
+			unsigned &n_nop,
+			unsigned &n_act,
+			unsigned &n_pre,
+			unsigned &n_rd,
+			unsigned &n_wr,
+			unsigned &n_req) const;
+
+   void set_L2cache_power_stats(unsigned &n_read_access,
+			unsigned &n_read_miss,
+			unsigned &n_write_access,
+			unsigned &n_write_miss) const;
+
+   void set_icnt_power_stats(unsigned &n_mem_to_simt) const;
+
+   void set_bloom_filter_loc(unsigned loc){bloom_filter[loc]=true; bloom_filter_size++;}
+   void set_bloom_filter(mem_fetch* mf);
+   bool get_bloom_filter_loc(unsigned loc){return bloom_filter[loc];}
+   bool bloom_filter_contain(new_addr_type addr);
+   void bloom_filter_clear();
 
 private:
+   //Rachata: Collecting warp info
+   std::map<unsigned, l2_warp_info*> warp_info;
+   std::map<unsigned, address_info*> addr_info;
+   //Rachata: for PCAL bypassing policy (HPCA'15)
+   std::set<unsigned> token_list_PCAL;
+   std::list<unsigned> warp_queue_PCAL;
+
+   unsigned leftover_tokens;
+
+   bool bloom_filter[1000000];
+   unsigned bloom_filter_size;
+
 // data
-   unsigned m_id;  //< the global sub partition ID
+   unsigned m_id;
    const struct memory_config *m_config;
-   class l2_cache *m_L2cache;
+   class dram_t *m_dram;
+   // Saugata: support for multiple cache banks
+   unsigned int m_L2_num_banks;
+   unsigned int m_L2_current_bank; // fakes randomness by rotating through the starting bank
+   // class l2_cache *m_L2cache;
+   class l2_cache **m_L2cache;
    class L2interface *m_L2interface;
    partition_mf_allocator *m_mf_allocator;
 
@@ -193,25 +252,59 @@
    };
    std::queue<rop_delay_t> m_rop;
 
+   // model DRAM access scheduler latency (fixed latency between L2 and DRAM)
+   struct dram_delay_t
+   {
+      unsigned long long ready_cycle;
+      class mem_fetch* req;
+   };
+   std::queue<dram_delay_t> m_dram_latency_queue;
+
    // these are various FIFOs between units within a memory partition
-   fifo_pipeline<mem_fetch> *m_icnt_L2_queue;
+   // Saugata: support for multiple cache banks
+   // fifo_pipeline<mem_fetch> *m_icnt_L2_queue;
+   fifo_pipeline<mem_fetch> **m_icnt_L2_queue;
    fifo_pipeline<mem_fetch> *m_L2_dram_queue;
-   fifo_pipeline<mem_fetch> *m_dram_L2_queue;
+   // fifo_pipeline<mem_fetch> *m_dram_L2_queue;
+   fifo_pipeline<mem_fetch> **m_dram_L2_queue;
    fifo_pipeline<mem_fetch> *m_L2_icnt_queue; // L2 cache hit response queue
 
    class mem_fetch *L2dramout; 
    unsigned long long int wb_addr;
 
+   // Saugata: added structures to manage request input ports
+   class mem_fetch ** input_ports;
+   unsigned long long * input_port_enqueue_time;
+
+   // Saugata: added per-bank contention model to simulate L2 access times
+   queue_contention_model ** L2cache_contention;
+
    class memory_stats_t *m_stats;
 
    std::set<mem_fetch*> m_request_tracker;
 
    friend class L2interface;
+
+   // interconnect power stats
+   unsigned n_mem_to_simt;
+
+   // Saugata: per-bank histograms of L2 cache queuing latency
+   contention_histogram<unsigned int> ** icnt_L2_latency_hist;
+   contention_histogram<unsigned int> * mpu_icnt_L2_latency_hist;
+
+   // Saugata: stats for input ports
+   unsigned long long num_bank_arbitrations;
+   unsigned long long total_input_stall_cycles;
+   unsigned int max_input_stall_time;
+   unsigned long long total_pending_input_requests;
+   unsigned long long num_bank_arbitration_cycles;
+   unsigned long long num_incomplete_bank_arbitration_cycles;
+   unsigned int max_slot_to_write;
 };
 
 class L2interface : public mem_fetch_interface {
 public:
-    L2interface( memory_sub_partition *unit ) { m_unit=unit; }
+    L2interface( memory_partition_unit *unit ) { m_unit=unit; }
     virtual ~L2interface() {}
     virtual bool full( unsigned size, bool write) const 
     {
@@ -224,7 +317,7 @@
         m_unit->m_L2_dram_queue->push(mf);
     }
 private:
-    memory_sub_partition *m_unit;
+    memory_partition_unit *m_unit;
 };
 
 #endif
diff -Naur gpgpu-sim-baseline/l2cache_trace.h gpgpu-sim/l2cache_trace.h
--- gpgpu-sim-baseline/l2cache_trace.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/l2cache_trace.h	1969-12-31 19:00:00.000000000 -0500
@@ -1,55 +0,0 @@
-// Copyright (c) 2009-2011, Tor M. Aamodt, Tim Rogers, Wilson W. L. Fung 
-// The University of British Columbia
-// All rights reserved.
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-// Redistributions of source code must retain the above copyright notice, this
-// list of conditions and the following disclaimer.
-// Redistributions in binary form must reproduce the above copyright notice, this
-// list of conditions and the following disclaimer in the documentation and/or
-// other materials provided with the distribution.
-// Neither the name of The University of British Columbia nor the names of its
-// contributors may be used to endorse or promote products derived from this
-// software without specific prior written permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-#pragma once 
-
-#include "../trace.h"
-
-#if TRACING_ON
-
-#define MEMPART_PRINT_STR SIM_PRINT_STR " %d - "
-#define MEMPART_DTRACE(x)  ( DTRACE(x) && (Trace::sampling_memory_partition == -1 || Trace::sampling_memory_partition == (int)get_mpid()) )
-
-// Intended to be called from inside components of a memory partition
-// Depends on a get_mpid() function
-#define MEMPART_DPRINTF(...) do {\
-    if (MEMPART_DTRACE(MEMORY_PARTITION_UNIT)) {\
-        printf( MEMPART_PRINT_STR,\
-                gpu_sim_cycle + gpu_tot_sim_cycle,\
-                Trace::trace_streams_str[Trace::MEMORY_PARTITION_UNIT],\
-                get_mpid() );\
-        printf(__VA_ARGS__);\
-    }\
-} while (0)
-
-#else
-
-#define MEMPART_DTRACE(x)  (false)
-#define MEMPART_DPRINTF(x, ...) do {} while (0)
-
-#endif
-
diff -Naur gpgpu-sim-baseline/Makefile gpgpu-sim/Makefile
--- gpgpu-sim-baseline/Makefile	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/Makefile	2015-10-13 06:38:50.502481856 -0400
@@ -33,8 +33,10 @@
 
 ifeq ($(DEBUG),1)
 	CXXFLAGS = -Wall -DDEBUG
+	CXXFLAGS_L2CACHE = -Wall -DDEBUG
 else
 	CXXFLAGS = -Wall
+	CXXFLAGS_L2CACHE = -Wall
 endif
 
 ifeq ($(TRACE),1)
@@ -45,11 +47,13 @@
 
 ifeq ($(GNUC_CPP0X), 1)
     CXXFLAGS += -std=c++0x
+    CXXFLAGS_L2CACHE += -std=c++0x
 endif
 
 ifneq ($(DEBUG),1)
 	OPTFLAGS += -O3
 else
+	CXXFLAGS_L2CACHE += 
 	CXXFLAGS += 
 endif
 
@@ -63,7 +67,7 @@
 CPP = g++ $(SNOW)
 OEXT = o
 
-OUTPUT_DIR=$(SIM_OBJ_FILES_DIR)/gpgpu-sim
+OUTPUT_DIR=../../$(SIM_OBJ_FILES_DIR)/gpgpu-sim
 
 SRCS = $(shell ls *.cc)
 
@@ -80,11 +84,14 @@
 libgpu_uarch_sim.a:$(OBJS)
 	ar rcs  $(OUTPUT_DIR)/libgpu_uarch_sim.a $(OBJS)
 
-$(OUTPUT_DIR)/Makefile.makedepend: depend
+Makefile.makedepend: depend
 
 depend:
-	touch $(OUTPUT_DIR)/Makefile.makedepend
-	makedepend -f$(OUTPUT_DIR)/Makefile.makedepend -p$(OUTPUT_DIR)/ $(CSRCS) 2> /dev/null
+	touch Makefile.makedepend
+	makedepend -fMakefile.makedepend -p$(OUTPUT_DIR)/ $(CSRCS) 2> /dev/null
+
+$(OUTPUT_DIR)/l2cache.$(OEXT): l2cache.cc
+	$(CPP) $(OPTFLAGS) $(CXXFLAGS_L2CACHE) -o $*.$(OEXT) -c l2cache.cc
 
 $(OUTPUT_DIR)/%.$(OEXT): %.cc
 	$(CPP) $(OPTFLAGS) $(CXXFLAGS) $(POWER_FLAGS) -o $(OUTPUT_DIR)/$*.$(OEXT) -c $*.cc
@@ -95,10 +102,10 @@
 
 $(OUTPUT_DIR)/option_parser.$(OEXT): option_parser.h
 
-$(OUTPUT_DIR)/dram_sched.$(OEXT): $(OUTPUT_DIR)/../cuda-sim/ptx.tab.h
+$(OUTPUT_DIR)/dram_sched.$(OEXT): ../cuda-sim/ptx.tab.h
 
-$(OUTPUT_DIR)/../cuda-sim/ptx.tab.h:
-	make -C ../cuda-sim/ $(OUTPUT_DIR)/../cuda-sim/ptx.tab.c
+../cuda-sim/ptx.tab.h:
+	make -C ../cuda-sim/ ptx.tab.c
 
-include $(OUTPUT_DIR)/Makefile.makedepend
+include Makefile.makedepend
 
diff -Naur gpgpu-sim-baseline/Makefile.makedepend gpgpu-sim/Makefile.makedepend
--- gpgpu-sim-baseline/Makefile.makedepend	1969-12-31 19:00:00.000000000 -0500
+++ gpgpu-sim/Makefile.makedepend	2015-10-13 06:38:50.594481858 -0400
@@ -0,0 +1,486 @@
+# DO NOT DELETE
+
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/string.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/xlocale.h
+../../build/4000/release/gpgpu-sim/addrdec.o: addrdec.h /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/addrdec.o: ../option_parser.h
+../../build/4000/release/gpgpu-sim/addrdec.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/addrdec.o: gpu-sim.h ../trace.h
+../../build/4000/release/gpgpu-sim/addrdec.o: ../trace_streams.tup shader.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/math.h
+../../build/4000/release/gpgpu-sim/addrdec.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/addrdec.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/addrdec.o: gpu-misc.h stack.h dram.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/addrdec.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/addrdec.o: scoreboard.h mem_fetch.h
+../../build/4000/release/gpgpu-sim/addrdec.o: mem_fetch_status.tup stats.h
+../../build/4000/release/gpgpu-sim/addrdec.o: gpu-cache.h ../tr1_hash_map.h
+../../build/4000/release/gpgpu-sim/dram.o: gpu-sim.h ../option_parser.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/dram.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/dram.o: ../trace.h ../trace_streams.tup
+../../build/4000/release/gpgpu-sim/dram.o: addrdec.h /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/dram.o: shader.h /usr/include/math.h
+../../build/4000/release/gpgpu-sim/dram.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/dram.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/dram.o: gpu-misc.h stack.h dram.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/dram.o: /usr/include/getopt.h scoreboard.h
+../../build/4000/release/gpgpu-sim/dram.o: mem_fetch.h mem_fetch_status.tup
+../../build/4000/release/gpgpu-sim/dram.o: stats.h gpu-cache.h
+../../build/4000/release/gpgpu-sim/dram.o: ../tr1_hash_map.h
+../../build/4000/release/gpgpu-sim/dram.o: mem_latency_stat.h dram_sched.h
+../../build/4000/release/gpgpu-sim/dram.o: l2cache.h contention.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: dram_sched.h dram.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: gpu-misc.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: shader.h /usr/include/math.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: stack.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: scoreboard.h mem_fetch.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: addrdec.h ../option_parser.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: mem_fetch_status.tup stats.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: gpu-cache.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: ../tr1_hash_map.h gpu-sim.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: ../trace.h
+../../build/4000/release/gpgpu-sim/dram_sched.o: ../trace_streams.tup
+../../build/4000/release/gpgpu-sim/dram_sched.o: mem_latency_stat.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: gpu-cache.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: gpu-misc.h mem_fetch.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: addrdec.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: ../option_parser.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: mem_fetch_status.tup
+../../build/4000/release/gpgpu-sim/gpu-cache.o: ../tr1_hash_map.h stat-tool.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: histogram.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/gpu-cache.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/gpu-misc.o: gpu-misc.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: gpu-sim.h ../option_parser.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../trace.h ../trace_streams.tup
+../../build/4000/release/gpgpu-sim/gpu-sim.o: addrdec.h /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: shader.h /usr/include/math.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: gpu-misc.h stack.h dram.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: scoreboard.h mem_fetch.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: mem_fetch_status.tup stats.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: gpu-cache.h ../tr1_hash_map.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/time.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/xlocale.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: icnt_wrapper.h stat-tool.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: histogram.h l2cache.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: contention.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../cuda-sim/ptx-stats.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/interconnect_interface.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/flit.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/booksim.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/trafficmanager.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/module.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/config_utils.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/network.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/credit.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/router.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/buffer_state.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/stats.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/traffic.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/routefunc.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/outputset.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../intersim/injection.hpp
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../debug.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../gpgpusim_entrypoint.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../cuda-sim/cuda-sim.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../gpgpu-sim/shader.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../cuda-sim/ptx_sim.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../cuda-sim/opcodes.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../cuda-sim/opcodes.def
+../../build/4000/release/gpgpu-sim/gpu-sim.o: ../cuda-sim/memory.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: /usr/include/string.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: mem_latency_stat.h power_stat.h
+../../build/4000/release/gpgpu-sim/gpu-sim.o: visualizer.h
+../../build/4000/release/gpgpu-sim/histogram.o: histogram.h
+../../build/4000/release/gpgpu-sim/histogram.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/histogram.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: icnt_wrapper.h
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/interconnect_interface.h
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/flit.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/booksim.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/trafficmanager.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/module.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/config_utils.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/network.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/credit.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/router.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/buffer_state.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/stats.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/traffic.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/routefunc.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/outputset.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../intersim/injection.hpp
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: ../option_parser.h
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/icnt_wrapper.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/string.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/xlocale.h
+../../build/4000/release/gpgpu-sim/l2cache.o: ../option_parser.h mem_fetch.h
+../../build/4000/release/gpgpu-sim/l2cache.o: addrdec.h /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/l2cache.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/l2cache.o: mem_fetch_status.tup dram.h
+../../build/4000/release/gpgpu-sim/l2cache.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/l2cache.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/l2cache.o: gpu-misc.h /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/l2cache.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/l2cache.o: gpu-cache.h ../tr1_hash_map.h
+../../build/4000/release/gpgpu-sim/l2cache.o: histogram.h l2cache.h
+../../build/4000/release/gpgpu-sim/l2cache.o: contention.h gpu-sim.h
+../../build/4000/release/gpgpu-sim/l2cache.o: ../trace.h ../trace_streams.tup
+../../build/4000/release/gpgpu-sim/l2cache.o: shader.h /usr/include/math.h
+../../build/4000/release/gpgpu-sim/l2cache.o: stack.h scoreboard.h stats.h
+../../build/4000/release/gpgpu-sim/l2cache.o: mem_latency_stat.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: mem_fetch.h addrdec.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: ../option_parser.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: mem_fetch_status.tup
+../../build/4000/release/gpgpu-sim/mem_fetch.o: mem_latency_stat.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: shader.h /usr/include/math.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: gpu-misc.h stack.h dram.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: scoreboard.h stats.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: gpu-cache.h ../tr1_hash_map.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: visualizer.h gpu-sim.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: ../trace.h
+../../build/4000/release/gpgpu-sim/mem_fetch.o: ../trace_streams.tup
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: mem_latency_stat.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: gpu-sim.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: ../option_parser.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: ../trace.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: ../trace_streams.tup
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: addrdec.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: shader.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/math.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: gpu-misc.h stack.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: dram.h scoreboard.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: mem_fetch.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: mem_fetch_status.tup
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: stats.h gpu-cache.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: ../tr1_hash_map.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: stat-tool.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: histogram.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: ../cuda-sim/ptx-stats.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: visualizer.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/string.h
+../../build/4000/release/gpgpu-sim/mem_latency_stat.o: /usr/include/xlocale.h
+../../build/4000/release/gpgpu-sim/power_interface.o: power_interface.h
+../../build/4000/release/gpgpu-sim/power_interface.o: gpu-sim.h
+../../build/4000/release/gpgpu-sim/power_interface.o: ../option_parser.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/power_interface.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/power_interface.o: ../trace.h
+../../build/4000/release/gpgpu-sim/power_interface.o: ../trace_streams.tup
+../../build/4000/release/gpgpu-sim/power_interface.o: addrdec.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/power_interface.o: shader.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/math.h
+../../build/4000/release/gpgpu-sim/power_interface.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/power_interface.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/power_interface.o: gpu-misc.h stack.h
+../../build/4000/release/gpgpu-sim/power_interface.o: dram.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/power_interface.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/power_interface.o: scoreboard.h
+../../build/4000/release/gpgpu-sim/power_interface.o: mem_fetch.h
+../../build/4000/release/gpgpu-sim/power_interface.o: mem_fetch_status.tup
+../../build/4000/release/gpgpu-sim/power_interface.o: stats.h gpu-cache.h
+../../build/4000/release/gpgpu-sim/power_interface.o: ../tr1_hash_map.h
+../../build/4000/release/gpgpu-sim/power_interface.o: power_stat.h
+../../build/4000/release/gpgpu-sim/power_interface.o: mem_latency_stat.h
+../../build/4000/release/gpgpu-sim/power_stat.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/power_stat.o: power_stat.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/power_stat.o: mem_latency_stat.h shader.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/math.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/power_stat.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/power_stat.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/power_stat.o: gpu-misc.h stack.h dram.h
+../../build/4000/release/gpgpu-sim/power_stat.o: scoreboard.h mem_fetch.h
+../../build/4000/release/gpgpu-sim/power_stat.o: addrdec.h ../option_parser.h
+../../build/4000/release/gpgpu-sim/power_stat.o: mem_fetch_status.tup stats.h
+../../build/4000/release/gpgpu-sim/power_stat.o: gpu-cache.h
+../../build/4000/release/gpgpu-sim/power_stat.o: ../tr1_hash_map.h gpu-sim.h
+../../build/4000/release/gpgpu-sim/power_stat.o: ../trace.h
+../../build/4000/release/gpgpu-sim/power_stat.o: ../trace_streams.tup
+../../build/4000/release/gpgpu-sim/power_stat.o: stat-tool.h histogram.h
+../../build/4000/release/gpgpu-sim/power_stat.o: ../cuda-sim/ptx-stats.h
+../../build/4000/release/gpgpu-sim/power_stat.o: visualizer.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/string.h
+../../build/4000/release/gpgpu-sim/power_stat.o: /usr/include/xlocale.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: scoreboard.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: shader.h /usr/include/math.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: gpu-misc.h stack.h dram.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: mem_fetch.h addrdec.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: ../option_parser.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: mem_fetch_status.tup stats.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: gpu-cache.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: ../tr1_hash_map.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: ../cuda-sim/ptx_sim.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: ../cuda-sim/opcodes.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: ../cuda-sim/opcodes.def
+../../build/4000/release/gpgpu-sim/scoreboard.o: ../cuda-sim/memory.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/string.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: /usr/include/xlocale.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: shader_trace.h ../trace.h
+../../build/4000/release/gpgpu-sim/scoreboard.o: ../trace_streams.tup
+../../build/4000/release/gpgpu-sim/shader.o: shader.h /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/math.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/shader.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/shader.o: gpu-misc.h stack.h
+../../build/4000/release/gpgpu-sim/shader.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/shader.o: dram.h /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/shader.o: scoreboard.h mem_fetch.h
+../../build/4000/release/gpgpu-sim/shader.o: addrdec.h ../option_parser.h
+../../build/4000/release/gpgpu-sim/shader.o: mem_fetch_status.tup stats.h
+../../build/4000/release/gpgpu-sim/shader.o: gpu-cache.h ../tr1_hash_map.h
+../../build/4000/release/gpgpu-sim/shader.o: gpu-sim.h ../trace.h
+../../build/4000/release/gpgpu-sim/shader.o: ../trace_streams.tup stat-tool.h
+../../build/4000/release/gpgpu-sim/shader.o: histogram.h
+../../build/4000/release/gpgpu-sim/shader.o: ../cuda-sim/ptx_sim.h
+../../build/4000/release/gpgpu-sim/shader.o: ../cuda-sim/opcodes.h
+../../build/4000/release/gpgpu-sim/shader.o: ../cuda-sim/opcodes.def
+../../build/4000/release/gpgpu-sim/shader.o: ../cuda-sim/memory.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/string.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/xlocale.h
+../../build/4000/release/gpgpu-sim/shader.o: ../cuda-sim/ptx-stats.h
+../../build/4000/release/gpgpu-sim/shader.o: ../cuda-sim/cuda-sim.h
+../../build/4000/release/gpgpu-sim/shader.o: ../gpgpu-sim/shader.h
+../../build/4000/release/gpgpu-sim/shader.o: ../cuda-sim/ptx_sim.h
+../../build/4000/release/gpgpu-sim/shader.o: mem_latency_stat.h visualizer.h
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/interconnect_interface.h
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/flit.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/booksim.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/trafficmanager.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/module.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/config_utils.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/network.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/credit.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/router.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/buffer_state.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/stats.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/traffic.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/routefunc.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/outputset.hpp
+../../build/4000/release/gpgpu-sim/shader.o: ../intersim/injection.hpp
+../../build/4000/release/gpgpu-sim/shader.o: icnt_wrapper.h
+../../build/4000/release/gpgpu-sim/shader.o: /usr/include/limits.h
+../../build/4000/release/gpgpu-sim/shader.o: shader_trace.h
+../../build/4000/release/gpgpu-sim/stack.o: stack.h
+../../build/4000/release/gpgpu-sim/stack.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/stack.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/stack.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/stack.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/stack.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: stat-tool.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: histogram.h ../tr1_hash_map.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/stat-tool.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/visualizer.o: visualizer.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/stdio.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/features.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/libio.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/_G_config.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/wchar.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/zlib.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/zconf.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/zlibdefs.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/unistd.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/getopt.h
+../../build/4000/release/gpgpu-sim/visualizer.o: gpu-sim.h ../option_parser.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/stdlib.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/alloca.h
+../../build/4000/release/gpgpu-sim/visualizer.o: ../abstract_hardware_model.h
+../../build/4000/release/gpgpu-sim/visualizer.o: ../trace.h
+../../build/4000/release/gpgpu-sim/visualizer.o: ../trace_streams.tup
+../../build/4000/release/gpgpu-sim/visualizer.o: addrdec.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/assert.h
+../../build/4000/release/gpgpu-sim/visualizer.o: shader.h /usr/include/math.h
+../../build/4000/release/gpgpu-sim/visualizer.o: delayqueue.h
+../../build/4000/release/gpgpu-sim/visualizer.o: ../intersim/statwraper.h
+../../build/4000/release/gpgpu-sim/visualizer.o: gpu-misc.h stack.h dram.h
+../../build/4000/release/gpgpu-sim/visualizer.o: scoreboard.h mem_fetch.h
+../../build/4000/release/gpgpu-sim/visualizer.o: mem_fetch_status.tup stats.h
+../../build/4000/release/gpgpu-sim/visualizer.o: gpu-cache.h
+../../build/4000/release/gpgpu-sim/visualizer.o: ../tr1_hash_map.h l2cache.h
+../../build/4000/release/gpgpu-sim/visualizer.o: contention.h
+../../build/4000/release/gpgpu-sim/visualizer.o: mem_latency_stat.h
+../../build/4000/release/gpgpu-sim/visualizer.o: power_stat.h stat-tool.h
+../../build/4000/release/gpgpu-sim/visualizer.o: histogram.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/time.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/xlocale.h
+../../build/4000/release/gpgpu-sim/visualizer.o: /usr/include/string.h
+../../build/4000/release/gpgpu-sim/visualizer.o: ../gpgpu-sim/shader.h
+../../build/4000/release/gpgpu-sim/visualizer.o: ../gpgpu-sim/mem_fetch.h
diff -Naur gpgpu-sim-baseline/mem_fetch.cc gpgpu-sim/mem_fetch.cc
--- gpgpu-sim-baseline/mem_fetch.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/mem_fetch.cc	2015-10-13 06:38:50.602481858 -0400
@@ -39,6 +39,7 @@
                       unsigned wid,
                       unsigned sid, 
                       unsigned tpc, 
+                      unsigned tid, // Rachata --> Added tid
                       const class memory_config *config )
 {
    m_request_uid = sm_next_mf_request_uid++;
@@ -61,6 +62,17 @@
    m_status_change = gpu_sim_cycle + gpu_tot_sim_cycle;
    m_mem_config = config;
    icnt_flit_size = config->icnt_flit_size;
+   // Rachata: set the default priority to 0
+   m_priority = 0;
+   m_bypassed = false;
+   // Saugata: ROP latency no longer used
+   // m_rop_latency = 0;
+   m_tid = tid;
+   m_l2_util = 1;
+
+   // Saugata: added information to track where/when request was serviced
+   m_service_level = 1; // start at L1 cache
+   m_service_time = 0;
 }
 
 mem_fetch::~mem_fetch()
@@ -96,6 +108,22 @@
 {
     m_status = status;
     m_status_change = cycle;
+
+    // Saugata: for requests entering the L2 and DRAM, mark that tehy had to go down to a lower level to be serviced
+    if(status == IN_PARTITION_ICNT_TO_L2_QUEUE || status == IN_PARTITION_ROP_DELAY) {
+        m_service_level = 2;  // going into L2
+    }
+    else if(status == IN_PARTITION_L2_TO_DRAM_QUEUE) {
+        m_service_level = 3;  // going into DRAM
+    }
+
+    // Saugata: for requests heading back to the shader, mark the time at which their requests complete within the memory system (do NOT count interconnection delays)
+    if(status == IN_PARTITION_DRAM_TO_L2_QUEUE) {
+        m_service_time = cycle;
+    }
+    else if(m_service_level == 2 && status == IN_PARTITION_L2_TO_ICNT_QUEUE) {
+        m_service_time = cycle;
+    }
 }
 
 bool mem_fetch::isatomic() const
@@ -124,8 +152,7 @@
 /// Returns number of flits traversing interconnect. simt_to_mem specifies the direction
 unsigned mem_fetch::get_num_flits(bool simt_to_mem){
 	unsigned sz=0;
-	// If atomic, write going to memory, or read coming back from memory, size = ctrl + data. Else, only ctrl
-	if( isatomic() || (simt_to_mem && get_is_write()) || !(simt_to_mem || get_is_write()) )
+	if( (simt_to_mem && get_is_write()) || !(simt_to_mem || get_is_write()) )
 		sz = size();
 	else
 		sz = get_ctrl_size();
diff -Naur gpgpu-sim-baseline/mem_fetch.h gpgpu-sim/mem_fetch.h
--- gpgpu-sim-baseline/mem_fetch.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/mem_fetch.h	2015-10-13 06:38:50.546481857 -0400
@@ -55,9 +55,16 @@
                unsigned wid,
                unsigned sid, 
                unsigned tpc, 
+               unsigned tid, 
                const class memory_config *config );
    ~mem_fetch();
 
+   // Rachata --> set the priority based on cache hit ratio, the higher the more priority
+   unsigned m_priority;
+   bool m_bypassed;
+   // Saugata: ROP latency no longer used
+   // unsigned m_rop_latency;
+
    void set_status( enum mem_fetch_status status, unsigned long long cycle );
    void set_reply() 
    { 
@@ -83,12 +90,17 @@
    void set_addr(new_addr_type addr) { m_access.set_addr(addr); }
    new_addr_type get_addr() const { return m_access.get_addr(); }
    new_addr_type get_partition_addr() const { return m_partition_addr; }
-   unsigned get_sub_partition_id() const { return m_raw_addr.sub_partition; }
+   // Saugata: added method to retrieve L2 bank ID
+   new_addr_type get_L2_bank_id() const { return m_raw_addr.L2_bank_id; }
    bool     get_is_write() const { return m_access.is_write(); }
    unsigned get_request_uid() const { return m_request_uid; }
    unsigned get_sid() const { return m_sid; }
    unsigned get_tpc() const { return m_tpc; }
    unsigned get_wid() const { return m_wid; }
+   float get_l2util() const { return m_l2_util; }
+   void set_l2util(float new_util) { m_l2_util = new_util; }
+   unsigned get_tid() { return m_tid; }
+   void set_tid( unsigned tid) { m_tid = tid; }
    bool istexture() const;
    bool isconst() const;
    enum mf_type get_type() const { return m_type; }
@@ -108,20 +120,34 @@
    const warp_inst_t &get_inst() { return m_inst; }
    enum mem_fetch_status get_status() const { return m_status; }
 
+   // Saugata: added in method to get last status change time
+   unsigned long long at_status_since() const { return m_status_change; }
+
    const memory_config *get_mem_config(){return m_mem_config;}
 
    unsigned get_num_flits(bool simt_to_mem);
+
+   // Saugata: added in method to retrieve information about where/when the request was serviced
+   unsigned get_service_level() { return m_service_level; }
+   unsigned get_service_time() { return m_service_time; }
 private:
    // request source information
    unsigned m_request_uid;
    unsigned m_sid;
    unsigned m_tpc;
    unsigned m_wid;
+   unsigned m_tid;
+
+   float m_l2_util;
 
    // where is this request now?
    enum mem_fetch_status m_status;
    unsigned long long m_status_change;
 
+   // Saugata: added where/when the request was finally serviced from
+   unsigned m_service_level;
+   unsigned m_service_time;
+
    // request type, address, size, mask
    mem_access_t m_access;
    unsigned m_data_size; // how much data is being written
diff -Naur gpgpu-sim-baseline/mem_latency_stat.cc gpgpu-sim/mem_latency_stat.cc
--- gpgpu-sim-baseline/mem_latency_stat.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/mem_latency_stat.cc	2015-10-13 06:38:50.558481857 -0400
@@ -30,7 +30,6 @@
 #include "mem_latency_stat.h"
 #include "gpu-sim.h"
 #include "gpu-misc.h"
-#include "gpu-cache.h"
 #include "shader.h"
 #include "mem_fetch.h"
 #include "stat-tool.h"
@@ -125,6 +124,34 @@
       }
    }
 
+   L2_write_miss=0;
+   L2_write_access=0;
+   L2_read_access=0;
+   L2_read_miss=0;
+
+   L2_bypassed=0;
+   L2_reuse_bypassed=0;
+   bypassed_rop_count=0;
+   non_bypassed_rop_count=0;
+   bypassed_rop_count_in_bypass=0;
+   L2_bloom_filter_cleared=0;
+   L2_info_cleared = 0;
+   DRAM_high_prio=0;
+   sched_from_high_prio=0;
+
+   for(int i=0;i<11;i++){
+   L2_util_histogram[i]=0;
+   }
+
+   // Saugata: these two stats are now irrelevant
+   // for(int i=0;i<14;i++){
+   // rop_bin_count[i]=0;
+   // }
+   //
+   // for(int i=0;i<11;i++){
+   // bypassed_ratio[i]=0;
+   // }
+
    L2_cbtoL2length = (unsigned int*) calloc(mem_config->m_n_mem, sizeof(unsigned int));
    L2_cbtoL2writelength = (unsigned int*) calloc(mem_config->m_n_mem, sizeof(unsigned int));
    L2_L2tocblength = (unsigned int*) calloc(mem_config->m_n_mem, sizeof(unsigned int));
diff -Naur gpgpu-sim-baseline/mem_latency_stat.h gpgpu-sim/mem_latency_stat.h
--- gpgpu-sim-baseline/mem_latency_stat.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/mem_latency_stat.h	2015-10-13 06:38:50.534481857 -0400
@@ -30,7 +30,6 @@
 
 #include <stdio.h>
 #include <zlib.h>
-#include <map>
 
 class memory_stats_t {
 public:
@@ -45,6 +44,7 @@
    void memlatstat_lat_pw();
    void memlatstat_print(unsigned n_mem, unsigned gpu_mem_n_bk);
 
+   void print( FILE *fp );
    void visualizer_print( gzFile visualizer_file );
 
    unsigned m_n_shader;
@@ -81,7 +81,20 @@
    unsigned ***mem_access_type_stats; // dram access type classification
 
 
-   // L2 cache stats
+   // stats
+   unsigned L2_write_access;
+   unsigned L2_write_miss;
+   unsigned L2_read_access;
+   unsigned L2_read_miss;
+   unsigned L2_bypassed; // Rachata --> How many times warps bypasses the cache
+   unsigned L2_reuse_bypassed; // Rachata --> How many times warps bypasses the cache
+   unsigned bypassed_rop_count; // Rachata --> How many times warps bypasses the cache
+   unsigned non_bypassed_rop_count; // Rachata --> How many times warps bypasses the cache
+   unsigned bypassed_rop_count_in_bypass; // Rachata --> How many times warps bypasses the cache
+   unsigned L2_info_cleared; // Rachata --> How many times warps info is cleared
+   unsigned L2_bloom_filter_cleared; // Rachata --> How many times bloom filter is cleared
+   unsigned DRAM_high_prio; // Rachata --> How many times warps info is cleared
+   unsigned sched_from_high_prio; // Rachata --> How many times warps info is cleared
    unsigned int *L2_cbtoL2length;
    unsigned int *L2_cbtoL2writelength;
    unsigned int *L2_L2tocblength;
@@ -89,7 +102,14 @@
    unsigned int *L2_dramtoL2writelength;
    unsigned int *L2_L2todramlength;
 
-   // DRAM access row locality stats 
+   // TODO: How to figure out number of warps
+   unsigned int *L2_warp_util; // Rachata --> cache utilization for each warp
+   unsigned int *L2_warp_bypassed; // Rachata --> cache utilization for each warp
+   unsigned long L2_util_histogram[11]; // Rachata --> cache utilization for each warp
+   // Saugata: these two stats are now irrelevant
+   // unsigned long rop_bin_count[14]; // Rachata --> cache utilization for each warp
+   // unsigned long bypassed_ratio[11]; // Rachata --> bypassed ratio
+
    unsigned int **concurrent_row_access; //concurrent_row_access[dram chip id][bank id]
    unsigned int **num_activates; //num_activates[dram chip id][bank id]
    unsigned int **row_access; //row_access[dram chip id][bank id]
diff -Naur gpgpu-sim-baseline/power_interface.cc gpgpu-sim/power_interface.cc
--- gpgpu-sim-baseline/power_interface.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/power_interface.cc	2015-10-13 06:38:50.582481858 -0400
@@ -99,8 +99,11 @@
 				(power_stats->get_sfu_active_lanes())/stat_sample_freq);
 
 
-		double n_icnt_simt_to_mem = (double)power_stats->get_icnt_simt_to_mem(); // # flits from SIMT clusters to memory partitions
-		double n_icnt_mem_to_simt = (double)power_stats->get_icnt_mem_to_simt(); // # flits from memory partitions to SIMT clusters
+		//NoC stats (32/4)--> Number of 32 bit words in 32B block
+		//unsigned l2cache_tot_access = power_stats->get_l2_read_accesses() +  power_stats->get_l2_write_accesses();
+		unsigned n_icnt_simt_to_mem = power_stats->get_icnt_simt_to_mem(); // # flits from SIMT clusters to memory partitions
+		unsigned n_icnt_mem_to_simt = power_stats->get_icnt_mem_to_simt(); // # flits from memory partitions to SIMT clusters
+		//wrapper->set_NoC_power((double)(n_icnt_mem_to_simt + n_icnt_simt_to_mem)); // Number of flits traversing the interconnect
 		wrapper->set_NoC_power(n_icnt_mem_to_simt, n_icnt_simt_to_mem); // Number of flits traversing the interconnect
 
 		wrapper->compute();
@@ -120,6 +123,6 @@
 	//wrapper->close_files();
 }
 
-void mcpat_reset_perf_count(class gpgpu_sim_wrapper *wrapper){
-	wrapper->reset_counters();
+void mcpat_reset_perf_count(class gpgpu_sim_wrapper *wrapper, bool do_print){
+	wrapper->reset_counters(do_print);
 }
diff -Naur gpgpu-sim-baseline/power_interface.h gpgpu-sim/power_interface.h
--- gpgpu-sim-baseline/power_interface.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/power_interface.h	2015-10-13 06:38:50.558481857 -0400
@@ -37,7 +37,7 @@
 
 void init_mcpat(const gpgpu_sim_config &config, class gpgpu_sim_wrapper *wrapper, unsigned stat_sample_freq, unsigned tot_inst, unsigned inst);
 void mcpat_cycle(const gpgpu_sim_config &config, const struct shader_core_config *shdr_config, class gpgpu_sim_wrapper *wrapper, class power_stat_t *power_stats,
-        unsigned stat_sample_freq, unsigned tot_cycle, unsigned cycle, unsigned tot_inst, unsigned inst);
-void mcpat_reset_perf_count(class gpgpu_sim_wrapper *wrapper);
+unsigned stat_sample_freq, unsigned tot_cycle, unsigned cycle, unsigned tot_inst, unsigned inst);
+void mcpat_reset_perf_count(class gpgpu_sim_wrapper *wrapper, bool do_print);
 
 #endif /* POWER_INTERFACE_H_ */
diff -Naur gpgpu-sim-baseline/power_stat.cc gpgpu-sim/power_stat.cc
--- gpgpu-sim-baseline/power_stat.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/power_stat.cc	2015-10-13 06:38:50.602481858 -0400
@@ -53,53 +53,86 @@
 }
 
 void power_mem_stat_t::init(){
-
-    shmem_read_access[CURRENT_STAT_IDX] = m_core_stats->gpgpu_n_shmem_bank_access; 	// Shared memory access
-    shmem_read_access[PREV_STAT_IDX] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
-
-    for(unsigned i=0; i<NUM_STAT_IDX; ++i){
-        core_cache_stats[i].clear();
-        l2_cache_stats[i].clear();
-
-        n_cmd[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_activity[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_nop[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_act[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_pre[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_rd[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_wr[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-        n_req[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
-
-        // Interconnect stats
-        n_mem_to_simt[i] = (long *)calloc(m_core_config->n_simt_clusters,sizeof(long)); // Counted at SM
-        n_simt_to_mem[i] = (long *)calloc(m_core_config->n_simt_clusters,sizeof(long)); // Counted at SM
+	inst_c_read_access[0] = m_core_stats->inst_c_read_access;
+	inst_c_read_miss[0] = m_core_stats->inst_c_read_miss;
+	const_c_read_access[0] = m_core_stats->const_c_read_access;
+	const_c_read_miss[0] = m_core_stats->const_c_read_miss;
+	text_c_read_access[0] = m_core_stats->text_c_read_access;
+	text_c_read_miss[0] = m_core_stats->text_c_read_miss;
+	l1d_read_access[0] = m_core_stats->l1d_read_access;
+	l1d_read_miss[0] = m_core_stats->l1d_read_miss;
+	l1d_write_access[0] = m_core_stats->l1d_write_access;
+	l1d_write_miss[0] = m_core_stats->l1d_write_miss;
+
+	shmem_read_access[0] = m_core_stats->gpgpu_n_shmem_bank_access; 	// Shared memory access
+
+	inst_c_read_access[1] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
+	inst_c_read_miss[1] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
+	const_c_read_access[1] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
+	const_c_read_miss[1] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
+	text_c_read_access[1] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
+	text_c_read_miss[1] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
+	l1d_read_access[1] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
+	l1d_read_miss[1] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
+	l1d_write_access[1] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
+	l1d_write_miss[1] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
+
+	shmem_read_access[1] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
+
+	// Low-level DRAM/L2-cache stats
+    for(unsigned i=0; i<2; ++i){
+    	n_l2_read_access[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+        n_l2_read_miss[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+        n_l2_write_access[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+        n_l2_write_miss[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+		n_cmd[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+		n_activity[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+		n_nop[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+		n_act[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+		n_pre[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+		n_rd[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+		n_wr[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+		n_req[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
+
+	    // Interconnect stats
+	    n_mem_to_simt[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned)); // Counted at memory partition
+	    n_simt_to_mem[i] = (unsigned *)calloc(m_core_config->n_simt_clusters,sizeof(unsigned)); // Counted at SM
     }
 }
 
 void power_mem_stat_t::save_stats(){
+	for(unsigned i=0; i<m_core_config->num_shader(); ++i){
+		inst_c_read_access[1][i] = inst_c_read_access[0][i] ;
+		inst_c_read_miss[1][i] = inst_c_read_miss[0][i] ;
+		const_c_read_access[1][i] = const_c_read_access[0][i] ;
+		const_c_read_miss[1][i] = const_c_read_miss[0][i] ;
+		text_c_read_access[1][i] = text_c_read_access[0][i] ;
+		text_c_read_miss[1][i] = text_c_read_miss[0][i] ;
+		l1d_read_access[1][i] = l1d_read_access[0][i] ;
+		l1d_read_miss[1][i] = l1d_read_miss[0][i] ;
+		l1d_write_access[1][i] = l1d_write_access[0][i] ;
+		l1d_write_miss[1][i] = l1d_write_miss[0][i] ;
+		shmem_read_access[1][i] = shmem_read_access[0][i] ; 	// Shared memory access
+
+		n_simt_to_mem[1][i] = n_simt_to_mem[0][i]; // Interconnect
+	}
+
+	for(unsigned i=0; i<m_config->m_n_mem; ++i){
+    	n_l2_read_access[1][i] = n_l2_read_access[0][i];
+        n_l2_read_miss[1][i] = n_l2_read_miss[0][i];
+        n_l2_write_access[1][i] = n_l2_write_access[0][i];
+        n_l2_write_miss[1][i] = n_l2_write_miss[0][i];
+		n_cmd[1][i] = n_cmd[0][i];
+		n_activity[1][i] = n_activity[0][i];
+		n_nop[1][i] = n_nop[0][i];
+		n_act[1][i] = n_act[0][i];
+		n_pre[1][i] = n_pre[0][i];
+		n_rd[1][i] = n_rd[0][i];
+		n_wr[1][i] = n_wr[0][i];
+		n_req[1][i] = n_req[0][i];
 
-    core_cache_stats[PREV_STAT_IDX] = core_cache_stats[CURRENT_STAT_IDX];
-    l2_cache_stats[PREV_STAT_IDX] = l2_cache_stats[CURRENT_STAT_IDX];
-
-    for(unsigned i=0; i<m_core_config->num_shader(); ++i){
-        shmem_read_access[PREV_STAT_IDX][i] = shmem_read_access[CURRENT_STAT_IDX][i] ; 	// Shared memory access
-    }
-
-    for(unsigned i=0; i<m_config->m_n_mem; ++i){
-        n_cmd[PREV_STAT_IDX][i] = n_cmd[CURRENT_STAT_IDX][i];
-        n_activity[PREV_STAT_IDX][i] = n_activity[CURRENT_STAT_IDX][i];
-        n_nop[PREV_STAT_IDX][i] = n_nop[CURRENT_STAT_IDX][i];
-        n_act[PREV_STAT_IDX][i] = n_act[CURRENT_STAT_IDX][i];
-        n_pre[PREV_STAT_IDX][i] = n_pre[CURRENT_STAT_IDX][i];
-        n_rd[PREV_STAT_IDX][i] = n_rd[CURRENT_STAT_IDX][i];
-        n_wr[PREV_STAT_IDX][i] = n_wr[CURRENT_STAT_IDX][i];
-        n_req[PREV_STAT_IDX][i] = n_req[CURRENT_STAT_IDX][i];
-    }
-
-    for(unsigned i=0; i<m_core_config->n_simt_clusters;i++){
-        n_simt_to_mem[PREV_STAT_IDX][i] = n_simt_to_mem[CURRENT_STAT_IDX][i]; // Interconnect
-        n_mem_to_simt[PREV_STAT_IDX][i] = n_mem_to_simt[CURRENT_STAT_IDX][i]; // Interconnect
-    }
+		n_mem_to_simt[1][i] = n_mem_to_simt[0][i]; // Interconnect
+	}
 }
 
 void power_mem_stat_t::visualizer_print( gzFile power_visualizer_file ){
@@ -108,20 +141,29 @@
 
 void power_mem_stat_t::print (FILE *fout) const {
 	fprintf(fout, "\n\n==========Power Metrics -- Memory==========\n");
-    unsigned total_mem_reads=0;
-    unsigned total_mem_writes=0;
-    for(unsigned i=0; i<m_config->m_n_mem; ++i){
-        total_mem_reads += n_rd[CURRENT_STAT_IDX][i];
-        total_mem_writes += n_wr[CURRENT_STAT_IDX][i];
-    }
-    fprintf(fout, "Total memory controller accesses: %u\n", total_mem_reads+total_mem_writes);
-    fprintf(fout, "Total memory controller reads: %u\n", total_mem_reads);
-    fprintf(fout, "Total memory controller writes: %u\n", total_mem_writes);
-
-    fprintf(fout, "Core cache stats:\n");
-    core_cache_stats->print_stats(fout);
-    fprintf(fout, "L2 cache stats:\n");
-    l2_cache_stats->print_stats(fout);
+	unsigned total_mem_reads=0;
+	unsigned total_mem_writes=0;
+	for(unsigned i=0; i<m_config->m_n_mem; ++i){
+		total_mem_reads += n_rd[0][i];
+		total_mem_writes += n_wr[0][i];
+	}
+	fprintf(fout, "Total memory controller accesses: %u\n", total_mem_reads+total_mem_writes);
+	fprintf(fout, "Total memory controller reads: %u\n", total_mem_reads);
+	fprintf(fout, "Total memory controller writes: %u\n", total_mem_writes);
+	for(unsigned i=0; i<m_core_config->num_shader(); ++i){
+		fprintf(fout, "Shader core %d\n", i);
+		fprintf(fout, "\tTotal instruction cache access: %u\n", inst_c_read_access[0][i]);
+		fprintf(fout, "\tTotal instruction cache miss: %u\n", inst_c_read_miss[0][i]);
+		fprintf(fout, "\tTotal constant cache access: %u\n", const_c_read_access[0][i]);
+		fprintf(fout, "\tTotal constant cache miss: %u\n", const_c_read_miss[0][i]);
+		fprintf(fout, "\tTotal texture cache access: %u\n", text_c_read_access[0][i]);
+		fprintf(fout, "\tTotal texture cache miss: %u\n", text_c_read_miss[0][i]);
+		fprintf(fout, "\tTotal l1d read access: %u\n", l1d_read_access[0][i]);
+		fprintf(fout, "\tTotal l1d read miss: %u\n", l1d_read_miss[0][i]);
+		fprintf(fout, "\tTotal l1d write access: %u\n", l1d_write_access[0][i]);
+		fprintf(fout, "\tTotal l1d write miss: %u\n", l1d_write_miss[0][i]);
+		fprintf(fout, "\tTotal shared memory access: %u\n", shmem_read_access[0][i]);
+	}
 }
 
 
@@ -145,125 +187,125 @@
 void power_core_stat_t::print (FILE *fout)
 {
 	// per core statistics
-    fprintf(fout,"Power Metrics: \n");
-    for(unsigned i=0; i<m_config->num_shader();i++){
-        fprintf(fout,"core %u:\n",i);
-        fprintf(fout,"\tpipeline duty cycle =%f\n",m_pipeline_duty_cycle[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal Deocded Instructions=%u\n",m_num_decoded_insn[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal FP Deocded Instructions=%u\n",m_num_FPdecoded_insn[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal INT Deocded Instructions=%u\n",m_num_INTdecoded_insn[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal LOAD Queued Instructions=%u\n",m_num_loadqueued_insn[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal STORE Queued Instructions=%u\n",m_num_storequeued_insn[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal IALU Acesses=%u\n",m_num_ialu_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal FP Acesses=%u\n",m_num_fp_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal IMUL Acesses=%u\n",m_num_imul_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal IMUL24 Acesses=%u\n",m_num_imul24_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal IMUL32 Acesses=%u\n",m_num_imul32_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal IDIV Acesses=%u\n",m_num_idiv_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal FPMUL Acesses=%u\n",m_num_fpmul_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal SFU Acesses=%u\n",m_num_trans_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal FPDIV Acesses=%u\n",m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal SFU Acesses=%u\n",m_num_sfu_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal SP Acesses=%u\n",m_num_sp_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal MEM Acesses=%u\n",m_num_mem_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal SFU Commissions=%u\n",m_num_sfu_committed[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal SP Commissions=%u\n",m_num_sp_committed[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal MEM Commissions=%u\n",m_num_mem_committed[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal REG Reads=%u\n",m_read_regfile_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal REG Writes=%u\n",m_write_regfile_acesses[CURRENT_STAT_IDX][i]);
-        fprintf(fout,"\tTotal NON REG=%u\n",m_non_rf_operands[CURRENT_STAT_IDX][i]);
-    }
+	fprintf(fout,"Power Metrics: \n");
+	for(unsigned i=0; i<m_config->num_shader();i++){
+		fprintf(fout,"core %u:\n",i);
+		fprintf(fout,"\tpipeline duty cycle =%f\n",m_pipeline_duty_cycle[0][i]);
+		fprintf(fout,"\tTotal Deocded Instructions=%u\n",m_num_decoded_insn[0][i]);
+		fprintf(fout,"\tTotal FP Deocded Instructions=%u\n",m_num_FPdecoded_insn[0][i]);
+		fprintf(fout,"\tTotal INT Deocded Instructions=%u\n",m_num_INTdecoded_insn[0][i]);
+		fprintf(fout,"\tTotal LOAD Queued Instructions=%u\n",m_num_loadqueued_insn[0][i]);
+		fprintf(fout,"\tTotal STORE Queued Instructions=%u\n",m_num_storequeued_insn[0][i]);
+		fprintf(fout,"\tTotal IALU Acesses=%u\n",m_num_ialu_acesses[0][i]);
+		fprintf(fout,"\tTotal FP Acesses=%u\n",m_num_fp_acesses[0][i]);
+		fprintf(fout,"\tTotal IMUL Acesses=%u\n",m_num_imul_acesses[0][i]);
+		fprintf(fout,"\tTotal IMUL24 Acesses=%u\n",m_num_imul24_acesses[0][i]);
+		fprintf(fout,"\tTotal IMUL32 Acesses=%u\n",m_num_imul32_acesses[0][i]);
+		fprintf(fout,"\tTotal IDIV Acesses=%u\n",m_num_idiv_acesses[0][i]);
+		fprintf(fout,"\tTotal FPMUL Acesses=%u\n",m_num_fpmul_acesses[0][i]);
+		fprintf(fout,"\tTotal SFU Acesses=%u\n",m_num_trans_acesses[0][i]);
+		fprintf(fout,"\tTotal FPDIV Acesses=%u\n",m_num_fpdiv_acesses[0][i]);
+		fprintf(fout,"\tTotal SFU Acesses=%u\n",m_num_sfu_acesses[0][i]);
+		fprintf(fout,"\tTotal SP Acesses=%u\n",m_num_sp_acesses[0][i]);
+		fprintf(fout,"\tTotal MEM Acesses=%u\n",m_num_mem_acesses[0][i]);
+		fprintf(fout,"\tTotal SFU Commissions=%u\n",m_num_sfu_committed[0][i]);
+		fprintf(fout,"\tTotal SP Commissions=%u\n",m_num_sp_committed[0][i]);
+		fprintf(fout,"\tTotal MEM Commissions=%u\n",m_num_mem_committed[0][i]);
+		fprintf(fout,"\tTotal REG Reads=%u\n",m_read_regfile_acesses[0][i]);
+		fprintf(fout,"\tTotal REG Writes=%u\n",m_write_regfile_acesses[0][i]);
+		fprintf(fout,"\tTotal NON REG=%u\n",m_non_rf_operands[0][i]);
+	}
 }
 void power_core_stat_t::init()
 {
-    m_pipeline_duty_cycle[CURRENT_STAT_IDX]=m_core_stats->m_pipeline_duty_cycle;
-    m_num_decoded_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_decoded_insn;
-    m_num_FPdecoded_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_FPdecoded_insn;
-    m_num_INTdecoded_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_INTdecoded_insn;
-    m_num_storequeued_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_storequeued_insn;
-    m_num_loadqueued_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_loadqueued_insn;
-    m_num_ialu_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_ialu_acesses;
-    m_num_fp_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_fp_acesses;
-    m_num_imul_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_imul_acesses;
-    m_num_imul24_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_imul24_acesses;
-    m_num_imul32_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_imul32_acesses;
-    m_num_fpmul_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_fpmul_acesses;
-    m_num_idiv_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_idiv_acesses;
-    m_num_fpdiv_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_fpdiv_acesses;
-    m_num_sp_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_sp_acesses;
-    m_num_sfu_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_sfu_acesses;
-    m_num_trans_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_trans_acesses;
-    m_num_mem_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_mem_acesses;
-    m_num_sp_committed[CURRENT_STAT_IDX]=m_core_stats->m_num_sp_committed;
-    m_num_sfu_committed[CURRENT_STAT_IDX]=m_core_stats->m_num_sfu_committed;
-    m_num_mem_committed[CURRENT_STAT_IDX]=m_core_stats->m_num_mem_committed;
-    m_read_regfile_acesses[CURRENT_STAT_IDX]=m_core_stats->m_read_regfile_acesses;
-    m_write_regfile_acesses[CURRENT_STAT_IDX]=m_core_stats->m_write_regfile_acesses;
-    m_non_rf_operands[CURRENT_STAT_IDX]=m_core_stats->m_non_rf_operands;
-    m_active_sp_lanes[CURRENT_STAT_IDX]=m_core_stats->m_active_sp_lanes;
-    m_active_sfu_lanes[CURRENT_STAT_IDX]=m_core_stats->m_active_sfu_lanes;
-    m_num_tex_inst[CURRENT_STAT_IDX]=m_core_stats->m_num_tex_inst;
-
-
-    m_pipeline_duty_cycle[PREV_STAT_IDX]=(float*)calloc(m_config->num_shader(),sizeof(float));
-    m_num_decoded_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_FPdecoded_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_INTdecoded_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_storequeued_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_loadqueued_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_ialu_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_fp_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_tex_inst[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_imul_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_imul24_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_imul32_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_fpmul_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_idiv_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_fpdiv_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_sp_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_sfu_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_trans_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_mem_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_sp_committed[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_sfu_committed[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_num_mem_committed[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_read_regfile_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_write_regfile_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_non_rf_operands[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_active_sp_lanes[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
-    m_active_sfu_lanes[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+	m_pipeline_duty_cycle[0]=m_core_stats->m_pipeline_duty_cycle;
+	m_num_decoded_insn[0]=m_core_stats->m_num_decoded_insn;
+	m_num_FPdecoded_insn[0]=m_core_stats->m_num_FPdecoded_insn;
+	m_num_INTdecoded_insn[0]=m_core_stats->m_num_INTdecoded_insn;
+	m_num_storequeued_insn[0]=m_core_stats->m_num_storequeued_insn;
+	m_num_loadqueued_insn[0]=m_core_stats->m_num_loadqueued_insn;
+    m_num_ialu_acesses[0]=m_core_stats->m_num_ialu_acesses;
+    m_num_fp_acesses[0]=m_core_stats->m_num_fp_acesses;
+    m_num_imul_acesses[0]=m_core_stats->m_num_imul_acesses;
+    m_num_imul24_acesses[0]=m_core_stats->m_num_imul24_acesses;
+    m_num_imul32_acesses[0]=m_core_stats->m_num_imul32_acesses;
+    m_num_fpmul_acesses[0]=m_core_stats->m_num_fpmul_acesses;
+    m_num_idiv_acesses[0]=m_core_stats->m_num_idiv_acesses;
+    m_num_fpdiv_acesses[0]=m_core_stats->m_num_fpdiv_acesses;
+    m_num_sp_acesses[0]=m_core_stats->m_num_sp_acesses;
+    m_num_sfu_acesses[0]=m_core_stats->m_num_sfu_acesses;
+    m_num_trans_acesses[0]=m_core_stats->m_num_trans_acesses;
+    m_num_mem_acesses[0]=m_core_stats->m_num_mem_acesses;
+    m_num_sp_committed[0]=m_core_stats->m_num_sp_committed;
+    m_num_sfu_committed[0]=m_core_stats->m_num_sfu_committed;
+    m_num_mem_committed[0]=m_core_stats->m_num_mem_committed;
+    m_read_regfile_acesses[0]=m_core_stats->m_read_regfile_acesses;
+    m_write_regfile_acesses[0]=m_core_stats->m_write_regfile_acesses;
+    m_non_rf_operands[0]=m_core_stats->m_non_rf_operands;
+    m_active_sp_lanes[0]=m_core_stats->m_active_sp_lanes;
+    m_active_sfu_lanes[0]=m_core_stats->m_active_sfu_lanes;
+    m_num_tex_inst[0]=m_core_stats->m_num_tex_inst;
+
+
+	m_pipeline_duty_cycle[1]=(float*)calloc(m_config->num_shader(),sizeof(float));
+	m_num_decoded_insn[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+	m_num_FPdecoded_insn[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+	m_num_INTdecoded_insn[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+	m_num_storequeued_insn[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+	m_num_loadqueued_insn[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_ialu_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_fp_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_tex_inst[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_imul_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_imul24_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_imul32_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_fpmul_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_idiv_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_fpdiv_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_sp_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_sfu_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_trans_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_mem_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_sp_committed[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_sfu_committed[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_num_mem_committed[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_read_regfile_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_write_regfile_acesses[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_non_rf_operands[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_active_sp_lanes[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
+    m_active_sfu_lanes[1]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
 }
 
 void power_core_stat_t::save_stats(){
-for(unsigned i=0; i<m_config->num_shader(); ++i){
-    m_pipeline_duty_cycle[PREV_STAT_IDX][i]=m_pipeline_duty_cycle[CURRENT_STAT_IDX][i];
-    m_num_decoded_insn[PREV_STAT_IDX][i]=	m_num_decoded_insn[CURRENT_STAT_IDX][i];
-    m_num_FPdecoded_insn[PREV_STAT_IDX][i]=m_num_FPdecoded_insn[CURRENT_STAT_IDX][i];
-    m_num_INTdecoded_insn[PREV_STAT_IDX][i]=m_num_INTdecoded_insn[CURRENT_STAT_IDX][i];
-    m_num_storequeued_insn[PREV_STAT_IDX][i]=m_num_storequeued_insn[CURRENT_STAT_IDX][i];
-    m_num_loadqueued_insn[PREV_STAT_IDX][i]=m_num_loadqueued_insn[CURRENT_STAT_IDX][i];
-    m_num_ialu_acesses[PREV_STAT_IDX][i]=m_num_ialu_acesses[CURRENT_STAT_IDX][i];
-    m_num_fp_acesses[PREV_STAT_IDX][i]=m_num_fp_acesses[CURRENT_STAT_IDX][i];
-    m_num_tex_inst[PREV_STAT_IDX][i]=m_num_tex_inst[CURRENT_STAT_IDX][i];
-    m_num_imul_acesses[PREV_STAT_IDX][i]=m_num_imul_acesses[CURRENT_STAT_IDX][i];
-    m_num_imul24_acesses[PREV_STAT_IDX][i]=m_num_imul24_acesses[CURRENT_STAT_IDX][i];
-    m_num_imul32_acesses[PREV_STAT_IDX][i]=m_num_imul32_acesses[CURRENT_STAT_IDX][i];
-    m_num_fpmul_acesses[PREV_STAT_IDX][i]=m_num_fpmul_acesses[CURRENT_STAT_IDX][i];
-    m_num_idiv_acesses[PREV_STAT_IDX][i]=m_num_idiv_acesses[CURRENT_STAT_IDX][i];
-    m_num_fpdiv_acesses[PREV_STAT_IDX][i]=m_num_fpdiv_acesses[CURRENT_STAT_IDX][i];
-    m_num_sp_acesses[PREV_STAT_IDX][i]=m_num_sp_acesses[CURRENT_STAT_IDX][i];
-    m_num_sfu_acesses[PREV_STAT_IDX][i]=m_num_sfu_acesses[CURRENT_STAT_IDX][i];
-    m_num_trans_acesses[PREV_STAT_IDX][i]=m_num_trans_acesses[CURRENT_STAT_IDX][i];
-    m_num_mem_acesses[PREV_STAT_IDX][i]=m_num_mem_acesses[CURRENT_STAT_IDX][i];
-    m_num_sp_committed[PREV_STAT_IDX][i]=m_num_sp_committed[CURRENT_STAT_IDX][i];
-    m_num_sfu_committed[PREV_STAT_IDX][i]=m_num_sfu_committed[CURRENT_STAT_IDX][i];
-    m_num_mem_committed[PREV_STAT_IDX][i]=m_num_mem_committed[CURRENT_STAT_IDX][i];
-    m_read_regfile_acesses[PREV_STAT_IDX][i]=m_read_regfile_acesses[CURRENT_STAT_IDX][i];
-    m_write_regfile_acesses[PREV_STAT_IDX][i]=m_write_regfile_acesses[CURRENT_STAT_IDX][i];
-    m_non_rf_operands[PREV_STAT_IDX][i]=m_non_rf_operands[CURRENT_STAT_IDX][i];
-    m_active_sp_lanes[PREV_STAT_IDX][i]=m_active_sp_lanes[CURRENT_STAT_IDX][i];
-    m_active_sfu_lanes[PREV_STAT_IDX][i]=m_active_sfu_lanes[CURRENT_STAT_IDX][i];
-    }
+	for(unsigned i=0; i<m_config->num_shader(); ++i){
+		m_pipeline_duty_cycle[1][i]=m_pipeline_duty_cycle[0][i];
+		m_num_decoded_insn[1][i]=	m_num_decoded_insn[0][i];
+		m_num_FPdecoded_insn[1][i]=m_num_FPdecoded_insn[0][i];
+		m_num_INTdecoded_insn[1][i]=m_num_INTdecoded_insn[0][i];
+		m_num_storequeued_insn[1][i]=m_num_storequeued_insn[0][i];
+		m_num_loadqueued_insn[1][i]=m_num_loadqueued_insn[0][i];
+		m_num_ialu_acesses[1][i]=m_num_ialu_acesses[0][i];
+		m_num_fp_acesses[1][i]=m_num_fp_acesses[0][i];
+		m_num_tex_inst[1][i]=m_num_tex_inst[0][i];
+		m_num_imul_acesses[1][i]=m_num_imul_acesses[0][i];
+		m_num_imul24_acesses[1][i]=m_num_imul24_acesses[0][i];
+		m_num_imul32_acesses[1][i]=m_num_imul32_acesses[0][i];
+		m_num_fpmul_acesses[1][i]=m_num_fpmul_acesses[0][i];
+		m_num_idiv_acesses[1][i]=m_num_idiv_acesses[0][i];
+		m_num_fpdiv_acesses[1][i]=m_num_fpdiv_acesses[0][i];
+		m_num_sp_acesses[1][i]=m_num_sp_acesses[0][i];
+		m_num_sfu_acesses[1][i]=m_num_sfu_acesses[0][i];
+		m_num_trans_acesses[1][i]=m_num_trans_acesses[0][i];
+		m_num_mem_acesses[1][i]=m_num_mem_acesses[0][i];
+		m_num_sp_committed[1][i]=m_num_sp_committed[0][i];
+		m_num_sfu_committed[1][i]=m_num_sfu_committed[0][i];
+		m_num_mem_committed[1][i]=m_num_mem_committed[0][i];
+		m_read_regfile_acesses[1][i]=m_read_regfile_acesses[0][i];
+		m_write_regfile_acesses[1][i]=m_write_regfile_acesses[0][i];
+		m_non_rf_operands[1][i]=m_non_rf_operands[0][i];
+	    m_active_sp_lanes[1][i]=m_active_sp_lanes[0][i];
+	    m_active_sfu_lanes[1][i]=m_active_sfu_lanes[0][i];
+	}
 }
 
 power_stat_t::power_stat_t( const struct shader_core_config *shader_config,float * average_pipeline_duty_cycle,float *active_sms,shader_core_stats * shader_stats, const struct memory_config *mem_config,memory_stats_t * memory_stats)
diff -Naur gpgpu-sim-baseline/power_stat.h gpgpu-sim/power_stat.h
--- gpgpu-sim-baseline/power_stat.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/power_stat.h	2015-10-13 06:38:50.502481856 -0400
@@ -31,44 +31,38 @@
 #include <stdio.h>
 #include <zlib.h>
 #include "mem_latency_stat.h"
+#include "shader.h"
 #include "gpu-sim.h"
 
-typedef enum _stat_idx{
-    CURRENT_STAT_IDX = 0,    // Current activity count
-    PREV_STAT_IDX,           // Previous sample activity count
-    NUM_STAT_IDX     // Total number of samples
-}stat_idx;
-
-
 struct shader_core_power_stats_pod {
-    // [CURRENT_STAT_IDX] = CURRENT_STAT_IDX stat, [PREV_STAT_IDX] = last reading
-    float *m_pipeline_duty_cycle[NUM_STAT_IDX];
-    unsigned *m_num_decoded_insn[NUM_STAT_IDX]; // number of instructions committed by this shader core
-    unsigned *m_num_FPdecoded_insn[NUM_STAT_IDX]; // number of instructions committed by this shader core
-    unsigned *m_num_INTdecoded_insn[NUM_STAT_IDX]; // number of instructions committed by this shader core
-    unsigned *m_num_storequeued_insn[NUM_STAT_IDX];
-    unsigned *m_num_loadqueued_insn[NUM_STAT_IDX];
-    unsigned *m_num_ialu_acesses[NUM_STAT_IDX];
-    unsigned *m_num_fp_acesses[NUM_STAT_IDX];
-    unsigned *m_num_tex_inst[NUM_STAT_IDX];
-    unsigned *m_num_imul_acesses[NUM_STAT_IDX];
-    unsigned *m_num_imul32_acesses[NUM_STAT_IDX];
-    unsigned *m_num_imul24_acesses[NUM_STAT_IDX];
-    unsigned *m_num_fpmul_acesses[NUM_STAT_IDX];
-    unsigned *m_num_idiv_acesses[NUM_STAT_IDX];
-    unsigned *m_num_fpdiv_acesses[NUM_STAT_IDX];
-    unsigned *m_num_sp_acesses[NUM_STAT_IDX];
-    unsigned *m_num_sfu_acesses[NUM_STAT_IDX];
-    unsigned *m_num_trans_acesses[NUM_STAT_IDX];
-    unsigned *m_num_mem_acesses[NUM_STAT_IDX];
-    unsigned *m_num_sp_committed[NUM_STAT_IDX];
-    unsigned *m_num_sfu_committed[NUM_STAT_IDX];
-    unsigned *m_num_mem_committed[NUM_STAT_IDX];
-    unsigned *m_active_sp_lanes[NUM_STAT_IDX];
-    unsigned *m_active_sfu_lanes[NUM_STAT_IDX];
-    unsigned *m_read_regfile_acesses[NUM_STAT_IDX];
-    unsigned *m_write_regfile_acesses[NUM_STAT_IDX];
-    unsigned *m_non_rf_operands[NUM_STAT_IDX];
+	// [0] = Current stat, [1] = last reading
+	float *m_pipeline_duty_cycle[2];
+    unsigned *m_num_decoded_insn[2]; // number of instructions committed by this shader core
+    unsigned *m_num_FPdecoded_insn[2]; // number of instructions committed by this shader core
+    unsigned *m_num_INTdecoded_insn[2]; // number of instructions committed by this shader core
+    unsigned *m_num_storequeued_insn[2];
+    unsigned *m_num_loadqueued_insn[2];
+    unsigned *m_num_ialu_acesses[2];
+    unsigned *m_num_fp_acesses[2];
+    unsigned *m_num_tex_inst[2];
+    unsigned *m_num_imul_acesses[2];
+    unsigned *m_num_imul32_acesses[2];
+    unsigned *m_num_imul24_acesses[2];
+    unsigned *m_num_fpmul_acesses[2];
+    unsigned *m_num_idiv_acesses[2];
+    unsigned *m_num_fpdiv_acesses[2];
+    unsigned *m_num_sp_acesses[2];
+    unsigned *m_num_sfu_acesses[2];
+    unsigned *m_num_trans_acesses[2];
+    unsigned *m_num_mem_acesses[2];
+    unsigned *m_num_sp_committed[2];
+    unsigned *m_num_sfu_committed[2];
+    unsigned *m_num_mem_committed[2];
+    unsigned *m_active_sp_lanes[2];
+    unsigned *m_active_sfu_lanes[2];
+    unsigned *m_read_regfile_acesses[2];
+    unsigned *m_write_regfile_acesses[2];
+    unsigned *m_non_rf_operands[2];
 };
 
 class power_core_stat_t : public shader_core_power_stats_pod {
@@ -88,25 +82,38 @@
 };
 
 struct mem_power_stats_pod{
-    // [CURRENT_STAT_IDX] = CURRENT_STAT_IDX stat, [PREV_STAT_IDX] = last reading
-    class cache_stats core_cache_stats[NUM_STAT_IDX]; // Total core stats
-    class cache_stats l2_cache_stats[NUM_STAT_IDX]; // Total L2 partition stats
-
-    unsigned *shmem_read_access[NUM_STAT_IDX];   // Shared memory access
-
-    // Low level DRAM stats
-    unsigned *n_cmd[NUM_STAT_IDX];
-    unsigned *n_activity[NUM_STAT_IDX];
-    unsigned *n_nop[NUM_STAT_IDX];
-    unsigned *n_act[NUM_STAT_IDX];
-    unsigned *n_pre[NUM_STAT_IDX];
-    unsigned *n_rd[NUM_STAT_IDX];
-    unsigned *n_wr[NUM_STAT_IDX];
-    unsigned *n_req[NUM_STAT_IDX];
+	// [0] = Current stat, [1] = last reading
+	unsigned *inst_c_read_access[2];	// Instruction cache read access
+	unsigned *inst_c_read_miss[2];		// Instruction cache read miss
+	unsigned *const_c_read_access[2];	// Constant cache read access
+	unsigned *const_c_read_miss[2];		// Constant cache read miss
+	unsigned *text_c_read_access[2];	// Texture cache read access
+	unsigned *text_c_read_miss[2];		// Texture cache read miss
+	unsigned *l1d_read_access[2];		// L1 Data cache read access
+	unsigned *l1d_read_miss[2];			// L1 Data cache read miss
+	unsigned *l1d_write_access[2];		// L1 Data cache write access
+	unsigned *l1d_write_miss[2];		// L1 Data cache write miss
+	unsigned *shmem_read_access[2]; 	// Shared memory access
+
+	// Low level L2 stats
+	unsigned *n_l2_read_access[2];
+	unsigned *n_l2_read_miss[2];
+	unsigned *n_l2_write_access[2];
+	unsigned *n_l2_write_miss[2];
+
+	// Low level DRAM stats
+    unsigned *n_cmd[2];
+    unsigned *n_activity[2];
+    unsigned *n_nop[2];
+    unsigned *n_act[2];
+    unsigned *n_pre[2];
+    unsigned *n_rd[2];
+    unsigned *n_wr[2];
+    unsigned *n_req[2];
 
     // Interconnect stats
-    long *n_simt_to_mem[NUM_STAT_IDX];
-    long *n_mem_to_simt[NUM_STAT_IDX];
+    unsigned *n_simt_to_mem[2];
+    unsigned *n_mem_to_simt[2];
 };
 
 
@@ -138,483 +145,461 @@
 	   *m_active_sms=0;
    }
 
-    unsigned get_total_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_decoded_insn[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_decoded_insn[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_total_int_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_INTdecoded_insn[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_INTdecoded_insn[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_total_fp_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_FPdecoded_insn[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_FPdecoded_insn[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_total_load_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_loadqueued_insn[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_loadqueued_insn[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_total_store_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_storequeued_insn[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_storequeued_insn[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_sp_committed_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_sp_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sp_committed[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_sfu_committed_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_sfu_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sfu_committed[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_mem_committed_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_mem_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_mem_committed[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_committed_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_mem_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_mem_committed[PREV_STAT_IDX][i])
-                    +(pwr_core_stat->m_num_sfu_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sfu_committed[PREV_STAT_IDX][i])
-                    +(pwr_core_stat->m_num_sp_committed[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sp_committed[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_regfile_reads(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_read_regfile_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_read_regfile_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_regfile_writes(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_write_regfile_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_write_regfile_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    float get_pipeline_duty(){
-        float total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_pipeline_duty_cycle[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_pipeline_duty_cycle[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_non_regfile_operands(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_non_rf_operands[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_non_rf_operands[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_sp_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_sp_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sp_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_sfu_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_sfu_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_sfu_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-    unsigned get_trans_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_trans_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_trans_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_mem_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_mem_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_mem_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_intdiv_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_idiv_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_idiv_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_fpdiv_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_fpdiv_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_intmul32_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_imul32_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul32_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_intmul24_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_imul24_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul24_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
-
-    unsigned get_intmul_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_imul_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_imul24_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul24_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_imul32_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul32_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
+   unsigned get_total_inst(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_decoded_insn[0][i]) - (pwr_core_stat->m_num_decoded_insn[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_total_int_inst(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_INTdecoded_insn[0][i]) - (pwr_core_stat->m_num_INTdecoded_insn[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_total_fp_inst(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_FPdecoded_insn[0][i]) - (pwr_core_stat->m_num_FPdecoded_insn[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_total_load_inst(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_loadqueued_insn[0][i]) - (pwr_core_stat->m_num_loadqueued_insn[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_total_store_inst(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_storequeued_insn[0][i]) - (pwr_core_stat->m_num_storequeued_insn[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_sp_committed_inst(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_sp_committed[0][i]) - (pwr_core_stat->m_num_sp_committed[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_sfu_committed_inst(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_sfu_committed[0][i]) - (pwr_core_stat->m_num_sfu_committed[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_mem_committed_inst(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_mem_committed[0][i]) - (pwr_core_stat->m_num_mem_committed[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_committed_inst(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_mem_committed[0][i]) - (pwr_core_stat->m_num_mem_committed[1][i])
+				       +(pwr_core_stat->m_num_sfu_committed[0][i]) - (pwr_core_stat->m_num_sfu_committed[1][i])
+				       +(pwr_core_stat->m_num_sp_committed[0][i]) - (pwr_core_stat->m_num_sp_committed[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_regfile_reads(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_read_regfile_acesses[0][i]) - (pwr_core_stat->m_read_regfile_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_regfile_writes(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_write_regfile_acesses[0][i]) - (pwr_core_stat->m_write_regfile_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-    unsigned get_fpmul_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_fp_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_fp_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
+   float get_pipeline_duty(){
+	   float total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_pipeline_duty_cycle[0][i]) - (pwr_core_stat->m_pipeline_duty_cycle[1][i]);
+	   }
+	   return total_inst;
+   }
 
-    float get_sp_active_lanes(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_active_sp_lanes[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_active_sp_lanes[PREV_STAT_IDX][i]);
-        }
-        return (total_inst/m_config->num_shader())/m_config->gpgpu_num_sp_units;
-    }
+   unsigned get_non_regfile_operands(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_non_rf_operands[0][i]) - (pwr_core_stat->m_non_rf_operands[1][i]);
+	   }
+	   return total_inst;
+   }
 
-    float get_sfu_active_lanes(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_active_sfu_lanes[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_active_sfu_lanes[PREV_STAT_IDX][i]);
-        }
+   unsigned get_sp_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_sp_acesses[0][i]) - (pwr_core_stat->m_num_sp_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-        return (total_inst/m_config->num_shader())/m_config->gpgpu_num_sfu_units;
-    }
+   unsigned get_sfu_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_sfu_acesses[0][i]) - (pwr_core_stat->m_num_sfu_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_trans_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_trans_acesses[0][i]) - (pwr_core_stat->m_num_trans_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-    unsigned get_tot_fpu_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_fp_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_fp_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_fpdiv_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_fpmul_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_fpmul_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_imul24_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul24_acesses[PREV_STAT_IDX][i])+
-                    (pwr_core_stat->m_num_imul_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul_acesses[PREV_STAT_IDX][i]);
-        }
-        total_inst += get_total_load_inst()+get_total_store_inst()+get_tex_inst();
-        return total_inst;
-    }
+   unsigned get_mem_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_mem_acesses[0][i]) - (pwr_core_stat->m_num_mem_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-    unsigned get_tot_sfu_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-                total_inst+= (pwr_core_stat->m_num_idiv_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_idiv_acesses[PREV_STAT_IDX][i])+
-                            (pwr_core_stat->m_num_imul32_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_imul32_acesses[PREV_STAT_IDX][i])+
-                            (pwr_core_stat->m_num_trans_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_trans_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
+   unsigned get_intdiv_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_idiv_acesses[0][i]) - (pwr_core_stat->m_num_idiv_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-    unsigned get_ialu_accessess(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_ialu_acesses[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_ialu_acesses[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
+   unsigned get_fpdiv_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_fpdiv_acesses[0][i]) - (pwr_core_stat->m_num_fpdiv_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-    unsigned get_tex_inst(){
-        unsigned total_inst=0;
-        for(unsigned i=0; i<m_config->num_shader();i++){
-            total_inst+=(pwr_core_stat->m_num_tex_inst[CURRENT_STAT_IDX][i]) - (pwr_core_stat->m_num_tex_inst[PREV_STAT_IDX][i]);
-        }
-        return total_inst;
-    }
+   unsigned get_intmul32_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_imul32_acesses[0][i]) - (pwr_core_stat->m_num_imul32_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-    unsigned get_constant_c_accesses(){
-        enum mem_access_type access_type[] = {CONST_ACC_R};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
+   unsigned get_intmul24_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_imul24_acesses[0][i]) - (pwr_core_stat->m_num_imul24_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_constant_c_misses(){
-        enum mem_access_type access_type[] = {CONST_ACC_R};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
+   unsigned get_intmul_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_imul_acesses[0][i]) - (pwr_core_stat->m_num_imul_acesses[1][i])+
+				   	   (pwr_core_stat->m_num_imul24_acesses[0][i]) - (pwr_core_stat->m_num_imul24_acesses[1][i])+
+				   	   (pwr_core_stat->m_num_imul32_acesses[0][i]) - (pwr_core_stat->m_num_imul32_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_constant_c_hits(){
-        return (get_constant_c_accesses()-get_constant_c_misses());
-    }
-    unsigned get_texture_c_accesses(){
-        enum mem_access_type access_type[] = {TEXTURE_ACC_R};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
+   unsigned get_fpmul_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_fp_acesses[0][i]) - (pwr_core_stat->m_num_fp_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_texture_c_misses(){
-        enum mem_access_type access_type[] = {TEXTURE_ACC_R};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
+   float get_sp_active_lanes(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_active_sp_lanes[0][i]) - (pwr_core_stat->m_active_sp_lanes[1][i]);
+	   }
+	   return (total_inst/m_config->num_shader())/m_config->gpgpu_num_sp_units;
+   }
 
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_texture_c_hits(){
-        return ( get_texture_c_accesses()- get_texture_c_misses());
-    }
-    unsigned get_inst_c_accesses(){
-        enum mem_access_type access_type[] = {INST_ACC_R};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
+   float get_sfu_active_lanes(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_active_sfu_lanes[0][i]) - (pwr_core_stat->m_active_sfu_lanes[1][i]);	   }
 
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_inst_c_misses(){
-        enum mem_access_type access_type[] = {INST_ACC_R};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_inst_c_hits(){
-        return (get_inst_c_accesses()-get_inst_c_misses());
-    }
+	   return (total_inst/m_config->num_shader())/m_config->gpgpu_num_sfu_units;
+   }
 
-    unsigned get_l1d_read_accesses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_R, LOCAL_ACC_R};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
+   unsigned get_tot_fpu_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_fp_acesses[0][i]) - (pwr_core_stat->m_num_fp_acesses[1][i])+
+			   (pwr_core_stat->m_num_fpdiv_acesses[0][i]) - (pwr_core_stat->m_num_fpdiv_acesses[1][i])+
+				       (pwr_core_stat->m_num_fpmul_acesses[0][i]) - (pwr_core_stat->m_num_fpmul_acesses[1][i])+
+				       (pwr_core_stat->m_num_imul24_acesses[0][i]) - (pwr_core_stat->m_num_imul24_acesses[1][i])+
+					(pwr_core_stat->m_num_imul_acesses[0][i]) - (pwr_core_stat->m_num_imul_acesses[1][i])       ;
+		   //printf("imul_accesses0: %d imul_acccesses1: %d imul0 - imul1: %d\n",(pwr_core_stat->m_num_imul_acesses[0][i]),(pwr_core_stat->m_num_imul_acesses[1][i]),(pwr_core_stat->m_num_imul_acesses[0][i]-pwr_core_stat->m_num_imul_acesses[1][i]));
+		   //printf("imul24_accesses0: %d imul24_acccesses1: %d imu24l0 - imul241: %d\n",(pwr_core_stat->m_num_imul24_acesses[0][i]),(pwr_core_stat->m_num_imul24_acesses[1][i]),(pwr_core_stat->m_num_imul24_acesses[0][i]-pwr_core_stat->m_num_imul24_acesses[1][i]));
+		   //printf("total_insn:%d\n",total_inst);
+
+
+	   }
+	   total_inst += get_total_load_inst()+get_total_store_inst()+get_tex_inst();
+	   return total_inst;
+   }
 
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_l1d_read_misses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_R, LOCAL_ACC_R};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
+   unsigned get_tot_sfu_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+= 
+				        (pwr_core_stat->m_num_idiv_acesses[0][i]) - (pwr_core_stat->m_num_idiv_acesses[1][i])+
+				        (pwr_core_stat->m_num_imul32_acesses[0][i]) - (pwr_core_stat->m_num_imul32_acesses[1][i])+
+						(pwr_core_stat->m_num_trans_acesses[0][i]) - (pwr_core_stat->m_num_trans_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_l1d_read_hits(){
-        return (get_l1d_read_accesses()-get_l1d_read_misses());
-    }
-    unsigned get_l1d_write_accesses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
+   unsigned get_ialu_accessess(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_ialu_acesses[0][i]) - (pwr_core_stat->m_num_ialu_acesses[1][i]);
+	   }
+	   return total_inst;
+   }
 
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_l1d_write_misses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
+   unsigned get_tex_inst(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_core_stat->m_num_tex_inst[0][i]) - (pwr_core_stat->m_num_tex_inst[1][i]);
+	   }
+	   return total_inst;
+   }
 
-        return (pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->core_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_l1d_write_hits(){
-        return (get_l1d_write_accesses()-get_l1d_write_misses());
-    }
-    unsigned get_cache_misses(){
-        return get_l1d_read_misses()+get_constant_c_misses()+get_l1d_write_misses()+get_texture_c_misses();
-    }
+   unsigned get_constant_c_accesses(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_mem_stat->const_c_read_access[0][i]) - (pwr_mem_stat->const_c_read_access[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_constant_c_misses(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_mem_stat->const_c_read_miss[0][i]) - (pwr_mem_stat->const_c_read_miss[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_constant_c_hits(){
+	   return (get_constant_c_accesses()-get_constant_c_misses());
+   }
+   unsigned get_texture_c_accesses(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_mem_stat->text_c_read_access[0][i]) - (pwr_mem_stat->text_c_read_access[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_texture_c_misses(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_mem_stat->text_c_read_miss[0][i]) - (pwr_mem_stat->text_c_read_miss[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_texture_c_hits(){
+	   return ( get_texture_c_accesses()- get_texture_c_misses());
+   }
+   unsigned get_inst_c_accesses(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_mem_stat->inst_c_read_access[0][i]) - (pwr_mem_stat->inst_c_read_access[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_inst_c_misses(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_mem_stat->inst_c_read_miss[0][i]) - (pwr_mem_stat->inst_c_read_miss[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_inst_c_hits(){
+	   return (get_inst_c_accesses()-get_inst_c_misses());
+   }
+   unsigned get_l1d_read_accesses(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_mem_stat->l1d_read_access[0][i]) - (pwr_mem_stat->l1d_read_access[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_l1d_read_misses(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_mem_stat->l1d_read_miss[0][i]) - (pwr_mem_stat->l1d_read_miss[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_l1d_read_hits(){
+	   return (get_l1d_read_accesses()-get_l1d_read_misses());
+   }
+   unsigned get_l1d_write_accesses(){
+ 	   unsigned total_inst=0;
+ 	   for(unsigned i=0; i<m_config->num_shader();i++){
+ 		   total_inst+=(pwr_mem_stat->l1d_write_access[0][i]) - (pwr_mem_stat->l1d_write_access[1][i]);
+ 	   }
+ 	   return total_inst;
+    }
+   unsigned get_l1d_write_misses(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_mem_stat->l1d_write_miss[0][i]) - (pwr_mem_stat->l1d_write_miss[1][i]);
+	   }
+	   return total_inst;
+   }
+   unsigned get_l1d_write_hits(){
+	   return (get_l1d_write_accesses()-get_l1d_write_misses());
+   }
+	unsigned get_cache_misses(){
+     return get_l1d_read_misses()+get_constant_c_misses()+get_l1d_write_misses()+
+		      get_texture_c_misses();
+	}
 	
-    unsigned get_cache_read_misses(){
-        return get_l1d_read_misses()+get_constant_c_misses()+get_texture_c_misses();
-    }
-
-    unsigned get_cache_write_misses(){
-        return get_l1d_write_misses();
-    }
-
-    unsigned get_shmem_read_access(){
-       unsigned total_inst=0;
-       for(unsigned i=0; i<m_config->num_shader();i++){
-           total_inst+=(pwr_mem_stat->shmem_read_access[CURRENT_STAT_IDX][i]) - (pwr_mem_stat->shmem_read_access[PREV_STAT_IDX][i]);
-       }
-       return total_inst;
-    }
-
-    unsigned get_l2_read_accesses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_R, LOCAL_ACC_R, CONST_ACC_R, TEXTURE_ACC_R, INST_ACC_R};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-
-    unsigned get_l2_read_misses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_R, LOCAL_ACC_R, CONST_ACC_R, TEXTURE_ACC_R, INST_ACC_R};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
-
-        return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
+	unsigned get_cache_read_misses(){
+     return get_l1d_read_misses()+get_constant_c_misses()+
+		      get_texture_c_misses();
+	}
+
+	unsigned get_cache_write_misses(){
+     return get_l1d_write_misses();
+	}
+
+   unsigned get_shmem_read_access(){
+	   unsigned total_inst=0;
+	   for(unsigned i=0; i<m_config->num_shader();i++){
+		   total_inst+=(pwr_mem_stat->shmem_read_access[0][i]) - (pwr_mem_stat->shmem_read_access[1][i]);
+	   }
+	   return total_inst;
+   }
 
-    unsigned get_l2_read_hits(){
-        return (get_l2_read_accesses()-get_l2_read_misses());
-    }
+   unsigned get_l2_read_accesses(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_l2_read_access[0][i] - pwr_mem_stat->n_l2_read_access[1][i]);
+	   }
+	   return total;
+   }
 
-    unsigned get_l2_write_accesses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W, L1_WRBK_ACC};
-        enum cache_request_status request_status[] = {HIT, MISS, HIT_RESERVED};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
+   unsigned get_l2_read_misses(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_l2_read_miss[0][i] - pwr_mem_stat->n_l2_read_miss[1][i]);
+	   }
+	   return total;
+   }
 
-        return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
+   unsigned get_l2_read_hits(){
+	   return (get_l2_read_accesses()-get_l2_read_misses());
+   }
 
-    unsigned get_l2_write_misses(){
-        enum mem_access_type access_type[] = {GLOBAL_ACC_W, LOCAL_ACC_W, L1_WRBK_ACC};
-        enum cache_request_status request_status[] = {MISS};
-        unsigned num_access_type = sizeof(access_type)/sizeof(enum mem_access_type);
-        unsigned num_request_status = sizeof(request_status)/sizeof(enum cache_request_status);
+   unsigned get_l2_write_accesses(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_l2_write_access[0][i] - pwr_mem_stat->n_l2_write_access[1][i]);
+	   }
+	   return total;
+   }
 
-        return (pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status)) -
-                (pwr_mem_stat->l2_cache_stats[PREV_STAT_IDX].get_stats(access_type, num_access_type, request_status, num_request_status));
-    }
-    unsigned get_l2_write_hits(){
-        return (get_l2_write_accesses()-get_l2_write_misses());
-    }
-    unsigned get_dram_cmd(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_cmd[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_cmd[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_activity(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_activity[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_activity[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_nop(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_nop[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_nop[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_act(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_act[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_act[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_pre(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_pre[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_pre[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_rd(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_rd[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_rd[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_wr(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_wr[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_wr[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
-    unsigned get_dram_req(){
-        unsigned total=0;
-        for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
-            total += (pwr_mem_stat->n_req[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_req[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
+   unsigned get_l2_write_misses(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_l2_write_miss[0][i] - pwr_mem_stat->n_l2_write_miss[1][i]);
+	   }
+	   return total;
+   }
+   unsigned get_l2_write_hits(){
+	   return (get_l2_write_accesses()-get_l2_write_misses());
+   }
+   unsigned get_dram_cmd(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_cmd[0][i] - pwr_mem_stat->n_cmd[1][i]);
+	   }
+	   return total;
+   }
+   unsigned get_dram_activity(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_activity[0][i] - pwr_mem_stat->n_activity[1][i]);
+	   }
+	   return total;
+   }
+   unsigned get_dram_nop(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_nop[0][i] - pwr_mem_stat->n_nop[1][i]);
+	   }
+	   return total;
+   }
+   unsigned get_dram_act(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_act[0][i] - pwr_mem_stat->n_act[1][i]);
+	   }
+	   return total;
+   }
+   unsigned get_dram_pre(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_pre[0][i] - pwr_mem_stat->n_pre[1][i]);
+	   }
+	   return total;
+   }
+   unsigned get_dram_rd(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_rd[0][i] - pwr_mem_stat->n_rd[1][i]);
+	   }
+	   return total;
+   }
+   unsigned get_dram_wr(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_wr[0][i] - pwr_mem_stat->n_wr[1][i]);
+	   }
+	   return total;
+   }
+   unsigned get_dram_req(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_req[0][i] - pwr_mem_stat->n_req[1][i]);
+	   }
+	   return total;
+   }
 
-    long get_icnt_simt_to_mem(){
-        long total=0;
-        for(unsigned i=0; i<m_config->n_simt_clusters; ++i){
-            total += (pwr_mem_stat->n_simt_to_mem[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_simt_to_mem[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
+   unsigned get_icnt_simt_to_mem(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_config->num_shader(); ++i){
+		   total += (pwr_mem_stat->n_simt_to_mem[0][i] - pwr_mem_stat->n_simt_to_mem[1][i]);
+	   }
+	   return total;
+   }
 
-    long get_icnt_mem_to_simt(){
-        long total=0;
-        for(unsigned i=0; i<m_config->n_simt_clusters; ++i){
-            total += (pwr_mem_stat->n_mem_to_simt[CURRENT_STAT_IDX][i] - pwr_mem_stat->n_mem_to_simt[PREV_STAT_IDX][i]);
-        }
-        return total;
-    }
+   unsigned get_icnt_mem_to_simt(){
+	   unsigned total=0;
+	   for(unsigned i=0; i<m_mem_config->m_n_mem; ++i){
+		   total += (pwr_mem_stat->n_mem_to_simt[0][i] - pwr_mem_stat->n_mem_to_simt[1][i]);
+	   }
+	   return total;
+   }
 
    power_core_stat_t * pwr_core_stat;
    power_mem_stat_t * pwr_mem_stat;
diff -Naur gpgpu-sim-baseline/shader.cc gpgpu-sim/shader.cc
--- gpgpu-sim-baseline/shader.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/shader.cc	2015-10-13 06:38:50.518481856 -0400
@@ -21,7 +21,7 @@
 // DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
 // FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 // DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-// SERVICES; LOSSp OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 // CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 // OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
@@ -40,16 +40,25 @@
 #include "mem_fetch.h"
 #include "mem_latency_stat.h"
 #include "visualizer.h"
-#include "../statwrapper.h"
+#include "../intersim/statwraper.h"
+#include "../intersim/interconnect_interface.h"
 #include "icnt_wrapper.h"
 #include <string.h>
 #include <limits.h>
-#include "traffic_breakdown.h"
 #include "shader_trace.h"
 
 #define PRIORITIZE_MSHR_OVER_WB 1
 #define MAX(a,b) (((a)>(b))?(a):(b))
-#define MIN(a,b) (((a)<(b))?(a):(b))
+
+// Saugata: static maps for global information on all memory requests
+std::map<unsigned, unsigned> ldst_unit::num_warps_all_to_icnt_by_DRAM_count;
+std::map<unsigned, unsigned long long> ldst_unit::total_divergence_all_to_icnt_by_DRAM_count;
+std::map<unsigned, unsigned long long> ldst_unit::total_L2_divergence_all_to_icnt_by_DRAM_count;
+std::map<unsigned, unsigned long long> ldst_unit::total_DRAM_divergence_all_to_icnt_by_DRAM_count;
+std::map<unsigned, unsigned> ldst_unit::num_warps_some_to_icnt_by_DRAM_count;
+std::map<unsigned, unsigned long long> ldst_unit::total_divergence_some_to_icnt_by_DRAM_count;
+std::map<unsigned, unsigned long long> ldst_unit::total_L2_divergence_some_to_icnt_by_DRAM_count;
+std::map<unsigned, unsigned long long> ldst_unit::total_DRAM_divergence_some_to_icnt_by_DRAM_count;
     
 
 /////////////////////////////////////////////////////////////////////////////
@@ -72,10 +81,11 @@
                                   const struct shader_core_config *config,
                                   const struct memory_config *mem_config,
                                   shader_core_stats *stats )
-   : core_t( gpu, NULL, config->warp_size, config->n_thread_per_shader ),
-     m_barriers( this, config->max_warps_per_shader, config->max_cta_per_core, config->max_barriers_per_cta, config->warp_size ),
+   : m_barriers( config->max_warps_per_shader, config->max_cta_per_core ),
      m_dynamic_warp_id(0)
 {
+    m_kernel = NULL;
+    m_gpu = gpu;
     m_cluster = cluster;
     m_config = config;
     m_memory_config = mem_config;
@@ -91,6 +101,7 @@
     }
     
     m_threadState = (thread_ctx_t*) calloc(sizeof(thread_ctx_t), config->n_thread_per_shader);
+    m_thread = (ptx_thread_info**) calloc(sizeof(ptx_thread_info*), config->n_thread_per_shader);
     
     m_not_completed = 0;
     m_active_threads.reset();
@@ -120,6 +131,7 @@
     m_L1I = new read_only_cache( name,m_config->m_L1I_config,m_sid,get_shader_instruction_cache_id(),m_icnt,IN_L1I_MISS_QUEUE);
     
     m_warp.resize(m_config->max_warps_per_shader, shd_warp_t(this, warp_size));
+    initilizeSIMTStack(config->max_warps_per_shader,this->get_config()->warp_size);
     m_scoreboard = new Scoreboard(m_sid, m_config->max_warps_per_shader);
     
     //scedulers
@@ -131,8 +143,6 @@
                                          CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE :
                                          sched_config.find("gto") != std::string::npos ?
                                          CONCRETE_SCHEDULER_GTO :
-                                         sched_config.find("warp_limiting") != std::string::npos ?
-                                         CONCRETE_SCHEDULER_WARP_LIMITING:
                                          NUM_CONCRETE_SCHEDULERS;
     assert ( scheduler != NUM_CONCRETE_SCHEDULERS );
     
@@ -182,21 +192,6 @@
                                      )
                 );
                 break;
-            case CONCRETE_SCHEDULER_WARP_LIMITING:
-                schedulers.push_back(
-                    new swl_scheduler( m_stats,
-                                       this,
-                                       m_scoreboard,
-                                       m_simt_stack,
-                                       &m_warp,
-                                       &m_pipeline_reg[ID_OC_SP],
-                                       &m_pipeline_reg[ID_OC_SFU],
-                                       &m_pipeline_reg[ID_OC_MEM],
-                                       i,
-                                       config->gpgpu_scheduler_string
-                                     )
-                );
-                break;
             default:
                 abort();
         };
@@ -367,13 +362,46 @@
 {
 	unsigned long long  thread_icount_uarch=0;
 	unsigned long long  warp_icount_uarch=0;
+	unsigned l1_dcache_read_hits=0;
+	unsigned l1_dcache_read_misses=0;
+	unsigned l1_dcache_write_accesses=0;
+	unsigned l1_dcache_write_misses=0;
+	unsigned icache_hits=0;
+	unsigned icache_misses=0;
+	unsigned ccache_hits=0;
+	unsigned ccache_misses=0;
+	unsigned tcache_hits=0;
+	unsigned tcache_misses=0;
+
+
 
     for(unsigned i=0; i < m_config->num_shader(); i++) {
         thread_icount_uarch += m_num_sim_insn[i];
         warp_icount_uarch += m_num_sim_winsn[i];
+        l1_dcache_read_hits += l1d_read_access[i]-l1d_read_miss[i];
+        l1_dcache_write_accesses += l1d_write_access[i];
+        l1_dcache_read_misses += l1d_read_miss[i];
+        l1_dcache_write_misses += l1d_write_miss[i];
+        icache_hits+=inst_c_read_access[i]-inst_c_read_miss[i];
+        icache_misses+=inst_c_read_miss[i];
+        tcache_hits+=text_c_read_access[i]-text_c_read_miss[i];
+        tcache_misses+=text_c_read_miss[i];
+        ccache_hits+=const_c_read_access[i]-const_c_read_miss[i];
+        ccache_misses+=const_c_read_miss[i];
     }
     fprintf(fout,"gpgpu_n_tot_thrd_icount = %lld\n", thread_icount_uarch);
     fprintf(fout,"gpgpu_n_tot_w_icount = %lld\n", warp_icount_uarch);
+    fprintf(fout,"gpgpu_n_icache_hits = %d\n", icache_hits );
+    fprintf(fout,"gpgpu_n_icache_misses = %d\n", icache_misses );
+    fprintf(fout,"gpgpu_n_l1dcache_read_hits = %d\n", l1_dcache_read_hits );
+    fprintf(fout,"gpgpu_n_l1dcache_read_misses = %d\n", l1_dcache_read_misses );
+    fprintf(fout,"gpgpu_n_l1dcache_write_accesses = %d\n", l1_dcache_write_accesses );
+    fprintf(fout,"gpgpu_n_l1dcache_wirte_misses = %d\n", l1_dcache_write_misses );
+    fprintf(fout,"gpgpu_n_tcache_hits = %d\n", tcache_hits );
+    fprintf(fout,"gpgpu_n_tcache_misses = %d\n", tcache_misses );
+    fprintf(fout,"gpgpu_n_ccache_hits = %d\n", ccache_hits );
+    fprintf(fout,"gpgpu_n_ccache_misses = %d\n", ccache_misses);
+
 
     fprintf(fout,"gpgpu_n_stall_shd_mem = %d\n", gpgpu_n_stall_shd_mem );
     fprintf(fout,"gpgpu_n_mem_read_local = %d\n", gpgpu_n_mem_read_local);
@@ -382,7 +410,18 @@
     fprintf(fout,"gpgpu_n_mem_write_global = %d\n", gpgpu_n_mem_write_global);
     fprintf(fout,"gpgpu_n_mem_texture = %d\n", gpgpu_n_mem_texture);
     fprintf(fout,"gpgpu_n_mem_const = %d\n", gpgpu_n_mem_const);
-
+/*
+   unsigned a,m;
+   for (unsigned i=0, a=0, m=0;i<m_n_shader;i++) 
+      m_sc[i]->L1cache_print(stdout,a,m);
+   printf("L1 Data Cache Total Miss Rate = %0.3f\n", (float)m/a);
+   for (i=0,a=0,m=0;i<m_n_shader;i++) 
+       m_sc[i]->L1texcache_print(stdout,a,m);
+   printf("L1 Texture Cache Total Miss Rate = %0.3f\n", (float)m/a);
+   for (i=0,a=0,m=0;i<m_n_shader;i++) 
+       m_sc[i]->L1constcache_print(stdout,a,m);
+   printf("L1 Const Cache Total Miss Rate = %0.3f\n", (float)m/a);
+*/
    fprintf(fout, "gpgpu_n_load_insn  = %d\n", gpgpu_n_load_insn);
    fprintf(fout, "gpgpu_n_store_insn = %d\n", gpgpu_n_store_insn);
    fprintf(fout, "gpgpu_n_shmem_insn = %d\n", gpgpu_n_shmem_insn);
@@ -399,10 +438,8 @@
    fprintf(fout, "gpgpu_stall_shd_mem[c_mem][bk_conf] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][BK_CONF]);
    fprintf(fout, "gpgpu_stall_shd_mem[c_mem][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][MSHR_RC_FAIL]);
    fprintf(fout, "gpgpu_stall_shd_mem[c_mem][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][ICNT_RC_FAIL]);
-   fprintf(fout, "gpgpu_stall_shd_mem[c_mem][data_port_stall] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][DATA_PORT_STALL]);
    fprintf(fout, "gpgpu_stall_shd_mem[t_mem][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][MSHR_RC_FAIL]);
    fprintf(fout, "gpgpu_stall_shd_mem[t_mem][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][ICNT_RC_FAIL]);
-   fprintf(fout, "gpgpu_stall_shd_mem[t_mem][data_port_stall] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][DATA_PORT_STALL]);
    fprintf(fout, "gpgpu_stall_shd_mem[s_mem][bk_conf] = %d\n", gpu_stall_shd_mem_breakdown[S_MEM][BK_CONF]);
    fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][bk_conf] = %d\n", 
            gpu_stall_shd_mem_breakdown[G_MEM_LD][BK_CONF] + 
@@ -416,12 +453,6 @@
            gpu_stall_shd_mem_breakdown[L_MEM_LD][COAL_STALL] + 
            gpu_stall_shd_mem_breakdown[L_MEM_ST][COAL_STALL]    
            ); // coalescing stall + bank conflict at data cache 
-   fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][data_port_stall] = %d\n", 
-           gpu_stall_shd_mem_breakdown[G_MEM_LD][DATA_PORT_STALL] + 
-           gpu_stall_shd_mem_breakdown[G_MEM_ST][DATA_PORT_STALL] + 
-           gpu_stall_shd_mem_breakdown[L_MEM_LD][DATA_PORT_STALL] + 
-           gpu_stall_shd_mem_breakdown[L_MEM_ST][DATA_PORT_STALL]    
-           ); // data port stall at data cache 
    fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][MSHR_RC_FAIL]);
    fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][ICNT_RC_FAIL]);
    fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][WB_ICNT_RC_FAIL]);
@@ -448,9 +479,6 @@
    for (unsigned i = 3; i < m_config->warp_size + 3; i++) 
       fprintf(fout, "\tW%d:%d", i-2, shader_cycle_distro[i]);
    fprintf(fout, "\n");
-
-   m_outgoing_traffic_stats->print(fout); 
-   m_incoming_traffic_stats->print(fout); 
 }
 
 void shader_core_stats::event_warp_issued( unsigned s_id, unsigned warp_id, unsigned num_issued, unsigned dynamic_warp_id ) {
@@ -490,22 +518,18 @@
 
     // warp issue breakdown
     unsigned sid = m_config->gpgpu_warp_issue_shader;
+    gzprintf(visualizer_file, "WarpIssueSlotBreakdown:");
     unsigned count = 0;
     unsigned warp_id_issued_sum = 0;
-    gzprintf(visualizer_file, "WarpIssueSlotBreakdown:");
-    if(m_shader_warp_slot_issue_distro[sid].size() > 0){
-        for ( std::vector<unsigned>::const_iterator iter = m_shader_warp_slot_issue_distro[ sid ].begin();
-              iter != m_shader_warp_slot_issue_distro[ sid ].end(); iter++, count++ ) {
-            unsigned diff = count < m_last_shader_warp_slot_issue_distro.size() ?
-                            *iter - m_last_shader_warp_slot_issue_distro[ count ] :
-                            *iter;
-            gzprintf( visualizer_file, " %d", diff );
-            warp_id_issued_sum += diff;
-        }
-        m_last_shader_warp_slot_issue_distro = m_shader_warp_slot_issue_distro[ sid ];
-    }else{
-        gzprintf( visualizer_file, " 0");
+    for ( std::vector<unsigned>::const_iterator iter = m_shader_warp_slot_issue_distro[ sid ].begin();
+          iter != m_shader_warp_slot_issue_distro[ sid ].end(); iter++, count++ ) {
+        unsigned diff = count < m_last_shader_warp_slot_issue_distro.size() ?
+                        *iter - m_last_shader_warp_slot_issue_distro[ count ] :
+                        *iter;
+        gzprintf( visualizer_file, " %d", diff );
+        warp_id_issued_sum += diff;
     }
+    m_last_shader_warp_slot_issue_distro = m_shader_warp_slot_issue_distro[ sid ];
     gzprintf(visualizer_file,"\n");
 
     #define DYNAMIC_WARP_PRINT_RESOLUTION 32
@@ -513,29 +537,25 @@
     unsigned dynamic_id_issued_sum = 0;
     count = 0;
     gzprintf(visualizer_file, "WarpIssueDynamicIdBreakdown:");
-    if(m_shader_dynamic_warp_issue_distro[sid].size() > 0){
-        for ( std::vector<unsigned>::const_iterator iter = m_shader_dynamic_warp_issue_distro[ sid ].begin();
-              iter != m_shader_dynamic_warp_issue_distro[ sid ].end(); iter++, count++ ) {
-            unsigned diff = count < m_last_shader_dynamic_warp_issue_distro.size() ?
-                            *iter - m_last_shader_dynamic_warp_issue_distro[ count ] :
-                            *iter;
-            total_issued_this_resolution += diff;
-            if ( ( count + 1 ) % DYNAMIC_WARP_PRINT_RESOLUTION == 0 ) {
-                gzprintf( visualizer_file, " %d", total_issued_this_resolution );
-                dynamic_id_issued_sum += total_issued_this_resolution;
-                total_issued_this_resolution = 0;
-            }
-        }
-        if ( count % DYNAMIC_WARP_PRINT_RESOLUTION != 0 ) {
+    for ( std::vector<unsigned>::const_iterator iter = m_shader_dynamic_warp_issue_distro[ sid ].begin();
+          iter != m_shader_dynamic_warp_issue_distro[ sid ].end(); iter++, count++ ) {
+        unsigned diff = count < m_last_shader_dynamic_warp_issue_distro.size() ?
+                        *iter - m_last_shader_dynamic_warp_issue_distro[ count ] :
+                        *iter;
+        total_issued_this_resolution += diff;
+        if ( ( count + 1 ) % DYNAMIC_WARP_PRINT_RESOLUTION == 0 ) {
             gzprintf( visualizer_file, " %d", total_issued_this_resolution );
             dynamic_id_issued_sum += total_issued_this_resolution;
+            total_issued_this_resolution = 0;
         }
-        m_last_shader_dynamic_warp_issue_distro = m_shader_dynamic_warp_issue_distro[ sid ];
-        assert( warp_id_issued_sum == dynamic_id_issued_sum );
-    }else{
-        gzprintf( visualizer_file, " 0");
     }
+    if ( count % DYNAMIC_WARP_PRINT_RESOLUTION != 0 ) {
+        gzprintf( visualizer_file, " %d", total_issued_this_resolution );
+        dynamic_id_issued_sum += total_issued_this_resolution;
+    }
+    m_last_shader_dynamic_warp_issue_distro = m_shader_dynamic_warp_issue_distro[ sid ];
     gzprintf(visualizer_file,"\n");
+    assert( warp_id_issued_sum == dynamic_id_issued_sum );
 
     // overall cache miss rates
     gzprintf(visualizer_file, "gpgpu_n_cache_bkconflict: %d\n", gpgpu_n_cache_bkconflict);
@@ -572,9 +592,9 @@
         m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();
         if( pI1 ) {
             m_stats->m_num_decoded_insn[m_sid]++;
-            if(pI1->oprnd_type==INT_OP){
+            if(pI1->op2==INT_OP){
                 m_stats->m_num_INTdecoded_insn[m_sid]++;
-            }else if(pI1->oprnd_type==FP_OP) {
+            }else if(pI1->op2==FP_OP) {
             	m_stats->m_num_FPdecoded_insn[m_sid]++;
             }
            const warp_inst_t* pI2 = ptx_fetch_inst(pc+pI1->isize);
@@ -582,9 +602,9 @@
                m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(1,pI2);
                m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();
                m_stats->m_num_decoded_insn[m_sid]++;
-               if(pI2->oprnd_type==INT_OP){
+               if(pI2->op2==INT_OP){
                    m_stats->m_num_INTdecoded_insn[m_sid]++;
-               }else if(pI2->oprnd_type==FP_OP) {
+               }else if(pI2->op2==FP_OP) {
             	   m_stats->m_num_FPdecoded_insn[m_sid]++;
                }
            }
@@ -602,6 +622,7 @@
             unsigned warp_id = (m_last_warp_fetched+1+i) % m_config->max_warps_per_shader;
 
             // this code checks if this warp has finished executing and can be reclaimed
+            // TODO: Rachata -> Can put some stat collections here
             if( m_warp[warp_id].hardware_done() && !m_scoreboard->pendingWrites(warp_id) && !m_warp[warp_id].done_exit() ) {
                 bool did_exit=false;
                 for( unsigned t=0; t<m_config->warp_size;t++) {
@@ -638,6 +659,7 @@
                                               warp_id,
                                               m_sid,
                                               m_tpc,
+                                              -1, // TODO: Rachata --> tid
                                               m_memory_config );
                 std::list<cache_event> events;
                 enum cache_request_status status = m_L1I->access( (new_addr_type)ppc, mf, gpu_sim_cycle+gpu_tot_sim_cycle,events);
@@ -662,6 +684,9 @@
 
     m_L1I->cycle();
 
+    // Power stats
+    m_L1I->get_stats(m_stats->inst_c_read_access[m_sid], m_stats->inst_c_read_miss[m_sid]);
+    assert(m_stats->inst_c_read_access[m_sid]>=m_stats->inst_c_read_miss[m_sid]);
     if( m_L1I->access_ready() ) {
         mem_fetch *mf = m_L1I->next_access();
         m_warp[mf->get_wid()].clear_imiss_pending();
@@ -671,7 +696,7 @@
 
 void shader_core_ctx::func_exec_inst( warp_inst_t &inst )
 {
-    execute_warp_inst_t(inst);
+    execute_warp_inst_t(inst, m_config->warp_size);
     if( inst.is_load() || inst.is_store() )
         inst.generate_mem_accesses();
 }
@@ -687,15 +712,12 @@
     (*pipe_reg)->issue( active_mask, warp_id, gpu_tot_sim_cycle + gpu_sim_cycle, m_warp[warp_id].get_dynamic_warp_id() ); // dynamic instruction information
     m_stats->shader_cycle_distro[2+(*pipe_reg)->active_count()]++;
     func_exec_inst( **pipe_reg );
-    if( next_inst->op == BARRIER_OP ){
-    	m_warp[warp_id].store_info_of_last_inst_at_barrier(*pipe_reg);
-        m_barriers.warp_reaches_barrier(m_warp[warp_id].get_cta_id(),warp_id,const_cast<warp_inst_t*> (next_inst));
-
-    }else if( next_inst->op == MEMORY_BARRIER_OP ){
+    if( next_inst->op == BARRIER_OP ) 
+        m_barriers.warp_reaches_barrier(m_warp[warp_id].get_cta_id(),warp_id);
+    else if( next_inst->op == MEMORY_BARRIER_OP ) 
         m_warp[warp_id].set_membar();
-    }
 
-    updateSIMTStack(warp_id,*pipe_reg);
+    updateSIMTStack(warp_id,m_config->warp_size,*pipe_reg);
     m_scoreboard->reserveRegisters(*pipe_reg);
     m_warp[warp_id].set_next_pc(next_inst->pc + next_inst->isize);
 }
@@ -868,7 +890,8 @@
                                     issued_inst=true;
                                     warp_inst_issued = true;
                                 }
-                            }                         }
+                            } 
+                        }
                     } else {
                         SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) fails scoreboard\n",
                                        (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
@@ -930,13 +953,7 @@
 bool scheduler_unit::sort_warps_by_oldest_dynamic_id(shd_warp_t* lhs, shd_warp_t* rhs)
 {
     if (rhs && lhs) {
-        if ( lhs->done_exit() || lhs->waiting() ) {
-            return false;
-        } else if ( rhs->done_exit() || rhs->waiting() ) {
-            return true;
-        } else {
-            return lhs->get_dynamic_warp_id() < rhs->get_dynamic_warp_id();
-        }
+        return lhs->get_dynamic_warp_id() < rhs->get_dynamic_warp_id();
     } else {
         return lhs < rhs;
     }
@@ -983,32 +1000,32 @@
 
 void two_level_active_scheduler::order_warps()
 {
-    //Move waiting warps to m_pending_warps
+	//Move waiting warps to m_pending_warps
     unsigned num_demoted = 0;
-    for (   std::vector< shd_warp_t* >::iterator iter = m_next_cycle_prioritized_warps.begin();
-            iter != m_next_cycle_prioritized_warps.end(); ) {
-        bool waiting = (*iter)->waiting();
-        for (int i=0; i<4; i++){
-            const warp_inst_t* inst = (*iter)->ibuffer_next_inst();
-            //Is the instruction waiting on a long operation?
-            if ( inst && inst->in[i] > 0 && this->m_scoreboard->islongop((*iter)->get_warp_id(), inst->in[i])){
-                waiting = true;
-            }
-        }
+	for (	std::vector< shd_warp_t* >::iterator iter = m_next_cycle_prioritized_warps.begin();
+			iter != m_next_cycle_prioritized_warps.end(); ) {
+		bool waiting = (*iter)->waiting();
+		for (int i=0; i<4; i++){
+			const warp_inst_t* inst = (*iter)->ibuffer_next_inst();
+			//Is the instruction waiting on a long operation?
+			if ( inst && inst->in[i] > 0 && this->m_scoreboard->islongop((*iter)->get_warp_id(), inst->in[i])){
+				waiting = true;
+			}
+		}
 
-        if( waiting ) {
-            m_pending_warps.push_back(*iter);
-            iter = m_next_cycle_prioritized_warps.erase(iter);
+		if( waiting ) {
+			m_pending_warps.push_back(*iter);
+			iter = m_next_cycle_prioritized_warps.erase(iter);
             SCHED_DPRINTF( "DEMOTED warp_id=%d, dynamic_warp_id=%d\n",
                            (*iter)->get_warp_id(),
                            (*iter)->get_dynamic_warp_id() );
             ++num_demoted;
-        } else {
+		} else {
             ++iter;
         }
-    }
+	}
 
-    //If there is space in m_next_cycle_prioritized_warps, promote the next m_pending_warps
+	//If there is space in m_next_cycle_prioritized_warps, promote the next m_pending_warps
     unsigned num_promoted = 0;
     if ( SCHEDULER_PRIORITIZATION_SRR == m_outer_level_prioritization ) {
         while ( m_next_cycle_prioritized_warps.size() < m_max_active_warps ) {
@@ -1018,7 +1035,7 @@
                            (m_next_cycle_prioritized_warps.back())->get_warp_id(),
                            (m_next_cycle_prioritized_warps.back())->get_dynamic_warp_id() );
             ++num_promoted;
-        }
+    	}
     } else {
         fprintf( stderr,
                  "Unimplemented m_outer_level_prioritization: %d\n",
@@ -1028,44 +1045,6 @@
     assert( num_promoted == num_demoted );
 }
 
-swl_scheduler::swl_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
-                               Scoreboard* scoreboard, simt_stack** simt,
-                               std::vector<shd_warp_t>* warp,
-                               register_set* sp_out,
-                               register_set* sfu_out,
-                               register_set* mem_out,
-                               int id,
-                               char* config_string )
-    : scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id )
-{
-    unsigned m_prioritization_readin;
-    int ret = sscanf( config_string,
-                      "warp_limiting:%d:%d",
-                      &m_prioritization_readin,
-                      &m_num_warps_to_limit
-                     );
-    assert( 2 == ret );
-    m_prioritization = (scheduler_prioritization_type)m_prioritization_readin;
-    // Currently only GTO is implemented
-    assert( m_prioritization == SCHEDULER_PRIORITIZATION_GTO );
-    assert( m_num_warps_to_limit <= shader->get_config()->max_warps_per_shader );
-}
-
-void swl_scheduler::order_warps()
-{
-    if ( SCHEDULER_PRIORITIZATION_GTO == m_prioritization ) {
-        order_by_priority( m_next_cycle_prioritized_warps,
-                           m_supervised_warps,
-                           m_last_supervised_issued,
-                           MIN( m_num_warps_to_limit, m_supervised_warps.size() ),
-                           ORDERING_GREEDY_THEN_PRIORITY_FUNC,
-                           scheduler_unit::sort_warps_by_oldest_dynamic_id );
-    } else {
-        fprintf(stderr, "swl_scheduler m_prioritization = %d\n", m_prioritization);
-        abort();
-    }
-}
-
 void shader_core_ctx::read_operands()
 {
 }
@@ -1155,7 +1134,7 @@
         m_fu[n]->active_lanes_in_pipeline();
         enum pipeline_stage_name_t issue_port = m_issue_port[n];
         register_set& issue_inst = m_pipeline_reg[ issue_port ];
-        warp_inst_t** ready_reg = issue_inst.get_ready();
+	warp_inst_t** ready_reg = issue_inst.get_ready();
         if( issue_inst.has_ready() && m_fu[n]->can_issue( **ready_reg ) ) {
             bool schedule_wb_now = !m_fu[n]->stallable();
             int resbus = -1;
@@ -1178,28 +1157,55 @@
    }
 }
 
-void ldst_unit::get_cache_stats(cache_stats &cs) {
-    // Adds stats to 'cs' from each cache
-    if(m_L1D)
-        cs += m_L1D->get_stats();
-    if(m_L1C)
-        cs += m_L1C->get_stats();
-    if(m_L1T)
-        cs += m_L1T->get_stats();
+void ldst_unit::get_cache_stats(unsigned &read_accesses, unsigned &write_accesses, unsigned &read_misses, unsigned &write_misses, unsigned cache_type){
+	switch(cache_type){
+	default:
+	case 0: // L1D
+		if( m_L1D ) {
+			//m_L1D->get_stats(accesses, misses);
+			m_L1D->get_data_stats(read_accesses,read_misses,write_accesses, write_misses);
+		}
+		break;
+	case 1:
+		if( m_L1C ){
+			m_L1C->get_stats(read_accesses, read_misses);
+		}
+		break;
+	case 2:
+		if( m_L1T ){
+			m_L1T->get_stats(read_accesses, read_misses);
+		}
+	}
+}
 
+void ldst_unit::set_stats(){
+	// Sets the cache stats in m_stats
+	if( m_L1D ) {
+		m_L1D->get_data_stats(m_stats->l1d_read_access[m_sid],  m_stats->l1d_read_miss[m_sid],m_stats->l1d_write_access[m_sid], m_stats->l1d_write_miss[m_sid]);
+	}
+	if( m_L1C ){
+		m_L1C->get_stats(m_stats->const_c_read_access[m_sid], m_stats->const_c_read_miss[m_sid]);
+	}
+	if( m_L1T ){
+		m_L1T->get_stats(m_stats->text_c_read_access[m_sid], m_stats->text_c_read_miss[m_sid]);
+	}
 }
 
-void ldst_unit::get_L1D_sub_stats(struct cache_sub_stats &css) const{
-    if(m_L1D)
-        m_L1D->get_sub_stats(css);
-}
-void ldst_unit::get_L1C_sub_stats(struct cache_sub_stats &css) const{
-    if(m_L1C)
-        m_L1C->get_sub_stats(css);
-}
-void ldst_unit::get_L1T_sub_stats(struct cache_sub_stats &css) const{
-    if(m_L1T)
-        m_L1T->get_sub_stats(css);
+void ldst_unit::set_icnt_power_stats(unsigned &simt_to_mem) const{
+	unsigned l1d=0;
+	unsigned tex=0;
+	unsigned l1c=0;
+
+	if( m_L1D ) {
+		m_L1D->set_icnt_power_stats(l1d);
+	}
+	if( m_L1T ){
+		m_L1T->set_icnt_power_stats(tex);
+	}
+	if( m_L1C ){
+		m_L1C->set_icnt_power_stats(l1c);
+	}
+	simt_to_mem = n_simt_to_mem+l1d+tex+l1c; // All components that push packets into the interconnect
 }
 
 void shader_core_ctx::warp_inst_complete(const warp_inst_t &inst)
@@ -1208,11 +1214,11 @@
       printf("[warp_inst_complete] uid=%u core=%u warp=%u pc=%#x @ time=%llu issued@%llu\n", 
              inst.get_uid(), m_sid, inst.warp_id(), inst.pc, gpu_tot_sim_cycle + gpu_sim_cycle, inst.get_issue_cycle()); 
    #endif
-  if(inst.op_pipe==SP__OP)
+  if(inst.op4==SP__OP)
 	  m_stats->m_num_sp_committed[m_sid]++;
-  else if(inst.op_pipe==SFU__OP)
+  else if(inst.op4==SFU__OP)
 	  m_stats->m_num_sfu_committed[m_sid]++;
-  else if(inst.op_pipe==MEM__OP)
+  else if(inst.op4==MEM__OP)
 	  m_stats->m_num_mem_committed[m_sid]++;
 
   if(m_config->gpgpu_clock_gated_lanes==false)
@@ -1236,7 +1242,7 @@
 
     warp_inst_t** preg = m_pipeline_reg[EX_WB].get_ready();
     warp_inst_t* pipe_reg = (preg==NULL)? NULL:*preg;
-    while( preg and !pipe_reg->empty()) {
+    while( preg and !pipe_reg->empty() ) {
     	/*
     	 * Right now, the writeback stage drains all waiting instructions
     	 * assuming there are enough ports in the register file or the
@@ -1253,7 +1259,6 @@
     	 * To handle this case, we ignore the return value (thus allowing
     	 * no stalling).
     	 */
-
         m_operand_collector.writeback(*pipe_reg);
         unsigned warp_id = pipe_reg->warp_id();
         m_scoreboard->releaseRegisters( pipe_reg );
@@ -1308,6 +1313,12 @@
                 if (inst.out[r] > 0)
                     m_pending_writes[inst.warp_id()][inst.out[r]]--; 
         }
+
+        // Saugata: these aren't being tallied in writeback(), so just call the warp return update function to record L1 divergence...
+        // TODO: do we need to clear out stats if the warp completes here?  it should hopefully not matter, if at least one L2 request is made, since that will take place afterwards and invoke the stat save...
+        // TODO: what happens to writebacks?
+        add_warp_return_time(inst.warp_id(), mf->get_service_level(), 0);
+
         if( !write_sent ) 
             delete mf;
     } else if ( status == RESERVATION_FAIL ) {
@@ -1331,9 +1342,6 @@
     if( inst.accessq_empty() )
         return result;
 
-    if( !cache->data_port_free() ) 
-        return DATA_PORT_STALL; 
-
     //const mem_access_t &access = inst.accessq_back();
     mem_fetch *mf = m_mf_allocator->alloc(inst,inst.accessq_back());
     std::list<cache_event> events;
@@ -1384,25 +1392,17 @@
    assert( !inst.accessq_empty() );
    mem_stage_stall_type stall_cond = NO_RC_FAIL;
    const mem_access_t &access = inst.accessq_back();
+   unsigned size = access.get_size(); 
 
-   bool bypassL1D = false; 
-   if ( CACHE_GLOBAL == inst.cache_op || (m_L1D == NULL) ) {
-       bypassL1D = true; 
-   } else if (inst.space.is_global()) { // global memory access 
-       // skip L1 cache if the option is enabled
-       if (m_core->get_config()->gmem_skip_L1D) 
-           bypassL1D = true; 
-   }
-
-   if( bypassL1D ) {
+   // TODO: Rachata --> tag thread info to mf
+   if( CACHE_GLOBAL == inst.cache_op || (m_L1D == NULL) ) {
        // bypass L1 cache
-       unsigned control_size = inst.is_store() ? WRITE_PACKET_SIZE : READ_PACKET_SIZE;
-       unsigned size = access.get_size() + control_size;
        if( m_icnt->full(size, inst.is_store() || inst.isatomic()) ) {
            stall_cond = ICNT_RC_FAIL;
        } else {
            mem_fetch *mf = m_mf_allocator->alloc(inst,access);
            m_icnt->push(mf);
+           n_simt_to_mem+=mf->get_num_flits(true); // Interconnect power stats (# of flits sent to the memory partitions)
            inst.accessq_pop_back();
            //inst.clear_active( access.get_warp_mask() );
            if( inst.is_load() ) { 
@@ -1446,6 +1446,120 @@
 	m_L1D->flush();
 }
 
+// Saugata: added functions to record stats on warp divergence in the memory partition
+void ldst_unit::init_warp_return_time(unsigned warp_id)
+{
+    if(!m_pending_writes_first_L2_return_time.count(warp_id)) {
+        m_pending_writes_first_L2_return_time[warp_id] = 0;
+        m_pending_writes_last_L2_return_time[warp_id] = 0;
+        m_pending_writes_first_DRAM_return_time[warp_id] = 0;
+        m_pending_writes_last_DRAM_return_time[warp_id] = 0;
+        m_pending_writes_num_DRAM_requests[warp_id] = 0;
+    }
+}
+
+void ldst_unit::add_warp_return_time(unsigned warp_id, unsigned service_level, unsigned service_time)
+{
+    switch(service_level) {
+        case 1:
+            m_pending_writes_some_hit_in_L1.insert(warp_id);
+            break;
+        case 2:
+            m_pending_writes_some_hit_in_MP.insert(warp_id);
+
+            if(!m_pending_writes_first_L2_return_time[warp_id] || service_time < m_pending_writes_first_L2_return_time[warp_id]) {
+                m_pending_writes_first_L2_return_time[warp_id] = service_time;
+            }
+
+            if(service_time > m_pending_writes_last_L2_return_time[warp_id]) {
+                m_pending_writes_last_L2_return_time[warp_id] = service_time;
+            }
+            break;
+        case 3:
+            m_pending_writes_some_hit_in_MP.insert(warp_id);
+            ++m_pending_writes_num_DRAM_requests[warp_id];
+
+            if(!m_pending_writes_first_DRAM_return_time[warp_id] || service_time < m_pending_writes_first_DRAM_return_time[warp_id]) {
+                m_pending_writes_first_DRAM_return_time[warp_id] = service_time;
+            }
+
+            if(service_time > m_pending_writes_last_DRAM_return_time[warp_id]) {
+                m_pending_writes_last_DRAM_return_time[warp_id] = service_time;
+            }
+            break;
+        default:
+            assert(false && "Unknown memory request service type...");
+            break;
+    }
+}
+
+void ldst_unit::save_warp_return_time_stats(unsigned warp_id)
+{
+    unsigned num_DRAM_requests = m_pending_writes_num_DRAM_requests[warp_id];
+
+    // verify that the structures are initialized before we use them
+    if(m_pending_writes_first_L2_return_time.count(warp_id)) {
+        // make sure at least one request went down to the memory partition
+        if(m_pending_writes_some_hit_in_MP.count(warp_id)) {
+            unsigned earliest_return = m_pending_writes_first_L2_return_time[warp_id];
+            unsigned latest_return = m_pending_writes_last_L2_return_time[warp_id];
+
+            // check if at least one request went to DRAM
+            if(num_DRAM_requests) {
+                if(!earliest_return || m_pending_writes_first_DRAM_return_time[warp_id] < earliest_return) {
+                    earliest_return = m_pending_writes_first_DRAM_return_time[warp_id];
+                }
+
+                if(m_pending_writes_last_DRAM_return_time[warp_id] > latest_return) {
+                    latest_return = m_pending_writes_last_DRAM_return_time[warp_id];
+                }
+            }
+
+            // for warps where at least one request hit in the L1, count them separately
+            if(m_pending_writes_some_hit_in_L1.count(warp_id)) {
+                ++ldst_unit::num_warps_some_to_icnt_by_DRAM_count[num_DRAM_requests];
+
+                ldst_unit::total_divergence_some_to_icnt_by_DRAM_count[num_DRAM_requests] += (latest_return - earliest_return);
+                ldst_unit::total_L2_divergence_some_to_icnt_by_DRAM_count[num_DRAM_requests] += 
+                    (m_pending_writes_last_L2_return_time[warp_id] - m_pending_writes_first_L2_return_time[warp_id]);
+                ldst_unit::total_DRAM_divergence_some_to_icnt_by_DRAM_count[num_DRAM_requests] += 
+                    (m_pending_writes_last_DRAM_return_time[warp_id] - m_pending_writes_first_DRAM_return_time[warp_id]);
+            }
+            else {
+                ++ldst_unit::num_warps_all_to_icnt_by_DRAM_count[num_DRAM_requests];
+
+                ldst_unit::total_divergence_all_to_icnt_by_DRAM_count[num_DRAM_requests] += (latest_return - earliest_return);
+                ldst_unit::total_L2_divergence_all_to_icnt_by_DRAM_count[num_DRAM_requests] += 
+                    (m_pending_writes_last_L2_return_time[warp_id] - m_pending_writes_first_L2_return_time[warp_id]);
+                ldst_unit::total_DRAM_divergence_all_to_icnt_by_DRAM_count[num_DRAM_requests] += 
+                    (m_pending_writes_last_DRAM_return_time[warp_id] - m_pending_writes_first_DRAM_return_time[warp_id]);
+            }
+        }
+    }
+
+    clear_warp_return_time_stats(warp_id);
+}
+
+void ldst_unit::clear_warp_return_time_stats(unsigned warp_id)
+{
+    // verify that the structures are initialized before we clear them
+    if(m_pending_writes_first_L2_return_time.count(warp_id)) {
+        m_pending_writes_first_L2_return_time.erase(warp_id);
+        m_pending_writes_last_L2_return_time.erase(warp_id);
+        m_pending_writes_first_DRAM_return_time.erase(warp_id);
+        m_pending_writes_last_DRAM_return_time.erase(warp_id);
+        m_pending_writes_num_DRAM_requests.erase(warp_id);
+    }
+
+    if(m_pending_writes_some_hit_in_MP.count(warp_id)) {
+        m_pending_writes_some_hit_in_MP.erase(warp_id);
+    }
+
+    if(m_pending_writes_some_hit_in_L1.count(warp_id)) {
+        m_pending_writes_some_hit_in_L1.erase(warp_id);
+    }
+}
+
 simd_function_unit::simd_function_unit( const shader_core_config *config )
 { 
     m_config=config;
@@ -1464,7 +1578,7 @@
     warp_inst_t** ready_reg = source_reg.get_ready();
 	//m_core->incexecstat((*ready_reg));
 
-	(*ready_reg)->op_pipe=SFU__OP;
+	(*ready_reg)->op4=SFU__OP;
 	m_core->incsfu_stat(m_core->get_config()->warp_size,(*ready_reg)->latency);
 	pipelined_simd_unit::issue(source_reg);
 }
@@ -1500,7 +1614,7 @@
 {
     warp_inst_t** ready_reg = source_reg.get_ready();
 	//m_core->incexecstat((*ready_reg));
-	(*ready_reg)->op_pipe=SP__OP;
+	(*ready_reg)->op4=SP__OP;
 	m_core->incsp_stat(m_core->get_config()->warp_size,(*ready_reg)->latency);
 	pipelined_simd_unit::issue(source_reg);
 }
@@ -1517,22 +1631,6 @@
     m_core=core;
 }
 
-void pipelined_simd_unit::cycle()
-{
-    if( !m_pipeline_reg[0]->empty() ){
-        m_result_port->move_in(m_pipeline_reg[0]);
-    }
-    for( unsigned stage=0; (stage+1)<m_pipeline_depth; stage++ )
-        move_warp(m_pipeline_reg[stage], m_pipeline_reg[stage+1]);
-    if( !m_dispatch_reg->empty() ) {
-        if( !m_dispatch_reg->dispatch_delay()){
-            int start_stage = m_dispatch_reg->latency - m_dispatch_reg->initiation_interval;
-            move_warp(m_pipeline_reg[start_stage],m_dispatch_reg);
-        }
-    }
-    occupied >>=1;
-}
-
 
 void pipelined_simd_unit::issue( register_set& source_reg )
 {
@@ -1583,6 +1681,7 @@
     m_mem_rc = NO_RC_FAIL;
     m_num_writeback_clients=5; // = shared memory, global/local (uncached), L1D, L1T, L1C
     m_writeback_arb = 0;
+    n_simt_to_mem = 0;
     m_next_global=NULL;
     m_last_inst_gpu_sim_cycle=0;
     m_last_inst_gpu_tot_sim_cycle=0;
@@ -1651,6 +1750,8 @@
 void ldst_unit:: issue( register_set &reg_set )
 {
 	warp_inst_t* inst = *(reg_set.get_ready());
+   // stat collection
+   m_core->mem_instruction_stats(*inst);
 
    // record how many pending register writes/memory accesses there are for this instruction
    assert(inst->empty() == false);
@@ -1661,23 +1762,29 @@
          unsigned reg_id = inst->out[r];
          if (reg_id > 0) {
             m_pending_writes[warp_id][reg_id] += n_accesses;
+
+            // Saugata: initialize warp tallying structures
+            init_warp_return_time(warp_id);
          }
       }
    }
 
 
-	inst->op_pipe=MEM__OP;
-	// stat collection
+	inst->op4=MEM__OP;
 	m_core->mem_instruction_stats(*inst);
 	m_core->incmem_stat(m_core->get_config()->warp_size,1);
 	pipelined_simd_unit::issue(reg_set);
 }
 
+// SAUGATA TODO: this is where they all come back, and are tracked by warp
 void ldst_unit::writeback()
 {
     // process next instruction that is going to writeback
     if( !m_next_wb.empty() ) {
         if( m_operand_collector->writeback(m_next_wb) ) {
+            // Saugata: on writeback, tally the stats
+            add_warp_return_time(m_next_wb.warp_id(), m_next_wb_service_level, m_next_wb_service_time);
+
             bool insn_completed = false; 
             for( unsigned r=0; r < 4; r++ ) {
                 if( m_next_wb.out[r] > 0 ) {
@@ -1697,6 +1804,9 @@
             }
             if( insn_completed ) {
                 m_core->warp_inst_complete(m_next_wb);
+
+                // Saugata: collect warp stats into global tallying structures
+                save_warp_return_time_stats(m_next_wb.warp_id());
             }
             m_next_wb.clear();
             m_last_inst_gpu_sim_cycle = gpu_sim_cycle;
@@ -1718,12 +1828,19 @@
                 m_core->dec_inst_in_pipeline(m_pipeline_reg[0]->warp_id());
                 m_pipeline_reg[0]->clear();
                 serviced_client = next_client; 
+
+                // Saugata: no associated mem_fetch object, so ignore latency? (TODO: check this)
+                m_next_wb_service_level = 1;
+                m_next_wb_service_time = 0;
             }
             break;
         case 1: // texture response
             if( m_L1T->access_ready() ) {
                 mem_fetch *mf = m_L1T->next_access();
                 m_next_wb = mf->get_inst();
+                // Saugata: save service level and time for use in writeback() function
+                m_next_wb_service_level = mf->get_service_level();
+                m_next_wb_service_time = mf->get_service_time();
                 delete mf;
                 serviced_client = next_client; 
             }
@@ -1732,6 +1849,9 @@
             if( m_L1C->access_ready() ) {
                 mem_fetch *mf = m_L1C->next_access();
                 m_next_wb = mf->get_inst();
+                // Saugata: save service level and time for use in writeback() function
+                m_next_wb_service_level = mf->get_service_level();
+                m_next_wb_service_time = mf->get_service_time();
                 delete mf;
                 serviced_client = next_client; 
             }
@@ -1744,12 +1864,19 @@
                 delete m_next_global;
                 m_next_global = NULL;
                 serviced_client = next_client; 
+
+                // Saugata: no associated mem_fetch object, so ignore latency? (TODO: check this)
+                m_next_wb_service_level = 1;
+                m_next_wb_service_time = 0;
             }
             break;
         case 4: 
             if( m_L1D && m_L1D->access_ready() ) {
                 mem_fetch *mf = m_L1D->next_access();
                 m_next_wb = mf->get_inst();
+                // Saugata: save service level and time for use in writeback() function
+                m_next_wb_service_level = mf->get_service_level();
+                m_next_wb_service_time = mf->get_service_time();
                 delete mf;
                 serviced_client = next_client; 
             }
@@ -1803,16 +1930,12 @@
    if( !m_response_fifo.empty() ) {
        mem_fetch *mf = m_response_fifo.front();
        if (mf->istexture()) {
-           if (m_L1T->fill_port_free()) {
-               m_L1T->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
-               m_response_fifo.pop_front(); 
-           }
+           m_L1T->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
+           m_response_fifo.pop_front(); 
        } else if (mf->isconst())  {
-           if (m_L1C->fill_port_free()) {
-               mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
-               m_L1C->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
-               m_response_fifo.pop_front(); 
-           }
+           mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
+           m_L1C->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
+           m_response_fifo.pop_front(); 
        } else {
     	   if( mf->get_type() == WRITE_ACK || ( m_config->gpgpu_perfect_mem && mf->get_is_write() )) {
                m_core->store_ack(mf);
@@ -1820,25 +1943,13 @@
                delete mf;
            } else {
                assert( !mf->get_is_write() ); // L1 cache is write evict, allocate line on load miss only
-
-               bool bypassL1D = false; 
-               if ( CACHE_GLOBAL == mf->get_inst().cache_op || (m_L1D == NULL) ) {
-                   bypassL1D = true; 
-               } else if (mf->get_access_type() == GLOBAL_ACC_R || mf->get_access_type() == GLOBAL_ACC_W) { // global memory access 
-                   if (m_core->get_config()->gmem_skip_L1D)
-                       bypassL1D = true; 
-               }
-               if( bypassL1D ) {
-                   if ( m_next_global == NULL ) {
-                       mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
-                       m_response_fifo.pop_front();
-                       m_next_global = mf;
-                   }
-               } else {
-                   if (m_L1D->fill_port_free()) {
-                       m_L1D->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
-                       m_response_fifo.pop_front();
-                   }
+               if( mf->get_inst().cache_op != CACHE_GLOBAL && m_L1D ) {
+                   m_L1D->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
+                   m_response_fifo.pop_front();
+               } else if( m_next_global == NULL ) {
+                   mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
+                   m_response_fifo.pop_front();
+                   m_next_global = mf;
                }
            }
        }
@@ -1857,6 +1968,7 @@
    done &= texture_cycle(pipe_reg, rc_fail, type);
    done &= memory_cycle(pipe_reg, rc_fail, type);
    m_mem_rc = rc_fail;
+   set_stats(); // Sets stats in m_stats object
 
    if (!done) { // log stall types and return
       assert(rc_fail != NO_RC_FAIL);
@@ -1898,6 +2010,9 @@
                if( !pending_requests ) {
                    m_core->warp_inst_complete(*m_dispatch_reg);
                    m_scoreboard->releaseRegisters(m_dispatch_reg);
+
+                   // Saugata: collect warp stats into global tallying structures
+                   save_warp_return_time_stats(warp_id);
                }
                m_core->dec_inst_in_pipeline(warp_id);
                m_dispatch_reg->clear();
@@ -1907,6 +2022,9 @@
            m_core->dec_inst_in_pipeline(warp_id);
            m_core->warp_inst_complete(*m_dispatch_reg);
            m_dispatch_reg->clear();
+
+           // Saugata: instead of collecting warp stats, just clear them for stores, since they do not stall the warps
+           clear_warp_return_time_stats(warp_id);
        }
    }
 }
@@ -2001,89 +2119,6 @@
     fprintf( fout, "\n" );
 }
 
-void gpgpu_sim::shader_print_cache_stats( FILE *fout ) const{
-
-    // L1I
-    struct cache_sub_stats total_css;
-    struct cache_sub_stats css;
-
-    if(!m_shader_config->m_L1I_config.disabled()){
-        total_css.clear();
-        css.clear();
-        fprintf(fout, "\n========= Core cache stats =========\n");
-        fprintf(fout, "L1I_cache:\n");
-        for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
-            m_cluster[i]->get_L1I_sub_stats(css);
-            total_css += css;
-        }
-        fprintf(fout, "\tL1I_total_cache_accesses = %u\n", total_css.accesses);
-        fprintf(fout, "\tL1I_total_cache_misses = %u\n", total_css.misses);
-        if(total_css.accesses > 0){
-            fprintf(fout, "\tL1I_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
-        }
-        fprintf(fout, "\tL1I_total_cache_pending_hits = %u\n", total_css.pending_hits);
-        fprintf(fout, "\tL1I_total_cache_reservation_fails = %u\n", total_css.res_fails);
-    }
-
-    // L1D
-    if(!m_shader_config->m_L1D_config.disabled()){
-        total_css.clear();
-        css.clear();
-        fprintf(fout, "L1D_cache:\n");
-        for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++){
-            m_cluster[i]->get_L1D_sub_stats(css);
-
-            fprintf( stdout, "\tL1D_cache_core[%d]: Access = %d, Miss = %d, Miss_rate = %.3lf, Pending_hits = %u, Reservation_fails = %u\n",
-                     i, css.accesses, css.misses, (double)css.misses / (double)css.accesses, css.pending_hits, css.res_fails);
-
-            total_css += css;
-        }
-        fprintf(fout, "\tL1D_total_cache_accesses = %u\n", total_css.accesses);
-        fprintf(fout, "\tL1D_total_cache_misses = %u\n", total_css.misses);
-        if(total_css.accesses > 0){
-            fprintf(fout, "\tL1D_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
-        }
-        fprintf(fout, "\tL1D_total_cache_pending_hits = %u\n", total_css.pending_hits);
-        fprintf(fout, "\tL1D_total_cache_reservation_fails = %u\n", total_css.res_fails);
-        total_css.print_port_stats(fout, "\tL1D_cache"); 
-    }
-
-    // L1C
-    if(!m_shader_config->m_L1C_config.disabled()){
-        total_css.clear();
-        css.clear();
-        fprintf(fout, "L1C_cache:\n");
-        for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
-            m_cluster[i]->get_L1C_sub_stats(css);
-            total_css += css;
-        }
-        fprintf(fout, "\tL1C_total_cache_accesses = %u\n", total_css.accesses);
-        fprintf(fout, "\tL1C_total_cache_misses = %u\n", total_css.misses);
-        if(total_css.accesses > 0){
-            fprintf(fout, "\tL1C_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
-        }
-        fprintf(fout, "\tL1C_total_cache_pending_hits = %u\n", total_css.pending_hits);
-        fprintf(fout, "\tL1C_total_cache_reservation_fails = %u\n", total_css.res_fails);
-    }
-
-    // L1T
-    if(!m_shader_config->m_L1T_config.disabled()){
-        total_css.clear();
-        css.clear();
-        fprintf(fout, "L1T_cache:\n");
-        for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
-            m_cluster[i]->get_L1T_sub_stats(css);
-            total_css += css;
-        }
-        fprintf(fout, "\tL1T_total_cache_accesses = %u\n", total_css.accesses);
-        fprintf(fout, "\tL1T_total_cache_misses = %u\n", total_css.misses);
-        if(total_css.accesses > 0){
-            fprintf(fout, "\tL1T_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
-        }
-        fprintf(fout, "\tL1T_total_cache_pending_hits = %u\n", total_css.pending_hits);
-        fprintf(fout, "\tL1T_total_cache_reservation_fails = %u\n", total_css.res_fails);
-    }
-}
 
 void gpgpu_sim::shader_print_l1_miss_stat( FILE *fout ) const
 {
@@ -2149,6 +2184,67 @@
    */
 }
 
+// Saugata: added function to print divergence stats from main simulator
+void gpgpu_sim::shader_print_divergence_stat( FILE *fout ) const
+{
+    unsigned num_warps = 0;
+    unsigned long long total_divergence = 0;
+    unsigned long long L2_divergence = 0;
+    unsigned long long DRAM_divergence = 0;
+
+    fprintf(fout, "*** Divergence Statistics ***\n");
+    fprintf(fout, "--> Request Divergence for Warps Where All Mem Requests Went to Memory Partition\n");
+
+    fprintf(fout, "--> (in cycles, sorted by # of requests to DRAM)\n");
+
+    for(std::map<unsigned, unsigned>::iterator it = ldst_unit::num_warps_all_to_icnt_by_DRAM_count.begin();
+            it != ldst_unit::num_warps_all_to_icnt_by_DRAM_count.end(); ++it)
+    {
+        fprintf(fout, "    * %u  CNT: %u\n", it->first, it->second);
+        fprintf(fout, "      %u  TTL: %.2f\n", it->first, float(ldst_unit::total_divergence_all_to_icnt_by_DRAM_count[it->first]) / it->second);
+        fprintf(fout, "      %u   L2: %.2f\n", it->first, float(ldst_unit::total_L2_divergence_all_to_icnt_by_DRAM_count[it->first]) / it->second);
+        fprintf(fout, "      %u DRAM: %.2f\n", it->first, float(ldst_unit::total_DRAM_divergence_all_to_icnt_by_DRAM_count[it->first]) / it->second);
+
+        num_warps += it->second;
+        total_divergence += ldst_unit::total_divergence_all_to_icnt_by_DRAM_count[it->first];
+        L2_divergence += ldst_unit::total_L2_divergence_all_to_icnt_by_DRAM_count[it->first];
+        DRAM_divergence += ldst_unit::total_DRAM_divergence_all_to_icnt_by_DRAM_count[it->first];
+    }
+
+    fprintf(fout, "    * ALL  CNT: %u\n", num_warps);
+    fprintf(fout, "      ALL  TTL: %.2f\n", float(total_divergence) / num_warps);
+    fprintf(fout, "      ALL   L2: %.2f\n", float(L2_divergence) / num_warps);
+    fprintf(fout, "      ALL DRAM: %.2f\n", float(DRAM_divergence) / num_warps);
+
+    num_warps = 0;
+    total_divergence = 0;
+    L2_divergence = 0;
+    DRAM_divergence = 0;
+
+    fprintf(fout, "--> Request Divergence for Warps Where Only SOME Mem Requests Went to Memory Partition\n");
+
+    fprintf(fout, "--> (in cycles, sorted by # of requests to DRAM)\n");
+
+    for(std::map<unsigned, unsigned>::iterator it = ldst_unit::num_warps_some_to_icnt_by_DRAM_count.begin();
+            it != ldst_unit::num_warps_some_to_icnt_by_DRAM_count.end(); ++it)
+    {
+        fprintf(fout, "    * %u  CNT: %u\n", it->first, it->second);
+        fprintf(fout, "      %u  TTL: %.2f\n", it->first, float(ldst_unit::total_divergence_some_to_icnt_by_DRAM_count[it->first]) / it->second);
+        fprintf(fout, "      %u   L2: %.2f\n", it->first, float(ldst_unit::total_L2_divergence_some_to_icnt_by_DRAM_count[it->first]) / it->second);
+        fprintf(fout, "      %u DRAM: %.2f\n", it->first, float(ldst_unit::total_DRAM_divergence_some_to_icnt_by_DRAM_count[it->first]) / it->second);
+
+        num_warps += it->second;
+        total_divergence += ldst_unit::total_divergence_some_to_icnt_by_DRAM_count[it->first];
+        L2_divergence += ldst_unit::total_L2_divergence_some_to_icnt_by_DRAM_count[it->first];
+        DRAM_divergence += ldst_unit::total_DRAM_divergence_some_to_icnt_by_DRAM_count[it->first];
+    }
+
+    fprintf(fout, "    * ALL  CNT: %u\n", num_warps);
+    fprintf(fout, "      ALL  TTL: %.2f\n", float(total_divergence) / num_warps);
+    fprintf(fout, "      ALL   L2: %.2f\n", float(L2_divergence) / num_warps);
+    fprintf(fout, "      ALL DRAM: %.2f\n", float(DRAM_divergence) / num_warps);
+}
+
 void warp_inst_t::print( FILE *fout ) const
 {
     if (empty() ) {
@@ -2165,19 +2261,18 @@
 }
 void shader_core_ctx::incexecstat(warp_inst_t *&inst)
 {
-	if(inst->mem_op==TEX)
+	if(inst->op5==TEX)
 		inctex_stat(inst->active_count(),1);
 
-    // Latency numbers for next operations are used to scale the power values
-    // for special operations, according observations from microbenchmarking
-    // TODO: put these numbers in the xml configuration
-
-	switch(inst->sp_op){
+	switch(inst->op3){
 	case INT__OP:
 		incialu_stat(inst->active_count(),25);
 		break;
 	case INT_MUL_OP:
-		incimul_stat(inst->active_count(),7.2);
+		if(m_config->gpgpu_shader_registers==32768) //i.e. FERMI
+			incimul_stat(inst->active_count(),7.2);
+		else
+			incimul_stat(inst->active_count(),16);
 		break;
 	case INT_MUL24_OP:
 		incimul24_stat(inst->active_count(),4.2);
@@ -2189,26 +2284,52 @@
 		incidiv_stat(inst->active_count(),40);
 		break;
 	case FP__OP:
+		if(m_config->gpgpu_shader_registers==32768)
 		incfpalu_stat(inst->active_count(),1);
+		else
+		incfpalu_stat(inst->active_count(),1.7);
 		break;
 	case FP_MUL_OP:
+		if(m_config->gpgpu_shader_registers==32768)
+		incfpmul_stat(inst->active_count(),1.8);
+		else
 		incfpmul_stat(inst->active_count(),1.8);
 		break;
 	case FP_DIV_OP:
+		if(m_config->gpgpu_shader_registers==32768)
 		incfpdiv_stat(inst->active_count(),48);
+		else 
+		incfpdiv_stat(inst->active_count(),22);
 		break;
 	case FP_SQRT_OP:
+		if(m_config->gpgpu_shader_registers==32768)
 		inctrans_stat(inst->active_count(),25);
+		else
+		inctrans_stat(inst->active_count(),8);
+
 		break;
 	case FP_LG_OP:
+		if (m_config->gpgpu_shader_registers==32768)
 		inctrans_stat(inst->active_count(),35);
+		else
+		inctrans_stat(inst->active_count(),0.3);
 		break;
 	case FP_SIN_OP:
+		if(m_config->gpgpu_shader_registers==32768)
 		inctrans_stat(inst->active_count(),12);
+		else 
+		inctrans_stat(inst->active_count(),40);
+
 		break;
 	case FP_EXP_OP:
+		if(m_config->gpgpu_shader_registers==32768)
 		inctrans_stat(inst->active_count(),35);
+		else 
+		inctrans_stat(inst->active_count(),9);
+
 		break;
+
+
 	default:
 		break;
 	}
@@ -2539,28 +2660,17 @@
    return result;
 }
 
-barrier_set_t::barrier_set_t(shader_core_ctx *shader,unsigned max_warps_per_core, unsigned max_cta_per_core, unsigned max_barriers_per_cta, unsigned warp_size)
+barrier_set_t::barrier_set_t( unsigned max_warps_per_core, unsigned max_cta_per_core )
 {
    m_max_warps_per_core = max_warps_per_core;
    m_max_cta_per_core = max_cta_per_core;
-   m_max_barriers_per_cta = max_barriers_per_cta;
-   m_warp_size = warp_size;
-   m_shader = shader;
    if( max_warps_per_core > WARP_PER_CTA_MAX ) {
       printf("ERROR ** increase WARP_PER_CTA_MAX in shader.h from %u to >= %u or warps per cta in gpgpusim.config\n",
              WARP_PER_CTA_MAX, max_warps_per_core );
       exit(1);
    }
-   if(max_barriers_per_cta > MAX_BARRIERS_PER_CTA){
-	   printf("ERROR ** increase MAX_BARRIERS_PER_CTA in abstract_hardware_model.h from %u to >= %u or barriers per cta in gpgpusim.config\n",
-			   MAX_BARRIERS_PER_CTA, max_barriers_per_cta );
-	   exit(1);
-   }
    m_warp_active.reset();
    m_warp_at_barrier.reset();
-   for(unsigned i=0; i<max_barriers_per_cta; i++){
-	   m_bar_id_to_warps[i].reset();
-   }
 }
 
 // during cta allocation
@@ -2574,10 +2684,6 @@
   
    m_warp_active |= warps;
    m_warp_at_barrier &= ~warps;
-   for(unsigned i=0; i<m_max_barriers_per_cta; i++){
-	   m_bar_id_to_warps[i] &=~warps;
-   }
-
 }
 
 // during cta deallocation
@@ -2593,22 +2699,12 @@
    assert( active.any() == false ); // no warps in CTA still running
    m_warp_active &= ~warps;
    m_warp_at_barrier &= ~warps;
-
-   for(unsigned i=0; i<m_max_barriers_per_cta; i++){
-	   warp_set_t at_a_specific_barrier = warps & m_bar_id_to_warps[i];
-	   assert( at_a_specific_barrier.any() == false ); // no warps stuck at barrier
-	   m_bar_id_to_warps[i] &=~warps;
-   }
    m_cta_to_warps.erase(w);
 }
 
 // individual warp hits barrier
-void barrier_set_t::warp_reaches_barrier(unsigned cta_id,unsigned warp_id,warp_inst_t* inst)
+void barrier_set_t::warp_reaches_barrier( unsigned cta_id, unsigned warp_id )
 {
-	barrier_type bar_type = inst->bar_type;
-	unsigned bar_id = inst->bar_id;
-	unsigned bar_count = inst->bar_count;
-	assert(bar_id!=(unsigned)-1);
    cta_to_warp_t::iterator w=m_cta_to_warps.find(cta_id);
 
    if( w == m_cta_to_warps.end() ) { // cta is active
@@ -2618,37 +2714,23 @@
    }
    assert( w->second.test(warp_id) == true ); // warp is in cta
 
-   m_bar_id_to_warps[bar_id].set(warp_id);
-   if(bar_type==SYNC || bar_type==RED){
-	   m_warp_at_barrier.set(warp_id);
-   }
+   m_warp_at_barrier.set(warp_id);
+
    warp_set_t warps_in_cta = w->second;
-   warp_set_t at_barrier = warps_in_cta & m_bar_id_to_warps[bar_id];
+   warp_set_t at_barrier = warps_in_cta & m_warp_at_barrier;
    warp_set_t active = warps_in_cta & m_warp_active;
-   if(bar_count==(unsigned)-1){
-	   if( at_barrier == active ) {
-		   // all warps have reached barrier, so release waiting warps...
-		   m_bar_id_to_warps[bar_id] &= ~at_barrier;
-		   m_warp_at_barrier &= ~at_barrier;
-		   if(bar_type==RED){
-			   m_shader->broadcast_barrier_reduction(cta_id, bar_id,at_barrier);
-		   }
-	   }
-  }else{
-	  // TODO: check on the hardware if the count should include warp that exited
-	  if ((at_barrier.count() * m_warp_size) == bar_count){
-		   // required number of warps have reached barrier, so release waiting warps...
-		   m_bar_id_to_warps[bar_id] &= ~at_barrier;
-		   m_warp_at_barrier &= ~at_barrier;
-		   if(bar_type==RED){
-			   m_shader->broadcast_barrier_reduction(cta_id, bar_id,at_barrier);
-		   }
-	  }
-  }
-
 
+   if( at_barrier == active ) {
+      // all warps have reached barrier, so release waiting warps...
+      m_warp_at_barrier &= ~at_barrier;
+   }
 }
 
+// fetching a warp
+bool barrier_set_t::available_for_fetch( unsigned warp_id ) const
+{
+   return m_warp_active.test(warp_id) && m_warp_at_barrier.test(warp_id);
+}
 
 // warp reaches exit 
 void barrier_set_t::warp_exit( unsigned warp_id )
@@ -2663,15 +2745,12 @@
       if (w->second.test(warp_id) == true) break; 
    }
    warp_set_t warps_in_cta = w->second;
+   warp_set_t at_barrier = warps_in_cta & m_warp_at_barrier;
    warp_set_t active = warps_in_cta & m_warp_active;
 
-   for(unsigned i=0; i<m_max_barriers_per_cta; i++){
-	   warp_set_t at_a_specific_barrier = warps_in_cta & m_bar_id_to_warps[i];
-	   if( at_a_specific_barrier == active ) {
-	      // all warps have reached barrier, so release waiting warps...
-		   m_bar_id_to_warps[i] &= ~at_a_specific_barrier;
-		   m_warp_at_barrier &= ~at_a_specific_barrier;
-	   }
+   if( at_barrier == active ) {
+      // all warps have reached barrier, so release waiting warps...
+      m_warp_at_barrier &= ~at_barrier;
    }
 }
 
@@ -2681,12 +2760,11 @@
    return m_warp_at_barrier.test(warp_id);
 }
 
-void barrier_set_t::dump()
+void barrier_set_t::dump() const
 {
    printf( "barrier set information\n");
    printf( "  m_max_cta_per_core = %u\n",  m_max_cta_per_core );
    printf( "  m_max_warps_per_core = %u\n", m_max_warps_per_core );
-   printf( " m_max_barriers_per_cta =%u\n", m_max_barriers_per_cta);
    printf( "  cta_to_warps:\n");
    
    cta_to_warp_t::const_iterator i;
@@ -2697,10 +2775,6 @@
    }
    printf("  warp_active: %s\n", m_warp_active.to_string().c_str() );
    printf("  warp_at_barrier: %s\n", m_warp_at_barrier.to_string().c_str() );
-   for( unsigned i=0; i<m_max_barriers_per_cta; i++){
-	   warp_set_t warps_reached_barrier = m_bar_id_to_warps[i];
-	   printf("  warp_at_barrier %u: %s\n", i, warps_reached_barrier.to_string().c_str() );
-   }
    fflush(stdout); 
 }
 
@@ -2724,18 +2798,6 @@
 		m_barriers.warp_exit( warp_id );
 }
 
-bool shader_core_ctx::check_if_non_released_reduction_barrier(warp_inst_t &inst)
-{
-	unsigned warp_id = inst.warp_id();
-	bool bar_red_op = (inst.op == BARRIER_OP) && (inst.bar_type == RED);
-    bool non_released_barrier_reduction = false;
-    bool warp_stucked_at_barrier = warp_waiting_at_barrier(warp_id);
-    bool single_inst_in_pipeline = (m_warp[warp_id].num_issued_inst_in_pipeline()==1);
-    non_released_barrier_reduction = single_inst_in_pipeline and warp_stucked_at_barrier and bar_red_op;
-    printf("non_released_barrier_reduction=%u\n",non_released_barrier_reduction);
-    return non_released_barrier_reduction;
-}
-
 bool shader_core_ctx::warp_waiting_at_barrier( unsigned warp_id ) const
 {
    return m_barriers.warp_waiting_at_barrier(warp_id);
@@ -2768,15 +2830,6 @@
    m_warp[wid].dec_n_atomic(n);
 }
 
-void shader_core_ctx::broadcast_barrier_reduction(unsigned cta_id,unsigned bar_id,warp_set_t warps)
-{
-	for(unsigned i=0; i<m_config->max_warps_per_shader;i++){
-		if(warps.test(i)){
-			const warp_inst_t * inst = m_warp[i].restore_info_of_last_inst_at_barrier();
-			const_cast<warp_inst_t *> (inst)->broadcast_barrier_reduction(inst->get_active_mask());
-		}
-	}
-}
 
 bool shader_core_ctx::fetch_unit_response_buffer_full() const
 {
@@ -2810,29 +2863,18 @@
    m_ldst_unit->print_cache_stats( fp, dl1_accesses, dl1_misses );
 }
 
-void shader_core_ctx::get_cache_stats(cache_stats &cs){
-    // Adds stats from each cache to 'cs'
-    cs += m_L1I->get_stats(); // Get L1I stats
-    m_ldst_unit->get_cache_stats(cs); // Get L1D, L1C, L1T stats
+void shader_core_ctx::get_cache_stats(unsigned &read_accesses, unsigned &write_accesses, unsigned &read_misses, unsigned &write_misses, unsigned cache_type) {
+   m_ldst_unit->get_cache_stats(read_accesses, write_accesses, read_misses, write_misses, cache_type);
 }
 
-void shader_core_ctx::get_L1I_sub_stats(struct cache_sub_stats &css) const{
-    if(m_L1I)
-        m_L1I->get_sub_stats(css);
-}
-void shader_core_ctx::get_L1D_sub_stats(struct cache_sub_stats &css) const{
-    m_ldst_unit->get_L1D_sub_stats(css);
-}
-void shader_core_ctx::get_L1C_sub_stats(struct cache_sub_stats &css) const{
-    m_ldst_unit->get_L1C_sub_stats(css);
-}
-void shader_core_ctx::get_L1T_sub_stats(struct cache_sub_stats &css) const{
-    m_ldst_unit->get_L1T_sub_stats(css);
-}
+void shader_core_ctx::set_icnt_power_stats(unsigned &n_simt_to_mem) const{
+	unsigned l1i=0;
+	if( m_L1I ){
+		m_L1I->set_icnt_power_stats(l1i);
+	}
+	m_ldst_unit->set_icnt_power_stats(n_simt_to_mem);
 
-void shader_core_ctx::get_icnt_power_stats(long &n_simt_to_mem, long &n_mem_to_simt) const{
-	n_simt_to_mem += m_stats->n_simt_to_mem[m_sid];
-	n_mem_to_simt += m_stats->n_mem_to_simt[m_sid];
+	n_simt_to_mem+=l1i; // l1i + l1d + l1c + l1t + any non-cached access
 }
 
 bool shd_warp_t::functional_done() const
@@ -3279,20 +3321,10 @@
     case INST_ACC_R: m_stats->gpgpu_n_mem_read_inst++; break;
     case L1_WRBK_ACC: m_stats->gpgpu_n_mem_write_global++; break;
     case L2_WRBK_ACC: m_stats->gpgpu_n_mem_l2_writeback++; break;
-    case L1_WR_ALLOC_R: m_stats->gpgpu_n_mem_l1_write_allocate++; break;
     case L2_WR_ALLOC_R: m_stats->gpgpu_n_mem_l2_write_allocate++; break;
     default: assert(0);
     }
-
-   // The packet size varies depending on the type of request: 
-   // - For write request and atomic request, the packet contains the data 
-   // - For read request (i.e. not write nor atomic), the packet only has control metadata
-   unsigned int packet_size = mf->size(); 
-   if (!mf->get_is_write() && !mf->isatomic()) {
-      packet_size = mf->get_ctrl_size(); 
-   }
-   m_stats->m_outgoing_traffic_stats->record_traffic(mf, packet_size); 
-   unsigned destination = mf->get_sub_partition_id();
+   unsigned destination = mf->get_tlx_addr().chip;
    mf->set_status(IN_ICNT_TO_MEM,gpu_sim_cycle+gpu_tot_sim_cycle);
    if (!mf->get_is_write() && !mf->isatomic())
       ::icnt_push(m_cluster_id, m_config->mem2device(destination), (void*)mf, mf->get_ctrl_size() );
@@ -3326,16 +3358,9 @@
             return;
         assert(mf->get_tpc() == m_cluster_id);
         assert(mf->get_type() == READ_REPLY || mf->get_type() == WRITE_ACK );
-
-        // The packet size varies depending on the type of request: 
-        // - For read request and atomic request, the packet contains the data 
-        // - For write-ack, the packet only has control metadata
-        unsigned int packet_size = (mf->get_is_write())? mf->get_ctrl_size() : mf->size(); 
-        m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size); 
         mf->set_status(IN_CLUSTER_TO_SHADER_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
         //m_memory_stats->memlatstat_read_done(mf,m_shader_config->max_warps_per_shader);
         m_response_fifo.push_back(mf);
-        m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);
     }
 }
 
@@ -3364,70 +3389,21 @@
    }
 }
 
-void simt_core_cluster::get_icnt_stats(long &n_simt_to_mem, long &n_mem_to_simt) const {
-	long simt_to_mem=0;
-	long mem_to_simt=0;
-	for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-		m_core[i]->get_icnt_power_stats(simt_to_mem, mem_to_simt);
-	}
-	n_simt_to_mem = simt_to_mem;
-	n_mem_to_simt = mem_to_simt;
-}
-
-void simt_core_cluster::get_cache_stats(cache_stats &cs) const{
-    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-        m_core[i]->get_cache_stats(cs);
-    }
+void simt_core_cluster::get_cache_stats(unsigned &read_accesses, unsigned &write_accesses, unsigned &read_misses, unsigned &write_misses, unsigned cache_type) const {
+   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
+      m_core[ i ]->get_cache_stats(read_accesses, write_accesses, read_misses, write_misses, cache_type);
+   }
 }
 
-void simt_core_cluster::get_L1I_sub_stats(struct cache_sub_stats &css) const{
-    struct cache_sub_stats temp_css;
-    struct cache_sub_stats total_css;
-    temp_css.clear();
-    total_css.clear();
-    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-        m_core[i]->get_L1I_sub_stats(temp_css);
-        total_css += temp_css;
-    }
-    css = total_css;
-}
-void simt_core_cluster::get_L1D_sub_stats(struct cache_sub_stats &css) const{
-    struct cache_sub_stats temp_css;
-    struct cache_sub_stats total_css;
-    temp_css.clear();
-    total_css.clear();
-    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-        m_core[i]->get_L1D_sub_stats(temp_css);
-        total_css += temp_css;
-    }
-    css = total_css;
-}
-void simt_core_cluster::get_L1C_sub_stats(struct cache_sub_stats &css) const{
-    struct cache_sub_stats temp_css;
-    struct cache_sub_stats total_css;
-    temp_css.clear();
-    total_css.clear();
-    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-        m_core[i]->get_L1C_sub_stats(temp_css);
-        total_css += temp_css;
-    }
-    css = total_css;
-}
-void simt_core_cluster::get_L1T_sub_stats(struct cache_sub_stats &css) const{
-    struct cache_sub_stats temp_css;
-    struct cache_sub_stats total_css;
-    temp_css.clear();
-    total_css.clear();
-    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
-        m_core[i]->get_L1T_sub_stats(temp_css);
-        total_css += temp_css;
-    }
-    css = total_css;
+void simt_core_cluster::set_icnt_stats(unsigned &n_simt_to_mem) const {
+	for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
+		m_core[i]->set_icnt_power_stats(n_simt_to_mem);
+	}
 }
 
 void shader_core_ctx::checkExecutionStatusAndUpdate(warp_inst_t &inst, unsigned t, unsigned tid)
 {
-    if(inst.isatomic())
+    if( inst.has_callback(t) ) 
            m_warp[inst.warp_id()].inc_n_atomic();
         if (inst.space.is_local() && (inst.is_load() || inst.is_store())) {
             new_addr_type localaddrs[MAX_ACCESSES_PER_INSN_PER_THREAD];
diff -Naur gpgpu-sim-baseline/shader.h gpgpu-sim/shader.h
--- gpgpu-sim-baseline/shader.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/shader.h	2015-10-13 06:38:50.602481858 -0400
@@ -52,7 +52,6 @@
 #include "mem_fetch.h"
 #include "stats.h"
 #include "gpu-cache.h"
-#include "traffic_breakdown.h"
 
 
 
@@ -70,6 +69,8 @@
 
 #define WRITE_MASK_SIZE 8
 
+//Set a hard limit of 32 CTAs per shader [cuda only has 8]
+#define MAX_CTA_PER_SHADER 32
 
 class thread_ctx_t {
 public:
@@ -107,7 +108,6 @@
         m_done_exit=true;
         m_last_fetch=0;
         m_next=0;
-        m_inst_at_barrier=NULL;
     }
     void init( address_type start_pc,
                unsigned cta_id,
@@ -156,9 +156,6 @@
     address_type get_pc() const { return m_next_pc; }
     void set_next_pc( address_type pc ) { m_next_pc = pc; }
 
-    void store_info_of_last_inst_at_barrier(const warp_inst_t *pI){ m_inst_at_barrier = pI;}
-    const warp_inst_t * restore_info_of_last_inst_at_barrier(){ return m_inst_at_barrier;}
-
     void ibuffer_fill( unsigned slot, const warp_inst_t *pI )
     {
        assert(slot < IBUFFER_SIZE );
@@ -203,17 +200,6 @@
         m_stores_outstanding--;
     }
 
-    unsigned num_inst_in_buffer() const
-    {
-    	unsigned count=0;
-        for(unsigned i=0;i<IBUFFER_SIZE;i++) {
-            if( m_ibuffer[i].m_valid )
-            	count++;
-        }
-    	return count;
-    }
-    unsigned num_inst_in_pipeline() const { return m_inst_in_pipeline;}
-    unsigned num_issued_inst_in_pipeline() const {return (num_inst_in_pipeline()-num_inst_in_buffer());}
     bool inst_in_pipeline() const { return m_inst_in_pipeline > 0; }
     void inc_inst_in_pipeline() { m_inst_in_pipeline++; }
     void dec_inst_in_pipeline() 
@@ -246,8 +232,6 @@
        const warp_inst_t *m_inst;
        bool m_valid;
     };
-
-    const warp_inst_t *m_inst_at_barrier;
     ibuffer_entry m_ibuffer[IBUFFER_SIZE]; 
     unsigned m_next;
                                    
@@ -294,7 +278,6 @@
     CONCRETE_SCHEDULER_LRR = 0,
     CONCRETE_SCHEDULER_GTO,
     CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE,
-    CONCRETE_SCHEDULER_WARP_LIMITING,
     NUM_CONCRETE_SCHEDULERS
 };
 
@@ -434,18 +417,14 @@
                           int id,
                           char* config_str )
 	: scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id ),
-	  m_pending_warps() 
+	  m_pending_warps()
     {
-        unsigned inner_level_readin;
-        unsigned outer_level_readin; 
         int ret = sscanf( config_str,
                           "two_level_active:%d:%d:%d",
                           &m_max_active_warps,
-                          &inner_level_readin,
-                          &outer_level_readin);
+                          (int*)&m_inner_level_prioritization,
+                          (int*)&m_outer_level_prioritization );
         assert( 3 == ret );
-        m_inner_level_prioritization=(scheduler_prioritization_type)inner_level_readin;
-        m_outer_level_prioritization=(scheduler_prioritization_type)outer_level_readin;
     }
 	virtual ~two_level_active_scheduler () {}
     virtual void order_warps();
@@ -466,34 +445,12 @@
                                     const std::vector< shd_warp_t* >::const_iterator& prioritized_iter );
 
 private:
-	std::deque< shd_warp_t* > m_pending_warps;
+	std::deque< shd_warp_t* > m_pending_warps; 
     scheduler_prioritization_type m_inner_level_prioritization;
     scheduler_prioritization_type m_outer_level_prioritization;
 	unsigned m_max_active_warps;
 };
 
-// Static Warp Limiting Scheduler
-class swl_scheduler : public scheduler_unit {
-public:
-	swl_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
-                    Scoreboard* scoreboard, simt_stack** simt,
-                    std::vector<shd_warp_t>* warp,
-                    register_set* sp_out,
-                    register_set* sfu_out,
-                    register_set* mem_out,
-                    int id,
-                    char* config_string );
-	virtual ~swl_scheduler () {}
-	virtual void order_warps ();
-    virtual void done_adding_supervised_warps() {
-        m_last_supervised_issued = m_supervised_warps.begin();
-    }
-
-protected:
-    scheduler_prioritization_type m_prioritization;
-    unsigned m_num_warps_to_limit;
-};
-
 
 
 class opndcoll_rfu_t { // operand collector based register file unit
@@ -599,10 +556,10 @@
           else if( m_cu ) return m_cu->get_active_mask();
           else abort();
       }
-      unsigned get_sp_op() const
+      unsigned get_op3() const
       {
-          if( m_warp ) return m_warp->sp_op;
-          else if( m_cu ) return m_cu->get_sp_op();
+          if( m_warp ) return m_warp->op3;
+          else if( m_cu ) return m_cu->get_op3();
           else abort();
       }
       unsigned get_oc_id() const { return m_cu->get_id(); }
@@ -797,7 +754,7 @@
       unsigned get_warp_id() const { return m_warp_id; }
       unsigned get_active_count() const { return m_warp->active_count(); }
       const active_mask_t & get_active_mask() const { return m_warp->get_active_mask(); }
-      unsigned get_sp_op() const { return m_warp->sp_op; }
+      unsigned get_op3() const { return m_warp->op3; }
       unsigned get_id() const { return m_cuid; } // returns CU hw id
 
       // modifiers
@@ -894,7 +851,7 @@
 
 class barrier_set_t {
 public:
-   barrier_set_t(shader_core_ctx * shader, unsigned max_warps_per_core, unsigned max_cta_per_core, unsigned max_barriers_per_cta, unsigned warp_size);
+   barrier_set_t( unsigned max_warps_per_core, unsigned max_cta_per_core );
 
    // during cta allocation
    void allocate_barrier( unsigned cta_id, warp_set_t warps );
@@ -903,12 +860,12 @@
    void deallocate_barrier( unsigned cta_id );
 
    typedef std::map<unsigned, warp_set_t >  cta_to_warp_t;
-   typedef std::map<unsigned, warp_set_t >  bar_id_to_warp_t; /*set of warps reached a specific barrier id*/
-
 
    // individual warp hits barrier
-   void warp_reaches_barrier( unsigned cta_id, unsigned warp_id, warp_inst_t* inst);
+   void warp_reaches_barrier( unsigned cta_id, unsigned warp_id );
 
+   // fetching a warp
+   bool available_for_fetch( unsigned warp_id ) const;
 
    // warp reaches exit 
    void warp_exit( unsigned warp_id );
@@ -917,19 +874,15 @@
    bool warp_waiting_at_barrier( unsigned warp_id ) const;
 
    // debug
-   void dump();
+   void dump() const;
 
 private:
    unsigned m_max_cta_per_core;
    unsigned m_max_warps_per_core;
-   unsigned m_max_barriers_per_cta;
-   unsigned m_warp_size;
-   cta_to_warp_t m_cta_to_warps;
-   bar_id_to_warp_t m_bar_id_to_warps;
+
+   cta_to_warp_t m_cta_to_warps; 
    warp_set_t m_warp_active;
    warp_set_t m_warp_at_barrier;
-   shader_core_ctx *m_shader;
-
 };
 
 struct insn_latency_info {
@@ -988,7 +941,21 @@
     pipelined_simd_unit( register_set* result_port, const shader_core_config *config, unsigned max_latency, shader_core_ctx *core );
 
     //modifiers
-    virtual void cycle();
+    virtual void cycle() 
+    {
+        if( !m_pipeline_reg[0]->empty() ){
+            m_result_port->move_in(m_pipeline_reg[0]);
+        }
+        for( unsigned stage=0; (stage+1)<m_pipeline_depth; stage++ ) 
+            move_warp(m_pipeline_reg[stage], m_pipeline_reg[stage+1]);
+        if( !m_dispatch_reg->empty() ) {
+            if( !m_dispatch_reg->dispatch_delay()) {
+                int start_stage = m_dispatch_reg->latency - m_dispatch_reg->initiation_interval;
+                move_warp(m_pipeline_reg[start_stage],m_dispatch_reg);
+            }
+        }
+        occupied >>=1;
+    }
     virtual void issue( register_set& source_reg );
     virtual unsigned get_active_lanes_in_pipeline()
     {
@@ -1113,11 +1080,20 @@
     void print(FILE *fout) const;
     void print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses );
     void get_cache_stats(unsigned &read_accesses, unsigned &write_accesses, unsigned &read_misses, unsigned &write_misses, unsigned cache_type);
-    void get_cache_stats(cache_stats &cs);
+    void set_stats();
 
-    void get_L1D_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1C_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1T_sub_stats(struct cache_sub_stats &css) const;
+    void set_icnt_power_stats(unsigned &simt_to_mem) const;
+
+    // Saugata: global information on all memory requests serviced across all LD/ST units
+    // TODO: not do the dirty thing, and actually read/modify these using accessors
+    static std::map<unsigned, unsigned> num_warps_all_to_icnt_by_DRAM_count;
+    static std::map<unsigned, unsigned long long> total_divergence_all_to_icnt_by_DRAM_count;
+    static std::map<unsigned, unsigned long long> total_L2_divergence_all_to_icnt_by_DRAM_count;
+    static std::map<unsigned, unsigned long long> total_DRAM_divergence_all_to_icnt_by_DRAM_count;
+    static std::map<unsigned, unsigned> num_warps_some_to_icnt_by_DRAM_count;
+    static std::map<unsigned, unsigned long long> total_divergence_some_to_icnt_by_DRAM_count;
+    static std::map<unsigned, unsigned long long> total_L2_divergence_some_to_icnt_by_DRAM_count;
+    static std::map<unsigned, unsigned long long> total_DRAM_divergence_some_to_icnt_by_DRAM_count;
 
 protected:
     ldst_unit( mem_fetch_interface *icnt,
@@ -1176,6 +1152,21 @@
    unsigned m_writeback_arb; // round-robin arbiter for writeback contention between L1T, L1C, shared
    unsigned m_num_writeback_clients;
 
+   // Saugata: information used to tally latencies of each request
+   unsigned m_next_wb_service_time;
+   unsigned m_next_wb_service_level; // did it come from the L1 (1), L2 (2), or DRAM (3)?
+   std::map<unsigned/*warp_id*/, unsigned> m_pending_writes_first_L2_return_time;
+   std::map<unsigned/*warp_id*/, unsigned> m_pending_writes_last_L2_return_time;
+   std::map<unsigned/*warp_id*/, unsigned> m_pending_writes_first_DRAM_return_time;
+   std::map<unsigned/*warp_id*/, unsigned> m_pending_writes_last_DRAM_return_time;
+   std::map<unsigned/*warp_id*/, unsigned> m_pending_writes_num_DRAM_requests;
+   std::set<unsigned/*warp_id*/> m_pending_writes_some_hit_in_L1;
+   std::set<unsigned/*warp_id*/> m_pending_writes_some_hit_in_MP;
+   void init_warp_return_time(unsigned warp_id);
+   void add_warp_return_time(unsigned warp_id, unsigned service_level, unsigned service_time);
+   void save_warp_return_time_stats(unsigned warp_id);
+   void clear_warp_return_time_stats(unsigned warp_id);
+
    enum mem_stage_stall_type m_mem_rc;
 
    shader_core_stats *m_stats; 
@@ -1183,6 +1174,9 @@
    // for debugging
    unsigned long long m_last_inst_gpu_sim_cycle;
    unsigned long long m_last_inst_gpu_tot_sim_cycle;
+
+   // Interconnect power stats
+   unsigned n_simt_to_mem;
 };
 
 enum pipeline_stage_name_t {
@@ -1245,10 +1239,10 @@
         assert( !(n_thread_per_shader % warp_size) );
         max_sfu_latency = 512;
         max_sp_latency = 32;
-        m_L1I_config.init(m_L1I_config.m_config_string,FuncCachePreferNone);
-        m_L1T_config.init(m_L1T_config.m_config_string,FuncCachePreferNone);
-        m_L1C_config.init(m_L1C_config.m_config_string,FuncCachePreferNone);
-        m_L1D_config.init(m_L1D_config.m_config_string,FuncCachePreferNone);
+        m_L1I_config.init();
+        m_L1T_config.init();
+        m_L1C_config.init();
+        m_L1D_config.init();
         gpgpu_cache_texl1_linesize = m_L1T_config.get_line_sz();
         gpgpu_cache_constl1_linesize = m_L1C_config.get_line_sz();
         m_valid = true;
@@ -1270,19 +1264,17 @@
     unsigned n_regfile_gating_group;
     unsigned max_warps_per_shader; 
     unsigned max_cta_per_core; //Limit on number of concurrent CTAs in shader core
-    unsigned max_barriers_per_cta;
+
     char * gpgpu_scheduler_string;
 
     char* pipeline_widths_string;
     int pipe_widths[N_PIPELINE_STAGES];
 
-    mutable cache_config m_L1I_config;
-    mutable cache_config m_L1T_config;
-    mutable cache_config m_L1C_config;
-    mutable l1d_cache_config m_L1D_config;
+    cache_config m_L1I_config;
+    cache_config m_L1T_config;
+    cache_config m_L1C_config;
+    cache_config m_L1D_config;
 
-    bool gmem_skip_L1D; // on = global memory access always skip the L1 cache 
-    
     bool gpgpu_dwf_reg_bankconflict;
 
     int gpgpu_num_sched_per_core;
@@ -1331,7 +1323,7 @@
 
 struct shader_core_stats_pod {
 
-	void* shader_core_stats_pod_start[0]; // DO NOT MOVE FROM THE TOP - spaceless pointer to the start of this structure
+	void* shader_core_stats_pod_start[]; // DO NOT MOVE FROM THE TOP - spaceless pointer to the start of this structure
 	unsigned long long *shader_cycles;
     unsigned *m_num_sim_insn; // number of scalar thread instructions committed by this shader core
     unsigned *m_num_sim_winsn; // number of warp instructions committed by this shader core
@@ -1396,15 +1388,24 @@
     int gpgpu_n_mem_read_inst;
     
     int gpgpu_n_mem_l2_writeback;
-    int gpgpu_n_mem_l1_write_allocate; 
     int gpgpu_n_mem_l2_write_allocate;
 
     unsigned made_write_mfs;
     unsigned made_read_mfs;
 
+    // Power stats
     unsigned *gpgpu_n_shmem_bank_access;
-    long *n_simt_to_mem; // Interconnect power stats
-    long *n_mem_to_simt;
+    unsigned *inst_c_read_access;	// Instruction cache read access
+    unsigned *inst_c_read_miss;		// Instruction cache read miss
+    unsigned *const_c_read_access;	// Constant cache read access
+    unsigned *const_c_read_miss;		// Constant cache read miss
+    unsigned *text_c_read_access;	// Texture cache read access
+    unsigned *text_c_read_miss;		// Texture cache read miss
+    unsigned *l1d_read_access;		// L1 Data cache read access
+    unsigned *l1d_read_miss;			// L1 Data cache read miss
+    unsigned *l1d_write_access;		// L1 Data cache write access
+    unsigned *l1d_write_miss;		// L1 Data cache write miss
+
 };
 
 class shader_core_stats : public shader_core_stats_pod {
@@ -1454,11 +1455,17 @@
         shader_cycle_distro = (unsigned*) calloc(config->warp_size+3, sizeof(unsigned));
         last_shader_cycle_distro = (unsigned*) calloc(m_config->warp_size+3, sizeof(unsigned));
 
-        n_simt_to_mem = (long *)calloc(config->num_shader(), sizeof(long));
-        n_mem_to_simt = (long *)calloc(config->num_shader(), sizeof(long));
-
-        m_outgoing_traffic_stats = new traffic_breakdown("coretomem"); 
-        m_incoming_traffic_stats = new traffic_breakdown("memtocore"); 
+        // Power stats
+        inst_c_read_access = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+        inst_c_read_miss = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+        const_c_read_access = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+        const_c_read_miss = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+        text_c_read_access = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+        text_c_read_miss = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+        l1d_read_access = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+        l1d_read_miss = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+        l1d_write_access = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
+        l1d_write_miss = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
 
         gpgpu_n_shmem_bank_access = (unsigned *)calloc(config->num_shader(), sizeof(unsigned));
 
@@ -1466,17 +1473,6 @@
         m_shader_warp_slot_issue_distro.resize( config->num_shader() );
     }
 
-    ~shader_core_stats()
-    {
-        delete m_outgoing_traffic_stats; 
-        delete m_incoming_traffic_stats; 
-        free(m_num_sim_insn); 
-        free(m_num_sim_winsn);
-        free(m_n_diverge); 
-        free(shader_cycle_distro);
-        free(last_shader_cycle_distro);
-    }
-
     void new_grid()
     {
     }
@@ -1499,10 +1495,6 @@
 
 private:
     const shader_core_config *m_config;
-
-    traffic_breakdown *m_outgoing_traffic_stats; // core to memory partitions
-    traffic_breakdown *m_incoming_traffic_stats; // memory partition to core 
-
     // Counts the instructions issued for each dynamic warp.
     std::vector< std::vector<unsigned> > m_shader_dynamic_warp_issue_distro;
     std::vector<unsigned> m_last_shader_dynamic_warp_issue_distro;
@@ -1535,6 +1527,7 @@
     				       -1, 
     				       m_core_id, 
     				       m_cluster_id,
+    				       -1, // TODO: Rachata --> tid
     				       m_memory_config );
     	return mf;
     }
@@ -1548,6 +1541,7 @@
                                       inst.warp_id(),
                                       m_core_id, 
                                       m_cluster_id, 
+                                      -1, // TODO: Rachata --> tid
                                       m_memory_config);
         return mf;
     }
@@ -1577,7 +1571,6 @@
     void cache_flush();
     void accept_fetch_response( mem_fetch *mf );
     void accept_ldst_unit_response( class mem_fetch * mf );
-    void broadcast_barrier_reduction(unsigned cta_id, unsigned bar_id,warp_set_t warps);
     void set_kernel( kernel_info_t *k ) 
     {
         assert(k);
@@ -1619,14 +1612,9 @@
     std::list<unsigned> get_regs_written( const inst_t &fvt ) const;
     const shader_core_config *get_config() const { return m_config; }
     void print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses );
+    void get_cache_stats(unsigned &read_accesses, unsigned &write_accesses, unsigned &read_misses, unsigned &write_misses, unsigned cache_type);
 
-    void get_cache_stats(cache_stats &cs);
-    void get_L1I_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1D_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1C_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1T_sub_stats(struct cache_sub_stats &css) const;
-
-    void get_icnt_power_stats(long &n_simt_to_mem, long &n_mem_to_simt) const;
+    void set_icnt_power_stats(unsigned &n_simt_to_mem) const;
 
 // debug:
     void display_simt_state(FILE *fout, int mask ) const;
@@ -1732,11 +1720,7 @@
 	 void incsfuactivelanes_stat(unsigned active_count) {m_stats->m_active_sfu_lanes[m_sid]=m_stats->m_active_sfu_lanes[m_sid]+active_count;}
 	 void incfuactivelanes_stat(unsigned active_count) {m_stats->m_active_fu_lanes[m_sid]=m_stats->m_active_fu_lanes[m_sid]+active_count;}
 	 void incfumemactivelanes_stat(unsigned active_count) {m_stats->m_active_fu_mem_lanes[m_sid]=m_stats->m_active_fu_mem_lanes[m_sid]+active_count;}
-
-	 void inc_simt_to_mem(unsigned n_flits){ m_stats->n_simt_to_mem[m_sid] += n_flits; }
-	 bool check_if_non_released_reduction_barrier(warp_inst_t &inst);
-
-	private:
+private:
 	 unsigned inactive_lanes_accesses_sfu(unsigned active_count,double latency){
       return  ( ((32-active_count)>>1)*latency) + ( ((32-active_count)>>3)*latency) + ( ((32-active_count)>>3)*latency);
 	 }
@@ -1869,14 +1853,9 @@
 
     void display_pipeline( unsigned sid, FILE *fout, int print_mem, int mask );
     void print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses ) const;
+    void get_cache_stats(unsigned &read_accesses, unsigned &write_accesses, unsigned &read_misses, unsigned &write_misses, unsigned cache_type) const;
 
-    void get_cache_stats(cache_stats &cs) const;
-    void get_L1I_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1D_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1C_sub_stats(struct cache_sub_stats &css) const;
-    void get_L1T_sub_stats(struct cache_sub_stats &css) const;
-
-    void get_icnt_stats(long &n_simt_to_mem, long &n_mem_to_simt) const;
+    void set_icnt_stats(unsigned &n_simt_to_mem) const;
 
 private:
     unsigned m_cluster_id;
@@ -1900,8 +1879,7 @@
     }
     virtual void push(mem_fetch *mf) 
     {
-    	m_core->inc_simt_to_mem(mf->get_num_flits(true));
-        m_cluster->icnt_inject_request_packet(mf);        
+        m_cluster->icnt_inject_request_packet(mf);
     }
 private:
     shader_core_ctx *m_core;
@@ -1917,10 +1895,11 @@
     }
     virtual void push(mem_fetch *mf)
     {
+        if( !mf->get_inst().empty() )
+            m_core->mem_instruction_stats(mf->get_inst()); // not I$-fetch
         if ( mf && mf->isatomic() )
             mf->do_atomic(); // execute atomic inside the "memory subsystem"
-        m_core->inc_simt_to_mem(mf->get_num_flits(true));
-        m_cluster->push_response_fifo(mf);        
+        m_cluster->push_response_fifo(mf);
     }
 private:
     shader_core_ctx *m_core;
diff -Naur gpgpu-sim-baseline/shader_trace.h gpgpu-sim/shader_trace.h
--- gpgpu-sim-baseline/shader_trace.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/shader_trace.h	2015-10-13 06:38:50.534481857 -0400
@@ -36,8 +36,7 @@
 
 #define SHADER_PRINT_STR SIM_PRINT_STR "Core %d - "
 #define SCHED_PRINT_STR SHADER_PRINT_STR "Scheduler %d - "
-#define SHADER_DTRACE(x)  (DTRACE(x) && (Trace::sampling_core == get_sid()\
-                                         || Trace::sampling_core == -1))
+#define SHADER_DTRACE(x)  (DTRACE(x) && Trace::sampling_core == get_sid())
 
 // Intended to be called from inside components of a shader core.
 // Depends on a get_sid() function
diff -Naur gpgpu-sim-baseline/stats.h gpgpu-sim/stats.h
--- gpgpu-sim-baseline/stats.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/stats.h	2015-10-13 06:38:50.558481857 -0400
@@ -50,7 +50,6 @@
    ICNT_RC_FAIL,
    COAL_STALL,
    TLB_STALL,
-   DATA_PORT_STALL,
    WB_ICNT_RC_FAIL,
    WB_CACHE_RSRV_FAIL,
    N_MEM_STAGE_STALL_TYPE
diff -Naur gpgpu-sim-baseline/traffic_breakdown.cc gpgpu-sim/traffic_breakdown.cc
--- gpgpu-sim-baseline/traffic_breakdown.cc	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/traffic_breakdown.cc	1969-12-31 19:00:00.000000000 -0500
@@ -1,51 +0,0 @@
-#include "traffic_breakdown.h" 
-#include "mem_fetch.h" 
-
-void traffic_breakdown::print(FILE* fout)
-{
-   for (traffic_stat_t::const_iterator i_stat = m_stats.begin(); i_stat != m_stats.end(); i_stat++) {
-      unsigned int byte_transferred = 0; 
-      for (traffic_class_t::const_iterator i_class = i_stat->second.begin(); i_class != i_stat->second.end(); i_class++) {
-         byte_transferred += i_class->first * i_class->second;  // byte/packet x #packets
-      }
-      fprintf(fout, "traffic_breakdown_%s[%s] = %u {", m_network_name.c_str(), i_stat->first.c_str(), byte_transferred);  
-      for (traffic_class_t::const_iterator i_class = i_stat->second.begin(); i_class != i_stat->second.end(); i_class++) {
-         fprintf(fout, "%u:%u,", i_class->first, i_class->second); 
-      }
-      fprintf(fout, "}\n"); 
-   }
-}
-
-void traffic_breakdown::record_traffic(class mem_fetch * mf, unsigned int size) 
-{
-   m_stats[classify_memfetch(mf)][size] += 1; 
-}
-
-std::string traffic_breakdown::classify_memfetch(class mem_fetch * mf)
-{
-   std::string traffic_name; 
-
-   enum mem_access_type access_type = mf->get_access_type(); 
-
-   switch (access_type) {
-   case CONST_ACC_R:    
-   case TEXTURE_ACC_R:   
-   case GLOBAL_ACC_W:   
-   case LOCAL_ACC_R:    
-   case LOCAL_ACC_W:    
-   case INST_ACC_R:     
-   case L1_WRBK_ACC:    
-   case L2_WRBK_ACC:    
-   case L1_WR_ALLOC_R:  
-   case L2_WR_ALLOC_R:  
-      traffic_name = mem_access_type_str(access_type); 
-      break; 
-   case GLOBAL_ACC_R:   
-      // check for global atomic operation 
-      traffic_name = (mf->isatomic())? "GLOBAL_ATOMIC" : mem_access_type_str(GLOBAL_ACC_R); 
-      break; 
-   default: assert(0 && "Unknown traffic type"); 
-   }
-   return traffic_name; 
-}
-
diff -Naur gpgpu-sim-baseline/traffic_breakdown.h gpgpu-sim/traffic_breakdown.h
--- gpgpu-sim-baseline/traffic_breakdown.h	2015-10-13 06:49:22.178497764 -0400
+++ gpgpu-sim/traffic_breakdown.h	1969-12-31 19:00:00.000000000 -0500
@@ -1,37 +0,0 @@
-#pragma once 
-
-#include <stdio.h>
-#include <map>
-#include <string> 
-
-// Breakdown traffic through the network according to category
-class traffic_breakdown
-{
-public: 
-   traffic_breakdown(const std::string &network_name) 
-   : m_network_name(network_name) 
-   { }
-
-   // print the stats 
-   void print(FILE* fout); 
-
-   // record the amount and type of traffic introduced by this mem_fetch object 
-   void record_traffic(class mem_fetch * mf, unsigned int size); 
-
-protected:
-
-   std::string m_network_name; 
-
-   /// helper functions to identify the type of traffic sent 
-   std::string classify_memfetch(class mem_fetch * mf); 
-
-   /// helper functions to identify the size of traffic sent 
-   unsigned int packet_size(class mem_fetch * mf); 
-
-   typedef std::string mf_packet_type;  // use string so that it remains extensible 
-   typedef unsigned int mf_packet_size; 
-   typedef std::map < mf_packet_size, unsigned int > traffic_class_t; 
-   typedef std::map < mf_packet_type, traffic_class_t > traffic_stat_t; 
-
-   traffic_stat_t m_stats; 
-}; 
